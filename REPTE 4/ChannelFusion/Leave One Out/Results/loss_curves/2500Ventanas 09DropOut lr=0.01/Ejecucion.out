LEAVE ONE OUT
Cargando datos por paciente...
Fold 1: Test Patient chb02
Fold 1: Distribution of classes
Training set: Class 0: 49724, Class 1: 49724
Test set: Class 0: 1352, Class 1: 1352

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8967, Val Loss: 0.9266
Epoch 2/50, Train Loss: 0.8772, Val Loss: 0.8855
Epoch 3/50, Train Loss: 0.8700, Val Loss: 0.8541
Epoch 4/50, Train Loss: 0.8632, Val Loss: 0.8600
Epoch 5/50, Train Loss: 0.8610, Val Loss: 0.8561
Epoch 6/50, Train Loss: 0.8584, Val Loss: 0.8305
Epoch 7/50, Train Loss: 0.8597, Val Loss: 0.8335
Epoch 8/50, Train Loss: 0.8591, Val Loss: 0.8461
Epoch 9/50, Train Loss: 0.8592, Val Loss: 0.8790
Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.
Epoch 10/50, Train Loss: 0.8584, Val Loss: 0.8399
Epoch 11/50, Train Loss: 0.8476, Val Loss: 0.8489
Epoch 12/50, Train Loss: 0.8410, Val Loss: 0.8660
Epoch 13/50, Train Loss: 0.8391, Val Loss: 0.8557
Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.
Epoch 14/50, Train Loss: 0.8363, Val Loss: 0.8492
Epoch 15/50, Train Loss: 0.8327, Val Loss: 0.8329
Epoch 16/50, Train Loss: 0.8315, Val Loss: 0.8364
Epoch 17/50, Train Loss: 0.8310, Val Loss: 0.8323
Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.
Epoch 18/50, Train Loss: 0.8301, Val Loss: 0.8353
Epoch 19/50, Train Loss: 0.8300, Val Loss: 0.8309
Epoch 20/50, Train Loss: 0.8295, Val Loss: 0.8340
Epoch 21/50, Train Loss: 0.8297, Val Loss: 0.8309
Epoch 00022: reducing learning rate of group 0 to 1.0000e-06.
Epoch 22/50, Train Loss: 0.8295, Val Loss: 0.8324
Epoch 23/50, Train Loss: 0.8294, Val Loss: 0.8343
Epoch 24/50, Train Loss: 0.8293, Val Loss: 0.8335
Epoch 25/50, Train Loss: 0.8301, Val Loss: 0.8325
Epoch 00026: reducing learning rate of group 0 to 1.0000e-07.
Epoch 26/50, Train Loss: 0.8292, Val Loss: 0.8343
Epoch 27/50, Train Loss: 0.8302, Val Loss: 0.8324
Epoch 28/50, Train Loss: 0.8295, Val Loss: 0.8330
Epoch 29/50, Train Loss: 0.8296, Val Loss: 0.8351
Epoch 00030: reducing learning rate of group 0 to 1.0000e-08.
Epoch 30/50, Train Loss: 0.8294, Val Loss: 0.8334
Epoch 31/50, Train Loss: 0.8295, Val Loss: 0.8323
Epoch 32/50, Train Loss: 0.8293, Val Loss: 0.8337
Epoch 33/50, Train Loss: 0.8290, Val Loss: 0.8356
Epoch 34/50, Train Loss: 0.8299, Val Loss: 0.8334
Epoch 35/50, Train Loss: 0.8294, Val Loss: 0.8337
Epoch 36/50, Train Loss: 0.8294, Val Loss: 0.8326
Epoch 37/50, Train Loss: 0.8293, Val Loss: 0.8342
Epoch 38/50, Train Loss: 0.8295, Val Loss: 0.8329
Epoch 39/50, Train Loss: 0.8294, Val Loss: 0.8354
Epoch 40/50, Train Loss: 0.8293, Val Loss: 0.8346
Epoch 41/50, Train Loss: 0.8291, Val Loss: 0.8333
Epoch 42/50, Train Loss: 0.8295, Val Loss: 0.8337
Epoch 43/50, Train Loss: 0.8295, Val Loss: 0.8350
Epoch 44/50, Train Loss: 0.8296, Val Loss: 0.8345
Epoch 45/50, Train Loss: 0.8297, Val Loss: 0.8347
Epoch 46/50, Train Loss: 0.8296, Val Loss: 0.8348
Epoch 47/50, Train Loss: 0.8294, Val Loss: 0.8346
Epoch 48/50, Train Loss: 0.8297, Val Loss: 0.8329
Epoch 49/50, Train Loss: 0.8291, Val Loss: 0.8335
Epoch 50/50, Train Loss: 0.8295, Val Loss: 0.8333
Epoch 1/50, Train Loss: 0.8296, Val Loss: 0.8343
Epoch 2/50, Train Loss: 0.8289, Val Loss: 0.8330
Epoch 3/50, Train Loss: 0.8300, Val Loss: 0.8345
Epoch 4/50, Train Loss: 0.8294, Val Loss: 0.8346
Epoch 5/50, Train Loss: 0.8297, Val Loss: 0.8338
Epoch 6/50, Train Loss: 0.8293, Val Loss: 0.8326
Epoch 7/50, Train Loss: 0.8291, Val Loss: 0.8339
Epoch 8/50, Train Loss: 0.8293, Val Loss: 0.8331
Epoch 9/50, Train Loss: 0.8293, Val Loss: 0.8349
Epoch 10/50, Train Loss: 0.8297, Val Loss: 0.8325
Epoch 11/50, Train Loss: 0.8296, Val Loss: 0.8326
Epoch 12/50, Train Loss: 0.8291, Val Loss: 0.8348
Epoch 13/50, Train Loss: 0.8297, Val Loss: 0.8344
Epoch 14/50, Train Loss: 0.8298, Val Loss: 0.8344
Epoch 15/50, Train Loss: 0.8296, Val Loss: 0.8338
Epoch 16/50, Train Loss: 0.8291, Val Loss: 0.8348
Epoch 17/50, Train Loss: 0.8295, Val Loss: 0.8350
Epoch 18/50, Train Loss: 0.8294, Val Loss: 0.8349
Epoch 19/50, Train Loss: 0.8295, Val Loss: 0.8357
Epoch 20/50, Train Loss: 0.8292, Val Loss: 0.8330
Epoch 21/50, Train Loss: 0.8296, Val Loss: 0.8342
Epoch 22/50, Train Loss: 0.8295, Val Loss: 0.8331
Epoch 23/50, Train Loss: 0.8294, Val Loss: 0.8326
Epoch 24/50, Train Loss: 0.8296, Val Loss: 0.8335
Epoch 25/50, Train Loss: 0.8293, Val Loss: 0.8333
Epoch 26/50, Train Loss: 0.8296, Val Loss: 0.8339
Epoch 27/50, Train Loss: 0.8295, Val Loss: 0.8329
Epoch 28/50, Train Loss: 0.8292, Val Loss: 0.8351
Epoch 29/50, Train Loss: 0.8296, Val Loss: 0.8359
Epoch 30/50, Train Loss: 0.8289, Val Loss: 0.8353
Epoch 31/50, Train Loss: 0.8293, Val Loss: 0.8337
Epoch 32/50, Train Loss: 0.8297, Val Loss: 0.8319
Epoch 33/50, Train Loss: 0.8297, Val Loss: 0.8351
Epoch 34/50, Train Loss: 0.8294, Val Loss: 0.8340
Epoch 35/50, Train Loss: 0.8296, Val Loss: 0.8332
Epoch 36/50, Train Loss: 0.8296, Val Loss: 0.8322
Epoch 37/50, Train Loss: 0.8295, Val Loss: 0.8341
Epoch 38/50, Train Loss: 0.8293, Val Loss: 0.8352
Epoch 39/50, Train Loss: 0.8297, Val Loss: 0.8340
Epoch 40/50, Train Loss: 0.8293, Val Loss: 0.8357
Epoch 41/50, Train Loss: 0.8295, Val Loss: 0.8335
Epoch 42/50, Train Loss: 0.8298, Val Loss: 0.8333
Epoch 43/50, Train Loss: 0.8298, Val Loss: 0.8315
Epoch 44/50, Train Loss: 0.8299, Val Loss: 0.8350
Epoch 45/50, Train Loss: 0.8295, Val Loss: 0.8336
Epoch 46/50, Train Loss: 0.8291, Val Loss: 0.8340
Epoch 47/50, Train Loss: 0.8294, Val Loss: 0.8341
Epoch 48/50, Train Loss: 0.8296, Val Loss: 0.8334
Epoch 49/50, Train Loss: 0.8293, Val Loss: 0.8331
Epoch 50/50, Train Loss: 0.8298, Val Loss: 0.8331
Metrics for Fold 1 (Train):
{'0.0': {'precision': 0.8722014730736328, 'recall': 0.840680556672834, 'f1-score': 0.8561509866770438, 'support': 49724.0}, '1.0': {'precision': 0.8462374565711069, 'recall': 0.8768200466575496, 'f1-score': 0.8612573460417798, 'support': 49724.0}, 'accuracy': 0.8587503016651918, 'macro avg': {'precision': 0.8592194648223699, 'recall': 0.8587503016651918, 'f1-score': 0.8587041663594118, 'support': 99448.0}, 'weighted avg': {'precision': 0.85921946482237, 'recall': 0.8587503016651918, 'f1-score': 0.8587041663594118, 'support': 99448.0}}
Metrics for Fold 1 (Test):
{'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}}
Fold 2: Test Patient chb05
Fold 2: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8810, Val Loss: 0.9518
Epoch 2/50, Train Loss: 0.8576, Val Loss: 0.9344
Epoch 3/50, Train Loss: 0.8522, Val Loss: 0.9536
Epoch 4/50, Train Loss: 0.8474, Val Loss: 0.9187
Epoch 5/50, Train Loss: 0.8463, Val Loss: 0.9326
Epoch 6/50, Train Loss: 0.8402, Val Loss: 0.9388
Epoch 7/50, Train Loss: 0.8365, Val Loss: 0.9644
Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.
Epoch 8/50, Train Loss: 0.8359, Val Loss: 0.9358
Epoch 9/50, Train Loss: 0.8190, Val Loss: 0.9162
Epoch 10/50, Train Loss: 0.8131, Val Loss: 0.8877
Epoch 11/50, Train Loss: 0.8105, Val Loss: 0.9070
Epoch 12/50, Train Loss: 0.8090, Val Loss: 0.8886
Epoch 13/50, Train Loss: 0.8087, Val Loss: 0.8789
Epoch 14/50, Train Loss: 0.8079, Val Loss: 0.9001
Epoch 15/50, Train Loss: 0.8063, Val Loss: 0.9041
Epoch 16/50, Train Loss: 0.8061, Val Loss: 0.8951
Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.
Epoch 17/50, Train Loss: 0.8046, Val Loss: 0.8961
Epoch 18/50, Train Loss: 0.8010, Val Loss: 0.8849
Epoch 19/50, Train Loss: 0.7993, Val Loss: 0.8853
Epoch 20/50, Train Loss: 0.7994, Val Loss: 0.8876
Epoch 00021: reducing learning rate of group 0 to 1.0000e-05.
Epoch 21/50, Train Loss: 0.7984, Val Loss: 0.8876
Epoch 22/50, Train Loss: 0.7978, Val Loss: 0.8850
Epoch 23/50, Train Loss: 0.7973, Val Loss: 0.8838
Epoch 24/50, Train Loss: 0.7976, Val Loss: 0.8839
Epoch 00025: reducing learning rate of group 0 to 1.0000e-06.
Epoch 25/50, Train Loss: 0.7973, Val Loss: 0.8827
Epoch 26/50, Train Loss: 0.7978, Val Loss: 0.8837
Epoch 27/50, Train Loss: 0.7973, Val Loss: 0.8832
Epoch 28/50, Train Loss: 0.7977, Val Loss: 0.8834
Epoch 00029: reducing learning rate of group 0 to 1.0000e-07.
Epoch 29/50, Train Loss: 0.7973, Val Loss: 0.8837
Epoch 30/50, Train Loss: 0.7977, Val Loss: 0.8830
Epoch 31/50, Train Loss: 0.7976, Val Loss: 0.8835
Epoch 32/50, Train Loss: 0.7973, Val Loss: 0.8832
Epoch 00033: reducing learning rate of group 0 to 1.0000e-08.
Epoch 33/50, Train Loss: 0.7970, Val Loss: 0.8835
Epoch 34/50, Train Loss: 0.7975, Val Loss: 0.8836
Epoch 35/50, Train Loss: 0.7976, Val Loss: 0.8841
Epoch 36/50, Train Loss: 0.7976, Val Loss: 0.8834
Epoch 37/50, Train Loss: 0.7974, Val Loss: 0.8833
Epoch 38/50, Train Loss: 0.7976, Val Loss: 0.8829
Epoch 39/50, Train Loss: 0.7973, Val Loss: 0.8831
Epoch 40/50, Train Loss: 0.7977, Val Loss: 0.8827
Epoch 41/50, Train Loss: 0.7971, Val Loss: 0.8829
Epoch 42/50, Train Loss: 0.7978, Val Loss: 0.8845
Epoch 43/50, Train Loss: 0.7973, Val Loss: 0.8831
Epoch 44/50, Train Loss: 0.7976, Val Loss: 0.8827
Epoch 45/50, Train Loss: 0.7973, Val Loss: 0.8833
Epoch 46/50, Train Loss: 0.7975, Val Loss: 0.8835
Epoch 47/50, Train Loss: 0.7974, Val Loss: 0.8832
Epoch 48/50, Train Loss: 0.7975, Val Loss: 0.8840
Epoch 49/50, Train Loss: 0.7976, Val Loss: 0.8834
Epoch 50/50, Train Loss: 0.7976, Val Loss: 0.8838
Epoch 1/50, Train Loss: 0.7973, Val Loss: 0.8832
Epoch 2/50, Train Loss: 0.7971, Val Loss: 0.8837
Epoch 3/50, Train Loss: 0.7973, Val Loss: 0.8842
Epoch 4/50, Train Loss: 0.7974, Val Loss: 0.8837
Epoch 5/50, Train Loss: 0.7975, Val Loss: 0.8840
Epoch 6/50, Train Loss: 0.7973, Val Loss: 0.8835
Epoch 7/50, Train Loss: 0.7974, Val Loss: 0.8833
Epoch 8/50, Train Loss: 0.7975, Val Loss: 0.8831
Epoch 9/50, Train Loss: 0.7977, Val Loss: 0.8835
Epoch 10/50, Train Loss: 0.7976, Val Loss: 0.8838
Epoch 11/50, Train Loss: 0.7974, Val Loss: 0.8839
Epoch 12/50, Train Loss: 0.7979, Val Loss: 0.8832
Epoch 13/50, Train Loss: 0.7977, Val Loss: 0.8833
Epoch 14/50, Train Loss: 0.7975, Val Loss: 0.8827
Epoch 15/50, Train Loss: 0.7976, Val Loss: 0.8831
Epoch 16/50, Train Loss: 0.7974, Val Loss: 0.8830
Epoch 17/50, Train Loss: 0.7974, Val Loss: 0.8833
Epoch 18/50, Train Loss: 0.7975, Val Loss: 0.8827
Epoch 19/50, Train Loss: 0.7971, Val Loss: 0.8837
Epoch 20/50, Train Loss: 0.7972, Val Loss: 0.8832
Epoch 21/50, Train Loss: 0.7972, Val Loss: 0.8825
Epoch 22/50, Train Loss: 0.7970, Val Loss: 0.8827
Epoch 23/50, Train Loss: 0.7975, Val Loss: 0.8833
Epoch 24/50, Train Loss: 0.7972, Val Loss: 0.8840
Epoch 25/50, Train Loss: 0.7977, Val Loss: 0.8830
Epoch 26/50, Train Loss: 0.7970, Val Loss: 0.8834
Epoch 27/50, Train Loss: 0.7975, Val Loss: 0.8834
Epoch 28/50, Train Loss: 0.7975, Val Loss: 0.8833
Epoch 29/50, Train Loss: 0.7975, Val Loss: 0.8838
Epoch 30/50, Train Loss: 0.7973, Val Loss: 0.8834
Epoch 31/50, Train Loss: 0.7971, Val Loss: 0.8833
Epoch 32/50, Train Loss: 0.7976, Val Loss: 0.8826
Epoch 33/50, Train Loss: 0.7975, Val Loss: 0.8837
Epoch 34/50, Train Loss: 0.7972, Val Loss: 0.8839
Epoch 35/50, Train Loss: 0.7972, Val Loss: 0.8828
Epoch 36/50, Train Loss: 0.7973, Val Loss: 0.8832
Epoch 37/50, Train Loss: 0.7976, Val Loss: 0.8830
Epoch 38/50, Train Loss: 0.7975, Val Loss: 0.8832
Epoch 39/50, Train Loss: 0.7976, Val Loss: 0.8832
Epoch 40/50, Train Loss: 0.7977, Val Loss: 0.8834
Epoch 41/50, Train Loss: 0.7978, Val Loss: 0.8836
Epoch 42/50, Train Loss: 0.7974, Val Loss: 0.8835
Epoch 43/50, Train Loss: 0.7976, Val Loss: 0.8836
Epoch 44/50, Train Loss: 0.7974, Val Loss: 0.8833
Epoch 45/50, Train Loss: 0.7976, Val Loss: 0.8835
Epoch 46/50, Train Loss: 0.7973, Val Loss: 0.8831
Epoch 47/50, Train Loss: 0.7977, Val Loss: 0.8826
Epoch 48/50, Train Loss: 0.7973, Val Loss: 0.8836
Epoch 49/50, Train Loss: 0.7976, Val Loss: 0.8841
Epoch 50/50, Train Loss: 0.7976, Val Loss: 0.8828
Metrics for Fold 2 (Train):
{'0.0': {'precision': 0.8972506648240048, 'recall': 0.882122859025033, 'f1-score': 0.8896224554410224, 'support': 48576.0}, '1.0': {'precision': 0.8840773357627291, 'recall': 0.8989830368906456, 'f1-score': 0.8914678833532372, 'support': 48576.0}, 'accuracy': 0.8905529479578392, 'macro avg': {'precision': 0.890664000293367, 'recall': 0.8905529479578393, 'f1-score': 0.8905451693971298, 'support': 97152.0}, 'weighted avg': {'precision': 0.890664000293367, 'recall': 0.8905529479578392, 'f1-score': 0.8905451693971298, 'support': 97152.0}}
Metrics for Fold 2 (Test):
{'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}}
Fold 3: Test Patient chb15
Fold 3: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8851, Val Loss: 0.9465
Epoch 2/50, Train Loss: 0.8586, Val Loss: 0.9378
Epoch 3/50, Train Loss: 0.8526, Val Loss: 0.9340
Epoch 4/50, Train Loss: 0.8478, Val Loss: 0.9196
Epoch 5/50, Train Loss: 0.8430, Val Loss: 1.0163
Epoch 6/50, Train Loss: 0.8387, Val Loss: 0.9165
Epoch 7/50, Train Loss: 0.8363, Val Loss: 0.9202
Epoch 8/50, Train Loss: 0.8343, Val Loss: 0.9298
Epoch 9/50, Train Loss: 0.8366, Val Loss: 0.9323
Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.
Epoch 10/50, Train Loss: 0.8366, Val Loss: 0.9624
Epoch 11/50, Train Loss: 0.8203, Val Loss: 0.9333
Epoch 12/50, Train Loss: 0.8134, Val Loss: 0.9393
Epoch 13/50, Train Loss: 0.8108, Val Loss: 0.9178
Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.
Epoch 14/50, Train Loss: 0.8097, Val Loss: 0.9303
Epoch 15/50, Train Loss: 0.8051, Val Loss: 0.9346
Epoch 16/50, Train Loss: 0.8038, Val Loss: 0.9409
Epoch 17/50, Train Loss: 0.8033, Val Loss: 0.9329
Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.
Epoch 18/50, Train Loss: 0.8022, Val Loss: 0.9403
Epoch 19/50, Train Loss: 0.8014, Val Loss: 0.9365
Epoch 20/50, Train Loss: 0.8014, Val Loss: 0.9341
Epoch 21/50, Train Loss: 0.8009, Val Loss: 0.9367
Epoch 00022: reducing learning rate of group 0 to 1.0000e-06.
Epoch 22/50, Train Loss: 0.8010, Val Loss: 0.9370
Epoch 23/50, Train Loss: 0.8008, Val Loss: 0.9345
Epoch 24/50, Train Loss: 0.8008, Val Loss: 0.9361
Epoch 25/50, Train Loss: 0.8005, Val Loss: 0.9352
Epoch 00026: reducing learning rate of group 0 to 1.0000e-07.
Epoch 26/50, Train Loss: 0.8012, Val Loss: 0.9348
Epoch 27/50, Train Loss: 0.8008, Val Loss: 0.9352
Epoch 28/50, Train Loss: 0.8008, Val Loss: 0.9354
Epoch 29/50, Train Loss: 0.8011, Val Loss: 0.9343
Epoch 00030: reducing learning rate of group 0 to 1.0000e-08.
Epoch 30/50, Train Loss: 0.8010, Val Loss: 0.9350
Epoch 31/50, Train Loss: 0.8008, Val Loss: 0.9337
Epoch 32/50, Train Loss: 0.8008, Val Loss: 0.9346
Epoch 33/50, Train Loss: 0.8011, Val Loss: 0.9344
Epoch 34/50, Train Loss: 0.8009, Val Loss: 0.9355
Epoch 35/50, Train Loss: 0.8012, Val Loss: 0.9352
Epoch 36/50, Train Loss: 0.8007, Val Loss: 0.9351
Epoch 37/50, Train Loss: 0.8009, Val Loss: 0.9357
Epoch 38/50, Train Loss: 0.8006, Val Loss: 0.9351
Epoch 39/50, Train Loss: 0.8005, Val Loss: 0.9336
Epoch 40/50, Train Loss: 0.8006, Val Loss: 0.9355
Epoch 41/50, Train Loss: 0.8011, Val Loss: 0.9356
Epoch 42/50, Train Loss: 0.8013, Val Loss: 0.9354
Epoch 43/50, Train Loss: 0.8009, Val Loss: 0.9348
Epoch 44/50, Train Loss: 0.8006, Val Loss: 0.9338
Epoch 45/50, Train Loss: 0.8007, Val Loss: 0.9345
Epoch 46/50, Train Loss: 0.8006, Val Loss: 0.9347
Epoch 47/50, Train Loss: 0.8009, Val Loss: 0.9340
Epoch 48/50, Train Loss: 0.8010, Val Loss: 0.9350
Epoch 49/50, Train Loss: 0.8008, Val Loss: 0.9350
Epoch 50/50, Train Loss: 0.8008, Val Loss: 0.9352
Epoch 1/50, Train Loss: 0.8005, Val Loss: 0.9332
Epoch 2/50, Train Loss: 0.8008, Val Loss: 0.9349
Epoch 3/50, Train Loss: 0.8008, Val Loss: 0.9346
Epoch 4/50, Train Loss: 0.8007, Val Loss: 0.9351
Epoch 5/50, Train Loss: 0.8009, Val Loss: 0.9348
Epoch 6/50, Train Loss: 0.8009, Val Loss: 0.9341
Epoch 7/50, Train Loss: 0.8006, Val Loss: 0.9329
Epoch 8/50, Train Loss: 0.8007, Val Loss: 0.9349
Epoch 9/50, Train Loss: 0.8007, Val Loss: 0.9338
Epoch 10/50, Train Loss: 0.8007, Val Loss: 0.9328
Epoch 11/50, Train Loss: 0.8010, Val Loss: 0.9337
Epoch 12/50, Train Loss: 0.8008, Val Loss: 0.9348
Epoch 13/50, Train Loss: 0.8007, Val Loss: 0.9351
Epoch 14/50, Train Loss: 0.8008, Val Loss: 0.9342
Epoch 15/50, Train Loss: 0.8006, Val Loss: 0.9362
Epoch 16/50, Train Loss: 0.8007, Val Loss: 0.9356
Epoch 17/50, Train Loss: 0.8009, Val Loss: 0.9336
Epoch 18/50, Train Loss: 0.8006, Val Loss: 0.9348
Epoch 19/50, Train Loss: 0.8014, Val Loss: 0.9325
Epoch 20/50, Train Loss: 0.8009, Val Loss: 0.9338
Epoch 21/50, Train Loss: 0.8006, Val Loss: 0.9341
Epoch 22/50, Train Loss: 0.8006, Val Loss: 0.9356
Epoch 23/50, Train Loss: 0.8003, Val Loss: 0.9360
Epoch 24/50, Train Loss: 0.8009, Val Loss: 0.9338
Epoch 25/50, Train Loss: 0.8010, Val Loss: 0.9355
Epoch 26/50, Train Loss: 0.8006, Val Loss: 0.9355
Epoch 27/50, Train Loss: 0.8008, Val Loss: 0.9358
Epoch 28/50, Train Loss: 0.8005, Val Loss: 0.9339
Epoch 29/50, Train Loss: 0.8006, Val Loss: 0.9341
Epoch 30/50, Train Loss: 0.8007, Val Loss: 0.9347
Epoch 31/50, Train Loss: 0.8011, Val Loss: 0.9350
Epoch 32/50, Train Loss: 0.8008, Val Loss: 0.9343
Epoch 33/50, Train Loss: 0.8008, Val Loss: 0.9344
Epoch 34/50, Train Loss: 0.8010, Val Loss: 0.9332
Epoch 35/50, Train Loss: 0.8008, Val Loss: 0.9348
Epoch 36/50, Train Loss: 0.8007, Val Loss: 0.9338
Epoch 37/50, Train Loss: 0.8007, Val Loss: 0.9350
Epoch 38/50, Train Loss: 0.8013, Val Loss: 0.9352
Epoch 39/50, Train Loss: 0.8011, Val Loss: 0.9355
Epoch 40/50, Train Loss: 0.8009, Val Loss: 0.9365
Epoch 41/50, Train Loss: 0.8011, Val Loss: 0.9350
Epoch 42/50, Train Loss: 0.8006, Val Loss: 0.9362
Epoch 43/50, Train Loss: 0.8007, Val Loss: 0.9356
Epoch 44/50, Train Loss: 0.8004, Val Loss: 0.9344
Epoch 45/50, Train Loss: 0.8010, Val Loss: 0.9344
Epoch 46/50, Train Loss: 0.8009, Val Loss: 0.9349
Epoch 47/50, Train Loss: 0.8010, Val Loss: 0.9363
Epoch 48/50, Train Loss: 0.8006, Val Loss: 0.9347
Epoch 49/50, Train Loss: 0.8009, Val Loss: 0.9351
Epoch 50/50, Train Loss: 0.8009, Val Loss: 0.9351
Metrics for Fold 3 (Train):
{'0.0': {'precision': 0.9001260710698946, 'recall': 0.8671977931488801, 'f1-score': 0.8833551769331586, 'support': 48576.0}, '1.0': {'precision': 0.8718844954620381, 'recall': 0.9037796442687747, 'f1-score': 0.887545613520808, 'support': 48576.0}, 'accuracy': 0.8854887187088274, 'macro avg': {'precision': 0.8860052832659664, 'recall': 0.8854887187088274, 'f1-score': 0.8854503952269832, 'support': 97152.0}, 'weighted avg': {'precision': 0.8860052832659663, 'recall': 0.8854887187088274, 'f1-score': 0.8854503952269833, 'support': 97152.0}}
Metrics for Fold 3 (Test):
{'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}}
Fold 4: Test Patient chb22
Fold 4: Distribution of classes
Training set: Class 0: 49468, Class 1: 49468
Test set: Class 0: 1608, Class 1: 1608

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8889, Val Loss: 0.9135
Epoch 2/50, Train Loss: 0.8596, Val Loss: 0.9053
Epoch 3/50, Train Loss: 0.8537, Val Loss: 0.8458
Epoch 4/50, Train Loss: 0.8498, Val Loss: 0.8363
Epoch 5/50, Train Loss: 0.8467, Val Loss: 0.8986
Epoch 6/50, Train Loss: 0.8461, Val Loss: 0.8592
Epoch 7/50, Train Loss: 0.8444, Val Loss: 0.8943
Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.
Epoch 8/50, Train Loss: 0.8447, Val Loss: 0.9067
Epoch 9/50, Train Loss: 0.8299, Val Loss: 0.8418
Epoch 10/50, Train Loss: 0.8228, Val Loss: 0.8408
Epoch 11/50, Train Loss: 0.8212, Val Loss: 0.8303
Epoch 12/50, Train Loss: 0.8211, Val Loss: 0.8266
Epoch 13/50, Train Loss: 0.8198, Val Loss: 0.8285
Epoch 14/50, Train Loss: 0.8194, Val Loss: 0.8309
Epoch 15/50, Train Loss: 0.8180, Val Loss: 0.8292
Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.
Epoch 16/50, Train Loss: 0.8170, Val Loss: 0.8369
Epoch 17/50, Train Loss: 0.8131, Val Loss: 0.8229
Epoch 18/50, Train Loss: 0.8118, Val Loss: 0.8211
Epoch 19/50, Train Loss: 0.8115, Val Loss: 0.8227
Epoch 20/50, Train Loss: 0.8111, Val Loss: 0.8209
Epoch 21/50, Train Loss: 0.8106, Val Loss: 0.8198
Epoch 22/50, Train Loss: 0.8102, Val Loss: 0.8185
Epoch 23/50, Train Loss: 0.8099, Val Loss: 0.8190
Epoch 24/50, Train Loss: 0.8103, Val Loss: 0.8196
Epoch 25/50, Train Loss: 0.8096, Val Loss: 0.8184
Epoch 26/50, Train Loss: 0.8094, Val Loss: 0.8171
Epoch 27/50, Train Loss: 0.8089, Val Loss: 0.8176
Epoch 28/50, Train Loss: 0.8087, Val Loss: 0.8176
Epoch 29/50, Train Loss: 0.8072, Val Loss: 0.8158
Epoch 30/50, Train Loss: 0.8068, Val Loss: 0.8166
Epoch 31/50, Train Loss: 0.8060, Val Loss: 0.8129
Epoch 32/50, Train Loss: 0.8053, Val Loss: 0.8152
Epoch 33/50, Train Loss: 0.8047, Val Loss: 0.8107
Epoch 34/50, Train Loss: 0.8040, Val Loss: 0.8150
Epoch 35/50, Train Loss: 0.8034, Val Loss: 0.8133
Epoch 36/50, Train Loss: 0.8032, Val Loss: 0.8128
Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.
Epoch 37/50, Train Loss: 0.8029, Val Loss: 0.8122
Epoch 38/50, Train Loss: 0.8022, Val Loss: 0.8117
Epoch 39/50, Train Loss: 0.8023, Val Loss: 0.8110
Epoch 40/50, Train Loss: 0.8017, Val Loss: 0.8113
Epoch 00041: reducing learning rate of group 0 to 1.0000e-06.
Epoch 41/50, Train Loss: 0.8023, Val Loss: 0.8110
Epoch 42/50, Train Loss: 0.8020, Val Loss: 0.8114
Epoch 43/50, Train Loss: 0.8020, Val Loss: 0.8115
Epoch 44/50, Train Loss: 0.8022, Val Loss: 0.8110
Epoch 00045: reducing learning rate of group 0 to 1.0000e-07.
Epoch 45/50, Train Loss: 0.8020, Val Loss: 0.8114
Epoch 46/50, Train Loss: 0.8020, Val Loss: 0.8104
Epoch 47/50, Train Loss: 0.8018, Val Loss: 0.8114
Epoch 48/50, Train Loss: 0.8018, Val Loss: 0.8116
Epoch 49/50, Train Loss: 0.8017, Val Loss: 0.8107
Epoch 00050: reducing learning rate of group 0 to 1.0000e-08.
Epoch 50/50, Train Loss: 0.8016, Val Loss: 0.8110
Epoch 1/50, Train Loss: 0.8016, Val Loss: 0.8107
Epoch 2/50, Train Loss: 0.8018, Val Loss: 0.8113
Epoch 3/50, Train Loss: 0.8016, Val Loss: 0.8107
Epoch 4/50, Train Loss: 0.8017, Val Loss: 0.8115
Epoch 5/50, Train Loss: 0.8017, Val Loss: 0.8112
Epoch 6/50, Train Loss: 0.8021, Val Loss: 0.8111
Epoch 7/50, Train Loss: 0.8018, Val Loss: 0.8112
Epoch 8/50, Train Loss: 0.8019, Val Loss: 0.8112
Epoch 9/50, Train Loss: 0.8019, Val Loss: 0.8108
Epoch 10/50, Train Loss: 0.8018, Val Loss: 0.8110
Epoch 11/50, Train Loss: 0.8021, Val Loss: 0.8115
Epoch 12/50, Train Loss: 0.8019, Val Loss: 0.8109
Epoch 13/50, Train Loss: 0.8018, Val Loss: 0.8108
Epoch 14/50, Train Loss: 0.8019, Val Loss: 0.8108
Epoch 15/50, Train Loss: 0.8012, Val Loss: 0.8108
Epoch 16/50, Train Loss: 0.8018, Val Loss: 0.8112
Epoch 17/50, Train Loss: 0.8018, Val Loss: 0.8114
Epoch 18/50, Train Loss: 0.8020, Val Loss: 0.8115
Epoch 19/50, Train Loss: 0.8014, Val Loss: 0.8110
Epoch 20/50, Train Loss: 0.8015, Val Loss: 0.8108
Epoch 21/50, Train Loss: 0.8019, Val Loss: 0.8119
Epoch 22/50, Train Loss: 0.8018, Val Loss: 0.8112
Epoch 23/50, Train Loss: 0.8016, Val Loss: 0.8106
Epoch 24/50, Train Loss: 0.8014, Val Loss: 0.8112
Epoch 25/50, Train Loss: 0.8019, Val Loss: 0.8115
Epoch 26/50, Train Loss: 0.8017, Val Loss: 0.8108
Epoch 27/50, Train Loss: 0.8020, Val Loss: 0.8111
Epoch 28/50, Train Loss: 0.8021, Val Loss: 0.8112
Epoch 29/50, Train Loss: 0.8019, Val Loss: 0.8111
Epoch 30/50, Train Loss: 0.8019, Val Loss: 0.8107
Epoch 31/50, Train Loss: 0.8016, Val Loss: 0.8118
Epoch 32/50, Train Loss: 0.8018, Val Loss: 0.8115
Epoch 33/50, Train Loss: 0.8019, Val Loss: 0.8104
Epoch 34/50, Train Loss: 0.8016, Val Loss: 0.8108
Epoch 35/50, Train Loss: 0.8013, Val Loss: 0.8108
Epoch 36/50, Train Loss: 0.8018, Val Loss: 0.8112
Epoch 37/50, Train Loss: 0.8016, Val Loss: 0.8109
Epoch 38/50, Train Loss: 0.8019, Val Loss: 0.8110
Epoch 39/50, Train Loss: 0.8017, Val Loss: 0.8117
Epoch 40/50, Train Loss: 0.8019, Val Loss: 0.8107
Epoch 41/50, Train Loss: 0.8016, Val Loss: 0.8109
Epoch 42/50, Train Loss: 0.8017, Val Loss: 0.8116
Epoch 43/50, Train Loss: 0.8020, Val Loss: 0.8108
Epoch 44/50, Train Loss: 0.8018, Val Loss: 0.8111
Epoch 45/50, Train Loss: 0.8022, Val Loss: 0.8105
Epoch 46/50, Train Loss: 0.8019, Val Loss: 0.8111
Epoch 47/50, Train Loss: 0.8020, Val Loss: 0.8106
Epoch 48/50, Train Loss: 0.8022, Val Loss: 0.8110
Epoch 49/50, Train Loss: 0.8020, Val Loss: 0.8112
Epoch 50/50, Train Loss: 0.8018, Val Loss: 0.8111
Metrics for Fold 4 (Train):
{'0.0': {'precision': 0.9014866370881258, 'recall': 0.8666612759763888, 'f1-score': 0.8837309971656789, 'support': 49468.0}, '1.0': {'precision': 0.8716207010646373, 'recall': 0.9052923101803186, 'f1-score': 0.8881374755818219, 'support': 49468.0}, 'accuracy': 0.8859767930783536, 'macro avg': {'precision': 0.8865536690763816, 'recall': 0.8859767930783538, 'f1-score': 0.8859342363737504, 'support': 98936.0}, 'weighted avg': {'precision': 0.8865536690763816, 'recall': 0.8859767930783536, 'f1-score': 0.8859342363737505, 'support': 98936.0}}
Metrics for Fold 4 (Test):
{'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}}
Fold 5: Test Patient chb08
Fold 5: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8879, Val Loss: 0.9462
Epoch 2/50, Train Loss: 0.8626, Val Loss: 0.9691
Epoch 3/50, Train Loss: 0.8552, Val Loss: 0.9866
Epoch 4/50, Train Loss: 0.8458, Val Loss: 0.9438
Epoch 5/50, Train Loss: 0.8414, Val Loss: 0.9583
Epoch 6/50, Train Loss: 0.8388, Val Loss: 0.9421
Epoch 7/50, Train Loss: 0.8349, Val Loss: 0.9547
Epoch 8/50, Train Loss: 0.8344, Val Loss: 0.9424
Epoch 9/50, Train Loss: 0.8345, Val Loss: 0.9479
Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.
Epoch 10/50, Train Loss: 0.8329, Val Loss: 0.9671
Epoch 11/50, Train Loss: 0.8181, Val Loss: 0.9422
Epoch 12/50, Train Loss: 0.8104, Val Loss: 0.9323
Epoch 13/50, Train Loss: 0.8086, Val Loss: 0.9290
Epoch 14/50, Train Loss: 0.8063, Val Loss: 0.9406
Epoch 15/50, Train Loss: 0.8061, Val Loss: 0.9427
Epoch 16/50, Train Loss: 0.8041, Val Loss: 0.9416
Epoch 17/50, Train Loss: 0.8023, Val Loss: 0.9234
Epoch 18/50, Train Loss: 0.8015, Val Loss: 0.9431
Epoch 19/50, Train Loss: 0.7996, Val Loss: 0.9363
Epoch 20/50, Train Loss: 0.7992, Val Loss: 0.9457
Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.
Epoch 21/50, Train Loss: 0.7986, Val Loss: 0.9387
Epoch 22/50, Train Loss: 0.7938, Val Loss: 0.9406
Epoch 23/50, Train Loss: 0.7922, Val Loss: 0.9398
Epoch 24/50, Train Loss: 0.7920, Val Loss: 0.9383
Epoch 00025: reducing learning rate of group 0 to 1.0000e-05.
Epoch 25/50, Train Loss: 0.7914, Val Loss: 0.9337
Epoch 26/50, Train Loss: 0.7908, Val Loss: 0.9338
Epoch 27/50, Train Loss: 0.7906, Val Loss: 0.9340
Epoch 28/50, Train Loss: 0.7909, Val Loss: 0.9343
Epoch 00029: reducing learning rate of group 0 to 1.0000e-06.
Epoch 29/50, Train Loss: 0.7901, Val Loss: 0.9351
Epoch 30/50, Train Loss: 0.7904, Val Loss: 0.9339
Epoch 31/50, Train Loss: 0.7904, Val Loss: 0.9340
Epoch 32/50, Train Loss: 0.7902, Val Loss: 0.9341
Epoch 00033: reducing learning rate of group 0 to 1.0000e-07.
Epoch 33/50, Train Loss: 0.7900, Val Loss: 0.9352
Epoch 34/50, Train Loss: 0.7904, Val Loss: 0.9342
Epoch 35/50, Train Loss: 0.7904, Val Loss: 0.9340
Epoch 36/50, Train Loss: 0.7905, Val Loss: 0.9355
Epoch 00037: reducing learning rate of group 0 to 1.0000e-08.
Epoch 37/50, Train Loss: 0.7902, Val Loss: 0.9340
Epoch 38/50, Train Loss: 0.7906, Val Loss: 0.9346
Epoch 39/50, Train Loss: 0.7902, Val Loss: 0.9339
Epoch 40/50, Train Loss: 0.7904, Val Loss: 0.9339
Epoch 41/50, Train Loss: 0.7902, Val Loss: 0.9353
Epoch 42/50, Train Loss: 0.7901, Val Loss: 0.9346
Epoch 43/50, Train Loss: 0.7899, Val Loss: 0.9340
Epoch 44/50, Train Loss: 0.7902, Val Loss: 0.9344
Epoch 45/50, Train Loss: 0.7905, Val Loss: 0.9347
Epoch 46/50, Train Loss: 0.7900, Val Loss: 0.9343
Epoch 47/50, Train Loss: 0.7904, Val Loss: 0.9337
Epoch 48/50, Train Loss: 0.7901, Val Loss: 0.9349
Epoch 49/50, Train Loss: 0.7905, Val Loss: 0.9342
Epoch 50/50, Train Loss: 0.7899, Val Loss: 0.9345
Epoch 1/50, Train Loss: 0.7902, Val Loss: 0.9334
Epoch 2/50, Train Loss: 0.7902, Val Loss: 0.9338
Epoch 3/50, Train Loss: 0.7905, Val Loss: 0.9339
Epoch 4/50, Train Loss: 0.7903, Val Loss: 0.9334
Epoch 5/50, Train Loss: 0.7901, Val Loss: 0.9341
Epoch 6/50, Train Loss: 0.7900, Val Loss: 0.9337
Epoch 7/50, Train Loss: 0.7906, Val Loss: 0.9344
Epoch 8/50, Train Loss: 0.7903, Val Loss: 0.9341
Epoch 9/50, Train Loss: 0.7903, Val Loss: 0.9343
Epoch 10/50, Train Loss: 0.7905, Val Loss: 0.9349
Epoch 11/50, Train Loss: 0.7903, Val Loss: 0.9341
Epoch 12/50, Train Loss: 0.7901, Val Loss: 0.9341
Epoch 13/50, Train Loss: 0.7899, Val Loss: 0.9340
Epoch 14/50, Train Loss: 0.7904, Val Loss: 0.9344
Epoch 15/50, Train Loss: 0.7903, Val Loss: 0.9342
Epoch 16/50, Train Loss: 0.7901, Val Loss: 0.9338
Epoch 17/50, Train Loss: 0.7905, Val Loss: 0.9339
Epoch 18/50, Train Loss: 0.7901, Val Loss: 0.9345
Epoch 19/50, Train Loss: 0.7903, Val Loss: 0.9342
Epoch 20/50, Train Loss: 0.7902, Val Loss: 0.9342
Epoch 21/50, Train Loss: 0.7904, Val Loss: 0.9342
Epoch 22/50, Train Loss: 0.7904, Val Loss: 0.9337
Epoch 23/50, Train Loss: 0.7902, Val Loss: 0.9343
Epoch 24/50, Train Loss: 0.7908, Val Loss: 0.9344
Epoch 25/50, Train Loss: 0.7903, Val Loss: 0.9341
Epoch 26/50, Train Loss: 0.7904, Val Loss: 0.9344
Epoch 27/50, Train Loss: 0.7902, Val Loss: 0.9341
Epoch 28/50, Train Loss: 0.7903, Val Loss: 0.9348
Epoch 29/50, Train Loss: 0.7900, Val Loss: 0.9336
Epoch 30/50, Train Loss: 0.7903, Val Loss: 0.9347
Epoch 31/50, Train Loss: 0.7902, Val Loss: 0.9348
Epoch 32/50, Train Loss: 0.7903, Val Loss: 0.9342
Epoch 33/50, Train Loss: 0.7903, Val Loss: 0.9336
Epoch 34/50, Train Loss: 0.7903, Val Loss: 0.9334
Epoch 35/50, Train Loss: 0.7900, Val Loss: 0.9340
Epoch 36/50, Train Loss: 0.7898, Val Loss: 0.9340
Epoch 37/50, Train Loss: 0.7904, Val Loss: 0.9347
Epoch 38/50, Train Loss: 0.7902, Val Loss: 0.9344
Epoch 39/50, Train Loss: 0.7904, Val Loss: 0.9334
Epoch 40/50, Train Loss: 0.7901, Val Loss: 0.9352
Epoch 41/50, Train Loss: 0.7904, Val Loss: 0.9340
Epoch 42/50, Train Loss: 0.7904, Val Loss: 0.9342
Epoch 43/50, Train Loss: 0.7907, Val Loss: 0.9345
Epoch 44/50, Train Loss: 0.7903, Val Loss: 0.9341
Epoch 45/50, Train Loss: 0.7905, Val Loss: 0.9344
Epoch 46/50, Train Loss: 0.7902, Val Loss: 0.9339
Epoch 47/50, Train Loss: 0.7900, Val Loss: 0.9340
Epoch 48/50, Train Loss: 0.7906, Val Loss: 0.9347
Epoch 49/50, Train Loss: 0.7901, Val Loss: 0.9333
Epoch 50/50, Train Loss: 0.7902, Val Loss: 0.9340
Metrics for Fold 5 (Train):
{'0.0': {'precision': 0.9101199242583632, 'recall': 0.890522068511199, 'f1-score': 0.9002143466588974, 'support': 48576.0}, '1.0': {'precision': 0.8928297932368707, 'recall': 0.9120553359683794, 'f1-score': 0.9023401698608933, 'support': 48576.0}, 'accuracy': 0.9012887022397892, 'macro avg': {'precision': 0.9014748587476169, 'recall': 0.9012887022397892, 'f1-score': 0.9012772582598954, 'support': 97152.0}, 'weighted avg': {'precision': 0.901474858747617, 'recall': 0.9012887022397892, 'f1-score': 0.9012772582598954, 'support': 97152.0}}
Metrics for Fold 5 (Test):
{'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}}
Fold 6: Test Patient chb17
Fold 6: Distribution of classes
Training set: Class 0: 48756, Class 1: 48756
Test set: Class 0: 2320, Class 1: 2320

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8888, Val Loss: 0.9160
Epoch 2/50, Train Loss: 0.8615, Val Loss: 0.8846
Epoch 3/50, Train Loss: 0.8524, Val Loss: 0.8365
Epoch 4/50, Train Loss: 0.8441, Val Loss: 0.8309
Epoch 5/50, Train Loss: 0.8407, Val Loss: 0.8425
Epoch 6/50, Train Loss: 0.8382, Val Loss: 0.8374
Epoch 7/50, Train Loss: 0.8362, Val Loss: 0.8459
Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.
Epoch 8/50, Train Loss: 0.8376, Val Loss: 0.8407
Epoch 9/50, Train Loss: 0.8212, Val Loss: 0.8404
Epoch 10/50, Train Loss: 0.8136, Val Loss: 0.8330
Epoch 11/50, Train Loss: 0.8121, Val Loss: 0.8425
Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.
Epoch 12/50, Train Loss: 0.8106, Val Loss: 0.8336
Epoch 13/50, Train Loss: 0.8063, Val Loss: 0.8329
Epoch 14/50, Train Loss: 0.8050, Val Loss: 0.8340
Epoch 15/50, Train Loss: 0.8049, Val Loss: 0.8335
Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.
Epoch 16/50, Train Loss: 0.8042, Val Loss: 0.8331
Epoch 17/50, Train Loss: 0.8034, Val Loss: 0.8340
Epoch 18/50, Train Loss: 0.8031, Val Loss: 0.8331
Epoch 19/50, Train Loss: 0.8031, Val Loss: 0.8339
Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.
Epoch 20/50, Train Loss: 0.8031, Val Loss: 0.8343
Epoch 21/50, Train Loss: 0.8033, Val Loss: 0.8341
Epoch 22/50, Train Loss: 0.8027, Val Loss: 0.8339
Epoch 23/50, Train Loss: 0.8030, Val Loss: 0.8340
Epoch 00024: reducing learning rate of group 0 to 1.0000e-07.
Epoch 24/50, Train Loss: 0.8030, Val Loss: 0.8341
Epoch 25/50, Train Loss: 0.8027, Val Loss: 0.8341
Epoch 26/50, Train Loss: 0.8027, Val Loss: 0.8340
Epoch 27/50, Train Loss: 0.8027, Val Loss: 0.8339
Epoch 00028: reducing learning rate of group 0 to 1.0000e-08.
Epoch 28/50, Train Loss: 0.8028, Val Loss: 0.8339
Epoch 29/50, Train Loss: 0.8032, Val Loss: 0.8340
Epoch 30/50, Train Loss: 0.8029, Val Loss: 0.8341
Epoch 31/50, Train Loss: 0.8031, Val Loss: 0.8339
Epoch 32/50, Train Loss: 0.8030, Val Loss: 0.8341
Epoch 33/50, Train Loss: 0.8028, Val Loss: 0.8342
Epoch 34/50, Train Loss: 0.8030, Val Loss: 0.8338
Epoch 35/50, Train Loss: 0.8031, Val Loss: 0.8339
Epoch 36/50, Train Loss: 0.8033, Val Loss: 0.8343
Epoch 37/50, Train Loss: 0.8031, Val Loss: 0.8342
Epoch 38/50, Train Loss: 0.8028, Val Loss: 0.8342
Epoch 39/50, Train Loss: 0.8028, Val Loss: 0.8340
Epoch 40/50, Train Loss: 0.8025, Val Loss: 0.8340
Epoch 41/50, Train Loss: 0.8030, Val Loss: 0.8341
Epoch 42/50, Train Loss: 0.8029, Val Loss: 0.8341
Epoch 43/50, Train Loss: 0.8029, Val Loss: 0.8345
Epoch 44/50, Train Loss: 0.8028, Val Loss: 0.8339
Epoch 45/50, Train Loss: 0.8026, Val Loss: 0.8343
Epoch 46/50, Train Loss: 0.8029, Val Loss: 0.8344
Epoch 47/50, Train Loss: 0.8029, Val Loss: 0.8342
Epoch 48/50, Train Loss: 0.8029, Val Loss: 0.8342
Epoch 49/50, Train Loss: 0.8028, Val Loss: 0.8341
Epoch 50/50, Train Loss: 0.8023, Val Loss: 0.8339
Epoch 1/50, Train Loss: 0.8027, Val Loss: 0.8343
Epoch 2/50, Train Loss: 0.8029, Val Loss: 0.8339
Epoch 3/50, Train Loss: 0.8032, Val Loss: 0.8337
Epoch 4/50, Train Loss: 0.8030, Val Loss: 0.8340
Epoch 5/50, Train Loss: 0.8031, Val Loss: 0.8339
Epoch 6/50, Train Loss: 0.8030, Val Loss: 0.8341
Epoch 7/50, Train Loss: 0.8033, Val Loss: 0.8337
Epoch 8/50, Train Loss: 0.8025, Val Loss: 0.8345
Epoch 9/50, Train Loss: 0.8028, Val Loss: 0.8342
Epoch 10/50, Train Loss: 0.8028, Val Loss: 0.8338
Epoch 11/50, Train Loss: 0.8026, Val Loss: 0.8340
Epoch 12/50, Train Loss: 0.8027, Val Loss: 0.8341
Epoch 13/50, Train Loss: 0.8030, Val Loss: 0.8339
Epoch 14/50, Train Loss: 0.8030, Val Loss: 0.8342
Epoch 15/50, Train Loss: 0.8028, Val Loss: 0.8342
Epoch 16/50, Train Loss: 0.8026, Val Loss: 0.8341
Epoch 17/50, Train Loss: 0.8032, Val Loss: 0.8340
Epoch 18/50, Train Loss: 0.8024, Val Loss: 0.8343
Epoch 19/50, Train Loss: 0.8027, Val Loss: 0.8344
Epoch 20/50, Train Loss: 0.8028, Val Loss: 0.8343
Epoch 21/50, Train Loss: 0.8026, Val Loss: 0.8339
Epoch 22/50, Train Loss: 0.8026, Val Loss: 0.8341
Epoch 23/50, Train Loss: 0.8025, Val Loss: 0.8338
Epoch 24/50, Train Loss: 0.8025, Val Loss: 0.8342
Epoch 25/50, Train Loss: 0.8027, Val Loss: 0.8344
Epoch 26/50, Train Loss: 0.8026, Val Loss: 0.8343
Epoch 27/50, Train Loss: 0.8026, Val Loss: 0.8338
Epoch 28/50, Train Loss: 0.8029, Val Loss: 0.8340
Epoch 29/50, Train Loss: 0.8031, Val Loss: 0.8341
Epoch 30/50, Train Loss: 0.8030, Val Loss: 0.8338
Epoch 31/50, Train Loss: 0.8027, Val Loss: 0.8344
Epoch 32/50, Train Loss: 0.8030, Val Loss: 0.8341
Epoch 33/50, Train Loss: 0.8028, Val Loss: 0.8342
Epoch 34/50, Train Loss: 0.8027, Val Loss: 0.8339
Epoch 35/50, Train Loss: 0.8025, Val Loss: 0.8342
Epoch 36/50, Train Loss: 0.8029, Val Loss: 0.8342
Epoch 37/50, Train Loss: 0.8028, Val Loss: 0.8342
Epoch 38/50, Train Loss: 0.8031, Val Loss: 0.8339
Epoch 39/50, Train Loss: 0.8031, Val Loss: 0.8342
Epoch 40/50, Train Loss: 0.8030, Val Loss: 0.8338
Epoch 41/50, Train Loss: 0.8025, Val Loss: 0.8338
Epoch 42/50, Train Loss: 0.8030, Val Loss: 0.8340
Epoch 43/50, Train Loss: 0.8030, Val Loss: 0.8340
Epoch 44/50, Train Loss: 0.8032, Val Loss: 0.8343
Epoch 45/50, Train Loss: 0.8031, Val Loss: 0.8340
Epoch 46/50, Train Loss: 0.8028, Val Loss: 0.8340
Epoch 47/50, Train Loss: 0.8027, Val Loss: 0.8339
Epoch 48/50, Train Loss: 0.8029, Val Loss: 0.8344
Epoch 49/50, Train Loss: 0.8029, Val Loss: 0.8341
Epoch 50/50, Train Loss: 0.8027, Val Loss: 0.8340
Metrics for Fold 6 (Train):
{'0.0': {'precision': 0.8895354966580218, 'recall': 0.8652883747641316, 'f1-score': 0.8772444194920099, 'support': 48756.0}, '1.0': {'precision': 0.8688629330138764, 'recall': 0.8925465583723029, 'f1-score': 0.8805455226070152, 'support': 48756.0}, 'accuracy': 0.8789174665682172, 'macro avg': {'precision': 0.8791992148359491, 'recall': 0.8789174665682172, 'f1-score': 0.8788949710495126, 'support': 97512.0}, 'weighted avg': {'precision': 0.879199214835949, 'recall': 0.8789174665682172, 'f1-score': 0.8788949710495126, 'support': 97512.0}}
Metrics for Fold 6 (Test):
{'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}}
Fold 7: Test Patient chb16
Fold 7: Distribution of classes
Training set: Class 0: 50588, Class 1: 50588
Test set: Class 0: 488, Class 1: 488

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8764, Val Loss: 0.9539
Epoch 2/50, Train Loss: 0.8589, Val Loss: 0.9173
Epoch 3/50, Train Loss: 0.8509, Val Loss: 0.9147
Epoch 4/50, Train Loss: 0.8459, Val Loss: 0.9070
Epoch 5/50, Train Loss: 0.8426, Val Loss: 0.8865
Epoch 6/50, Train Loss: 0.8385, Val Loss: 0.8939
Epoch 7/50, Train Loss: 0.8381, Val Loss: 0.9122
Epoch 8/50, Train Loss: 0.8367, Val Loss: 0.9147
Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.
Epoch 9/50, Train Loss: 0.8351, Val Loss: 0.9034
Epoch 10/50, Train Loss: 0.8194, Val Loss: 0.9163
Epoch 11/50, Train Loss: 0.8137, Val Loss: 0.9218
Epoch 12/50, Train Loss: 0.8120, Val Loss: 0.9072
Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.
Epoch 13/50, Train Loss: 0.8112, Val Loss: 0.9175
Epoch 14/50, Train Loss: 0.8078, Val Loss: 0.9219
Epoch 15/50, Train Loss: 0.8060, Val Loss: 0.9134
Epoch 16/50, Train Loss: 0.8059, Val Loss: 0.9214
Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.
Epoch 17/50, Train Loss: 0.8045, Val Loss: 0.9239
Epoch 18/50, Train Loss: 0.8044, Val Loss: 0.9213
Epoch 19/50, Train Loss: 0.8038, Val Loss: 0.9199
Epoch 20/50, Train Loss: 0.8038, Val Loss: 0.9196
Epoch 00021: reducing learning rate of group 0 to 1.0000e-06.
Epoch 21/50, Train Loss: 0.8038, Val Loss: 0.9193
Epoch 22/50, Train Loss: 0.8034, Val Loss: 0.9183
Epoch 23/50, Train Loss: 0.8039, Val Loss: 0.9193
Epoch 24/50, Train Loss: 0.8042, Val Loss: 0.9195
Epoch 00025: reducing learning rate of group 0 to 1.0000e-07.
Epoch 25/50, Train Loss: 0.8035, Val Loss: 0.9192
Epoch 26/50, Train Loss: 0.8033, Val Loss: 0.9198
Epoch 27/50, Train Loss: 0.8034, Val Loss: 0.9196
Epoch 28/50, Train Loss: 0.8037, Val Loss: 0.9201
Epoch 00029: reducing learning rate of group 0 to 1.0000e-08.
Epoch 29/50, Train Loss: 0.8038, Val Loss: 0.9192
Epoch 30/50, Train Loss: 0.8034, Val Loss: 0.9184
Epoch 31/50, Train Loss: 0.8037, Val Loss: 0.9203
Epoch 32/50, Train Loss: 0.8039, Val Loss: 0.9195
Epoch 33/50, Train Loss: 0.8035, Val Loss: 0.9202
Epoch 34/50, Train Loss: 0.8035, Val Loss: 0.9204
Epoch 35/50, Train Loss: 0.8034, Val Loss: 0.9189
Epoch 36/50, Train Loss: 0.8037, Val Loss: 0.9189
Epoch 37/50, Train Loss: 0.8035, Val Loss: 0.9203
Epoch 38/50, Train Loss: 0.8036, Val Loss: 0.9194
Epoch 39/50, Train Loss: 0.8036, Val Loss: 0.9201
Epoch 40/50, Train Loss: 0.8035, Val Loss: 0.9203
Epoch 41/50, Train Loss: 0.8033, Val Loss: 0.9207
Epoch 42/50, Train Loss: 0.8036, Val Loss: 0.9197
Epoch 43/50, Train Loss: 0.8036, Val Loss: 0.9191
Epoch 44/50, Train Loss: 0.8040, Val Loss: 0.9194
Epoch 45/50, Train Loss: 0.8032, Val Loss: 0.9200
Epoch 46/50, Train Loss: 0.8041, Val Loss: 0.9194
Epoch 47/50, Train Loss: 0.8030, Val Loss: 0.9190
Epoch 48/50, Train Loss: 0.8036, Val Loss: 0.9198
Epoch 49/50, Train Loss: 0.8035, Val Loss: 0.9201
Epoch 50/50, Train Loss: 0.8036, Val Loss: 0.9207
Epoch 1/50, Train Loss: 0.8037, Val Loss: 0.9190
Epoch 2/50, Train Loss: 0.8037, Val Loss: 0.9199
Epoch 3/50, Train Loss: 0.8036, Val Loss: 0.9193
Epoch 4/50, Train Loss: 0.8037, Val Loss: 0.9193
Epoch 5/50, Train Loss: 0.8038, Val Loss: 0.9195
Epoch 6/50, Train Loss: 0.8036, Val Loss: 0.9191
Epoch 7/50, Train Loss: 0.8037, Val Loss: 0.9195
Epoch 8/50, Train Loss: 0.8039, Val Loss: 0.9197
Epoch 9/50, Train Loss: 0.8036, Val Loss: 0.9184
Epoch 10/50, Train Loss: 0.8036, Val Loss: 0.9203
Epoch 11/50, Train Loss: 0.8036, Val Loss: 0.9208
Epoch 12/50, Train Loss: 0.8038, Val Loss: 0.9194
Epoch 13/50, Train Loss: 0.8037, Val Loss: 0.9194
Epoch 14/50, Train Loss: 0.8037, Val Loss: 0.9182
Epoch 15/50, Train Loss: 0.8037, Val Loss: 0.9197
Epoch 16/50, Train Loss: 0.8038, Val Loss: 0.9198
Epoch 17/50, Train Loss: 0.8035, Val Loss: 0.9196
Epoch 18/50, Train Loss: 0.8036, Val Loss: 0.9184
Epoch 19/50, Train Loss: 0.8035, Val Loss: 0.9202
Epoch 20/50, Train Loss: 0.8038, Val Loss: 0.9194
Epoch 21/50, Train Loss: 0.8037, Val Loss: 0.9198
Epoch 22/50, Train Loss: 0.8033, Val Loss: 0.9190
Epoch 23/50, Train Loss: 0.8036, Val Loss: 0.9196
Epoch 24/50, Train Loss: 0.8037, Val Loss: 0.9202
Epoch 25/50, Train Loss: 0.8038, Val Loss: 0.9205
Epoch 26/50, Train Loss: 0.8035, Val Loss: 0.9205
Epoch 27/50, Train Loss: 0.8038, Val Loss: 0.9200
Epoch 28/50, Train Loss: 0.8037, Val Loss: 0.9200
Epoch 29/50, Train Loss: 0.8035, Val Loss: 0.9200
Epoch 30/50, Train Loss: 0.8037, Val Loss: 0.9196
Epoch 31/50, Train Loss: 0.8036, Val Loss: 0.9207
Epoch 32/50, Train Loss: 0.8033, Val Loss: 0.9200
Epoch 33/50, Train Loss: 0.8039, Val Loss: 0.9198
Epoch 34/50, Train Loss: 0.8034, Val Loss: 0.9196
Epoch 35/50, Train Loss: 0.8040, Val Loss: 0.9193
Epoch 36/50, Train Loss: 0.8034, Val Loss: 0.9187
Epoch 37/50, Train Loss: 0.8042, Val Loss: 0.9198
Epoch 38/50, Train Loss: 0.8038, Val Loss: 0.9204
Epoch 39/50, Train Loss: 0.8036, Val Loss: 0.9193
Epoch 40/50, Train Loss: 0.8038, Val Loss: 0.9200
Epoch 41/50, Train Loss: 0.8036, Val Loss: 0.9200
Epoch 42/50, Train Loss: 0.8035, Val Loss: 0.9197
Epoch 43/50, Train Loss: 0.8033, Val Loss: 0.9204
Epoch 44/50, Train Loss: 0.8034, Val Loss: 0.9196
Epoch 45/50, Train Loss: 0.8038, Val Loss: 0.9195
Epoch 46/50, Train Loss: 0.8036, Val Loss: 0.9207
Epoch 47/50, Train Loss: 0.8037, Val Loss: 0.9198
Epoch 48/50, Train Loss: 0.8038, Val Loss: 0.9201
Epoch 49/50, Train Loss: 0.8036, Val Loss: 0.9208
Epoch 50/50, Train Loss: 0.8038, Val Loss: 0.9189
Metrics for Fold 7 (Train):
{'0.0': {'precision': 0.8946383783561531, 'recall': 0.8602237684826441, 'f1-score': 0.877093620880782, 'support': 50588.0}, '1.0': {'precision': 0.8654014542962652, 'recall': 0.8986913892622757, 'f1-score': 0.8817323170613448, 'support': 50588.0}, 'accuracy': 0.8794575788724599, 'macro avg': {'precision': 0.8800199163262092, 'recall': 0.8794575788724599, 'f1-score': 0.8794129689710635, 'support': 101176.0}, 'weighted avg': {'precision': 0.8800199163262092, 'recall': 0.8794575788724599, 'f1-score': 0.8794129689710635, 'support': 101176.0}}
Metrics for Fold 7 (Test):
{'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}}
Fold 8: Test Patient chb03
Fold 8: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8919, Val Loss: 0.9521
Epoch 2/50, Train Loss: 0.8661, Val Loss: 0.9035
Epoch 3/50, Train Loss: 0.8565, Val Loss: 0.8789
Epoch 4/50, Train Loss: 0.8515, Val Loss: 0.8891
Epoch 5/50, Train Loss: 0.8481, Val Loss: 0.8724
Epoch 6/50, Train Loss: 0.8466, Val Loss: 0.8505
Epoch 7/50, Train Loss: 0.8469, Val Loss: 0.8834
Epoch 8/50, Train Loss: 0.8452, Val Loss: 0.8648
Epoch 9/50, Train Loss: 0.8426, Val Loss: 0.8700
Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.
Epoch 10/50, Train Loss: 0.8455, Val Loss: 0.9195
Epoch 11/50, Train Loss: 0.8332, Val Loss: 0.8531
Epoch 12/50, Train Loss: 0.8251, Val Loss: 0.8539
Epoch 13/50, Train Loss: 0.8235, Val Loss: 0.8356
Epoch 14/50, Train Loss: 0.8216, Val Loss: 0.8374
Epoch 15/50, Train Loss: 0.8212, Val Loss: 0.8338
Epoch 16/50, Train Loss: 0.8214, Val Loss: 0.8203
Epoch 17/50, Train Loss: 0.8197, Val Loss: 0.8469
Epoch 18/50, Train Loss: 0.8193, Val Loss: 0.8313
Epoch 19/50, Train Loss: 0.8183, Val Loss: 0.8532
Epoch 20/50, Train Loss: 0.8179, Val Loss: 0.8142
Epoch 21/50, Train Loss: 0.8156, Val Loss: 0.8245
Epoch 22/50, Train Loss: 0.8148, Val Loss: 0.8137
Epoch 23/50, Train Loss: 0.8145, Val Loss: 0.8305
Epoch 24/50, Train Loss: 0.8136, Val Loss: 0.8305
Epoch 25/50, Train Loss: 0.8133, Val Loss: 0.8419
Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.
Epoch 26/50, Train Loss: 0.8125, Val Loss: 0.8571
Epoch 27/50, Train Loss: 0.8084, Val Loss: 0.8262
Epoch 28/50, Train Loss: 0.8075, Val Loss: 0.8273
Epoch 29/50, Train Loss: 0.8070, Val Loss: 0.8263
Epoch 00030: reducing learning rate of group 0 to 1.0000e-05.
Epoch 30/50, Train Loss: 0.8059, Val Loss: 0.8313
Epoch 31/50, Train Loss: 0.8059, Val Loss: 0.8220
Epoch 32/50, Train Loss: 0.8055, Val Loss: 0.8231
Epoch 33/50, Train Loss: 0.8056, Val Loss: 0.8241
Epoch 00034: reducing learning rate of group 0 to 1.0000e-06.
Epoch 34/50, Train Loss: 0.8055, Val Loss: 0.8225
Epoch 35/50, Train Loss: 0.8055, Val Loss: 0.8229
Epoch 36/50, Train Loss: 0.8052, Val Loss: 0.8226
Epoch 37/50, Train Loss: 0.8051, Val Loss: 0.8225
Epoch 00038: reducing learning rate of group 0 to 1.0000e-07.
Epoch 38/50, Train Loss: 0.8055, Val Loss: 0.8211
Epoch 39/50, Train Loss: 0.8053, Val Loss: 0.8229
Epoch 40/50, Train Loss: 0.8052, Val Loss: 0.8208
Epoch 41/50, Train Loss: 0.8052, Val Loss: 0.8225
Epoch 00042: reducing learning rate of group 0 to 1.0000e-08.
Epoch 42/50, Train Loss: 0.8051, Val Loss: 0.8212
Epoch 43/50, Train Loss: 0.8057, Val Loss: 0.8210
Epoch 44/50, Train Loss: 0.8055, Val Loss: 0.8230
Epoch 45/50, Train Loss: 0.8048, Val Loss: 0.8211
Epoch 46/50, Train Loss: 0.8048, Val Loss: 0.8239
Epoch 47/50, Train Loss: 0.8051, Val Loss: 0.8233
Epoch 48/50, Train Loss: 0.8053, Val Loss: 0.8229
Epoch 49/50, Train Loss: 0.8053, Val Loss: 0.8227
Epoch 50/50, Train Loss: 0.8051, Val Loss: 0.8229
Epoch 1/50, Train Loss: 0.8054, Val Loss: 0.8231
Epoch 2/50, Train Loss: 0.8052, Val Loss: 0.8219
Epoch 3/50, Train Loss: 0.8050, Val Loss: 0.8227
Epoch 4/50, Train Loss: 0.8053, Val Loss: 0.8215
Epoch 5/50, Train Loss: 0.8056, Val Loss: 0.8221
Epoch 6/50, Train Loss: 0.8053, Val Loss: 0.8212
Epoch 7/50, Train Loss: 0.8049, Val Loss: 0.8227
Epoch 8/50, Train Loss: 0.8054, Val Loss: 0.8228
Epoch 9/50, Train Loss: 0.8052, Val Loss: 0.8219
Epoch 10/50, Train Loss: 0.8051, Val Loss: 0.8216
Epoch 11/50, Train Loss: 0.8050, Val Loss: 0.8219
Epoch 12/50, Train Loss: 0.8053, Val Loss: 0.8229
Epoch 13/50, Train Loss: 0.8053, Val Loss: 0.8228
Epoch 14/50, Train Loss: 0.8049, Val Loss: 0.8228
Epoch 15/50, Train Loss: 0.8047, Val Loss: 0.8233
Epoch 16/50, Train Loss: 0.8054, Val Loss: 0.8222
Epoch 17/50, Train Loss: 0.8048, Val Loss: 0.8221
Epoch 18/50, Train Loss: 0.8053, Val Loss: 0.8231
Epoch 19/50, Train Loss: 0.8051, Val Loss: 0.8221
Epoch 20/50, Train Loss: 0.8049, Val Loss: 0.8217
Epoch 21/50, Train Loss: 0.8049, Val Loss: 0.8234
Epoch 22/50, Train Loss: 0.8053, Val Loss: 0.8223
Epoch 23/50, Train Loss: 0.8053, Val Loss: 0.8221
Epoch 24/50, Train Loss: 0.8049, Val Loss: 0.8228
Epoch 25/50, Train Loss: 0.8051, Val Loss: 0.8228
Epoch 26/50, Train Loss: 0.8056, Val Loss: 0.8215
Epoch 27/50, Train Loss: 0.8058, Val Loss: 0.8223
Epoch 28/50, Train Loss: 0.8055, Val Loss: 0.8221
Epoch 29/50, Train Loss: 0.8055, Val Loss: 0.8223
Epoch 30/50, Train Loss: 0.8054, Val Loss: 0.8221
Epoch 31/50, Train Loss: 0.8050, Val Loss: 0.8217
Epoch 32/50, Train Loss: 0.8054, Val Loss: 0.8227
Epoch 33/50, Train Loss: 0.8050, Val Loss: 0.8215
Epoch 34/50, Train Loss: 0.8049, Val Loss: 0.8226
Epoch 35/50, Train Loss: 0.8052, Val Loss: 0.8230
Epoch 36/50, Train Loss: 0.8054, Val Loss: 0.8226
Epoch 37/50, Train Loss: 0.8053, Val Loss: 0.8222
Epoch 38/50, Train Loss: 0.8050, Val Loss: 0.8219
Epoch 39/50, Train Loss: 0.8051, Val Loss: 0.8228
Epoch 40/50, Train Loss: 0.8054, Val Loss: 0.8227
Epoch 41/50, Train Loss: 0.8051, Val Loss: 0.8227
Epoch 42/50, Train Loss: 0.8054, Val Loss: 0.8231
Epoch 43/50, Train Loss: 0.8050, Val Loss: 0.8236
Epoch 44/50, Train Loss: 0.8049, Val Loss: 0.8235
Epoch 45/50, Train Loss: 0.8052, Val Loss: 0.8224
Epoch 46/50, Train Loss: 0.8053, Val Loss: 0.8232
Epoch 47/50, Train Loss: 0.8052, Val Loss: 0.8229
Epoch 48/50, Train Loss: 0.8048, Val Loss: 0.8240
Epoch 49/50, Train Loss: 0.8049, Val Loss: 0.8221
Epoch 50/50, Train Loss: 0.8054, Val Loss: 0.8209
Metrics for Fold 8 (Train):
{'0.0': {'precision': 0.8970432604047429, 'recall': 0.8550312911725956, 'f1-score': 0.8755335855897637, 'support': 48576.0}, '1.0': {'precision': 0.8615169809836581, 'recall': 0.9018651185770751, 'f1-score': 0.8812294447182354, 'support': 48576.0}, 'accuracy': 0.8784482048748353, 'macro avg': {'precision': 0.8792801206942005, 'recall': 0.8784482048748353, 'f1-score': 0.8783815151539995, 'support': 97152.0}, 'weighted avg': {'precision': 0.8792801206942007, 'recall': 0.8784482048748353, 'f1-score': 0.8783815151539994, 'support': 97152.0}}
Metrics for Fold 8 (Test):
{'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}}
Fold 9: Test Patient chb21
Fold 9: Distribution of classes
Training set: Class 0: 49516, Class 1: 49516
Test set: Class 0: 1560, Class 1: 1560

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8822, Val Loss: 0.9500
Epoch 2/50, Train Loss: 0.8606, Val Loss: 0.9216
Epoch 3/50, Train Loss: 0.8524, Val Loss: 0.9101
Epoch 4/50, Train Loss: 0.8478, Val Loss: 0.9153
Epoch 5/50, Train Loss: 0.8449, Val Loss: 0.9407
Epoch 6/50, Train Loss: 0.8417, Val Loss: 0.9223
Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.
Epoch 7/50, Train Loss: 0.8398, Val Loss: 0.9294
Epoch 8/50, Train Loss: 0.8195, Val Loss: 0.9013
Epoch 9/50, Train Loss: 0.8122, Val Loss: 0.8875
Epoch 10/50, Train Loss: 0.8103, Val Loss: 0.9169
Epoch 11/50, Train Loss: 0.8078, Val Loss: 0.9074
Epoch 12/50, Train Loss: 0.8061, Val Loss: 0.8966
Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.
Epoch 13/50, Train Loss: 0.8059, Val Loss: 0.9154
Epoch 14/50, Train Loss: 0.8008, Val Loss: 0.9032
Epoch 15/50, Train Loss: 0.7994, Val Loss: 0.8956
Epoch 16/50, Train Loss: 0.7986, Val Loss: 0.8969
Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.
Epoch 17/50, Train Loss: 0.7986, Val Loss: 0.8993
Epoch 18/50, Train Loss: 0.7977, Val Loss: 0.9011
Epoch 19/50, Train Loss: 0.7972, Val Loss: 0.8984
Epoch 20/50, Train Loss: 0.7970, Val Loss: 0.8979
Epoch 00021: reducing learning rate of group 0 to 1.0000e-06.
Epoch 21/50, Train Loss: 0.7971, Val Loss: 0.8985
Epoch 22/50, Train Loss: 0.7973, Val Loss: 0.8989
Epoch 23/50, Train Loss: 0.7971, Val Loss: 0.8992
Epoch 24/50, Train Loss: 0.7969, Val Loss: 0.8991
Epoch 00025: reducing learning rate of group 0 to 1.0000e-07.
Epoch 25/50, Train Loss: 0.7971, Val Loss: 0.8994
Epoch 26/50, Train Loss: 0.7968, Val Loss: 0.8988
Epoch 27/50, Train Loss: 0.7973, Val Loss: 0.8993
Epoch 28/50, Train Loss: 0.7972, Val Loss: 0.8990
Epoch 00029: reducing learning rate of group 0 to 1.0000e-08.
Epoch 29/50, Train Loss: 0.7973, Val Loss: 0.8994
Epoch 30/50, Train Loss: 0.7970, Val Loss: 0.8982
Epoch 31/50, Train Loss: 0.7975, Val Loss: 0.8991
Epoch 32/50, Train Loss: 0.7971, Val Loss: 0.8992
Epoch 33/50, Train Loss: 0.7971, Val Loss: 0.8998
Epoch 34/50, Train Loss: 0.7971, Val Loss: 0.9001
Epoch 35/50, Train Loss: 0.7973, Val Loss: 0.8991
Epoch 36/50, Train Loss: 0.7967, Val Loss: 0.8996
Epoch 37/50, Train Loss: 0.7967, Val Loss: 0.8982
Epoch 38/50, Train Loss: 0.7973, Val Loss: 0.8998
Epoch 39/50, Train Loss: 0.7972, Val Loss: 0.8978
Epoch 40/50, Train Loss: 0.7972, Val Loss: 0.8992
Epoch 41/50, Train Loss: 0.7973, Val Loss: 0.8989
Epoch 42/50, Train Loss: 0.7973, Val Loss: 0.8984
Epoch 43/50, Train Loss: 0.7976, Val Loss: 0.8987
Epoch 44/50, Train Loss: 0.7976, Val Loss: 0.8995
Epoch 45/50, Train Loss: 0.7972, Val Loss: 0.8984
Epoch 46/50, Train Loss: 0.7971, Val Loss: 0.8982
Epoch 47/50, Train Loss: 0.7971, Val Loss: 0.8989
Epoch 48/50, Train Loss: 0.7972, Val Loss: 0.8999
Epoch 49/50, Train Loss: 0.7970, Val Loss: 0.8994
Epoch 50/50, Train Loss: 0.7974, Val Loss: 0.8987
Epoch 1/50, Train Loss: 0.7972, Val Loss: 0.8992
Epoch 2/50, Train Loss: 0.7972, Val Loss: 0.8986
Epoch 3/50, Train Loss: 0.7972, Val Loss: 0.8986
Epoch 4/50, Train Loss: 0.7972, Val Loss: 0.8978
Epoch 5/50, Train Loss: 0.7973, Val Loss: 0.8985
Epoch 6/50, Train Loss: 0.7970, Val Loss: 0.8993
Epoch 7/50, Train Loss: 0.7968, Val Loss: 0.8982
Epoch 8/50, Train Loss: 0.7973, Val Loss: 0.8995
Epoch 9/50, Train Loss: 0.7971, Val Loss: 0.8987
Epoch 10/50, Train Loss: 0.7975, Val Loss: 0.8994
Epoch 11/50, Train Loss: 0.7973, Val Loss: 0.8986
Epoch 12/50, Train Loss: 0.7975, Val Loss: 0.8988
Epoch 13/50, Train Loss: 0.7975, Val Loss: 0.8990
Epoch 14/50, Train Loss: 0.7971, Val Loss: 0.8989
Epoch 15/50, Train Loss: 0.7975, Val Loss: 0.8987
Epoch 16/50, Train Loss: 0.7972, Val Loss: 0.8998
Epoch 17/50, Train Loss: 0.7973, Val Loss: 0.8991
Epoch 18/50, Train Loss: 0.7970, Val Loss: 0.8991
Epoch 19/50, Train Loss: 0.7968, Val Loss: 0.8990
Epoch 20/50, Train Loss: 0.7975, Val Loss: 0.8979
Epoch 21/50, Train Loss: 0.7972, Val Loss: 0.8991
Epoch 22/50, Train Loss: 0.7971, Val Loss: 0.8989
Epoch 23/50, Train Loss: 0.7973, Val Loss: 0.8985
Epoch 24/50, Train Loss: 0.7978, Val Loss: 0.8996
Epoch 25/50, Train Loss: 0.7969, Val Loss: 0.8982
Epoch 26/50, Train Loss: 0.7975, Val Loss: 0.8996
Epoch 27/50, Train Loss: 0.7971, Val Loss: 0.8990
Epoch 28/50, Train Loss: 0.7973, Val Loss: 0.8985
Epoch 29/50, Train Loss: 0.7971, Val Loss: 0.8978
Epoch 30/50, Train Loss: 0.7973, Val Loss: 0.8991
Epoch 31/50, Train Loss: 0.7972, Val Loss: 0.8985
Epoch 32/50, Train Loss: 0.7968, Val Loss: 0.8985
Epoch 33/50, Train Loss: 0.7974, Val Loss: 0.8990
Epoch 34/50, Train Loss: 0.7971, Val Loss: 0.8986
Epoch 35/50, Train Loss: 0.7972, Val Loss: 0.8989
Epoch 36/50, Train Loss: 0.7972, Val Loss: 0.8987
Epoch 37/50, Train Loss: 0.7970, Val Loss: 0.8991
Epoch 38/50, Train Loss: 0.7974, Val Loss: 0.8985
Epoch 39/50, Train Loss: 0.7968, Val Loss: 0.8988
Epoch 40/50, Train Loss: 0.7974, Val Loss: 0.8987
Epoch 41/50, Train Loss: 0.7974, Val Loss: 0.8996
Epoch 42/50, Train Loss: 0.7974, Val Loss: 0.8980
Epoch 43/50, Train Loss: 0.7970, Val Loss: 0.8992
Epoch 44/50, Train Loss: 0.7969, Val Loss: 0.8982
Epoch 45/50, Train Loss: 0.7971, Val Loss: 0.8995
Epoch 46/50, Train Loss: 0.7973, Val Loss: 0.8990
Epoch 47/50, Train Loss: 0.7973, Val Loss: 0.8990
Epoch 48/50, Train Loss: 0.7968, Val Loss: 0.8990
Epoch 49/50, Train Loss: 0.7976, Val Loss: 0.9000
Epoch 50/50, Train Loss: 0.7971, Val Loss: 0.8985
Metrics for Fold 9 (Train):
{'0.0': {'precision': 0.9013457644279447, 'recall': 0.8629735842959851, 'f1-score': 0.8817423961041642, 'support': 49516.0}, '1.0': {'precision': 0.8685688826902216, 'recall': 0.9055456822037321, 'f1-score': 0.8866719398853073, 'support': 49516.0}, 'accuracy': 0.8842596332498587, 'macro avg': {'precision': 0.8849573235590831, 'recall': 0.8842596332498587, 'f1-score': 0.8842071679947358, 'support': 99032.0}, 'weighted avg': {'precision': 0.8849573235590831, 'recall': 0.8842596332498587, 'f1-score': 0.8842071679947358, 'support': 99032.0}}
Metrics for Fold 9 (Test):
{'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}}
Fold 10: Test Patient chb14
Fold 10: Distribution of classes
Training set: Class 0: 49788, Class 1: 49788
Test set: Class 0: 1288, Class 1: 1288

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8712, Val Loss: 0.9964
Epoch 2/50, Train Loss: 0.8517, Val Loss: 0.9754
Epoch 3/50, Train Loss: 0.8446, Val Loss: 1.0475
Epoch 4/50, Train Loss: 0.8404, Val Loss: 0.9882
Epoch 5/50, Train Loss: 0.8354, Val Loss: 1.0151
Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.
Epoch 6/50, Train Loss: 0.8352, Val Loss: 1.0395
Epoch 7/50, Train Loss: 0.8188, Val Loss: 1.0427
Epoch 8/50, Train Loss: 0.8116, Val Loss: 1.0624
Epoch 9/50, Train Loss: 0.8092, Val Loss: 1.0412
Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.
Epoch 10/50, Train Loss: 0.8075, Val Loss: 1.0383
Epoch 11/50, Train Loss: 0.8032, Val Loss: 1.0436
Epoch 12/50, Train Loss: 0.8013, Val Loss: 1.0453
Epoch 13/50, Train Loss: 0.8009, Val Loss: 1.0376
Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.
Epoch 14/50, Train Loss: 0.8003, Val Loss: 1.0453
Epoch 15/50, Train Loss: 0.7996, Val Loss: 1.0398
Epoch 16/50, Train Loss: 0.7992, Val Loss: 1.0411
Epoch 17/50, Train Loss: 0.7993, Val Loss: 1.0409
Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.
Epoch 18/50, Train Loss: 0.7993, Val Loss: 1.0414
Epoch 19/50, Train Loss: 0.7993, Val Loss: 1.0415
Epoch 20/50, Train Loss: 0.7992, Val Loss: 1.0412
Epoch 21/50, Train Loss: 0.7993, Val Loss: 1.0415
Epoch 00022: reducing learning rate of group 0 to 1.0000e-07.
Epoch 22/50, Train Loss: 0.7991, Val Loss: 1.0419
Epoch 23/50, Train Loss: 0.7991, Val Loss: 1.0427
Epoch 24/50, Train Loss: 0.7991, Val Loss: 1.0416
Epoch 25/50, Train Loss: 0.7992, Val Loss: 1.0413
Epoch 00026: reducing learning rate of group 0 to 1.0000e-08.
Epoch 26/50, Train Loss: 0.7991, Val Loss: 1.0402
Epoch 27/50, Train Loss: 0.7993, Val Loss: 1.0418
Epoch 28/50, Train Loss: 0.7994, Val Loss: 1.0423
Epoch 29/50, Train Loss: 0.7994, Val Loss: 1.0401
Epoch 30/50, Train Loss: 0.7992, Val Loss: 1.0423
Epoch 31/50, Train Loss: 0.7991, Val Loss: 1.0412
Epoch 32/50, Train Loss: 0.7994, Val Loss: 1.0414
Epoch 33/50, Train Loss: 0.7991, Val Loss: 1.0399
Epoch 34/50, Train Loss: 0.7992, Val Loss: 1.0417
Epoch 35/50, Train Loss: 0.7990, Val Loss: 1.0422
Epoch 36/50, Train Loss: 0.7990, Val Loss: 1.0413
Epoch 37/50, Train Loss: 0.7995, Val Loss: 1.0411
Epoch 38/50, Train Loss: 0.7992, Val Loss: 1.0415
Epoch 39/50, Train Loss: 0.8001, Val Loss: 1.0414
Epoch 40/50, Train Loss: 0.7991, Val Loss: 1.0407
Epoch 41/50, Train Loss: 0.7994, Val Loss: 1.0411
Epoch 42/50, Train Loss: 0.7989, Val Loss: 1.0400
Epoch 43/50, Train Loss: 0.7993, Val Loss: 1.0419
Epoch 44/50, Train Loss: 0.7991, Val Loss: 1.0411
Epoch 45/50, Train Loss: 0.7993, Val Loss: 1.0426
Epoch 46/50, Train Loss: 0.7994, Val Loss: 1.0422
Epoch 47/50, Train Loss: 0.7994, Val Loss: 1.0415
Epoch 48/50, Train Loss: 0.7988, Val Loss: 1.0416
Epoch 49/50, Train Loss: 0.7990, Val Loss: 1.0421
Epoch 50/50, Train Loss: 0.7992, Val Loss: 1.0412
Epoch 1/50, Train Loss: 0.7990, Val Loss: 1.0421
Epoch 2/50, Train Loss: 0.7993, Val Loss: 1.0431
Epoch 3/50, Train Loss: 0.7992, Val Loss: 1.0424
Epoch 4/50, Train Loss: 0.7990, Val Loss: 1.0415
Epoch 5/50, Train Loss: 0.7990, Val Loss: 1.0403
Epoch 6/50, Train Loss: 0.7988, Val Loss: 1.0396
Epoch 7/50, Train Loss: 0.7992, Val Loss: 1.0409
Epoch 8/50, Train Loss: 0.7992, Val Loss: 1.0411
Epoch 9/50, Train Loss: 0.7991, Val Loss: 1.0417
Epoch 10/50, Train Loss: 0.7993, Val Loss: 1.0407
Epoch 11/50, Train Loss: 0.7989, Val Loss: 1.0422
Epoch 12/50, Train Loss: 0.7991, Val Loss: 1.0401
Epoch 13/50, Train Loss: 0.7993, Val Loss: 1.0405
Epoch 14/50, Train Loss: 0.7991, Val Loss: 1.0413
Epoch 15/50, Train Loss: 0.7994, Val Loss: 1.0418
Epoch 16/50, Train Loss: 0.7993, Val Loss: 1.0420
Epoch 17/50, Train Loss: 0.7993, Val Loss: 1.0421
Epoch 18/50, Train Loss: 0.7996, Val Loss: 1.0412
Epoch 19/50, Train Loss: 0.7997, Val Loss: 1.0416
Epoch 20/50, Train Loss: 0.7988, Val Loss: 1.0419
Epoch 21/50, Train Loss: 0.7988, Val Loss: 1.0406
Epoch 22/50, Train Loss: 0.7993, Val Loss: 1.0400
Epoch 23/50, Train Loss: 0.7991, Val Loss: 1.0407
Epoch 24/50, Train Loss: 0.7992, Val Loss: 1.0421
Epoch 25/50, Train Loss: 0.7989, Val Loss: 1.0427
Epoch 26/50, Train Loss: 0.7990, Val Loss: 1.0421
Epoch 27/50, Train Loss: 0.7989, Val Loss: 1.0410
Epoch 28/50, Train Loss: 0.7992, Val Loss: 1.0408
Epoch 29/50, Train Loss: 0.7990, Val Loss: 1.0428
Epoch 30/50, Train Loss: 0.7990, Val Loss: 1.0416
Epoch 31/50, Train Loss: 0.7991, Val Loss: 1.0407
Epoch 32/50, Train Loss: 0.7995, Val Loss: 1.0416
Epoch 33/50, Train Loss: 0.7993, Val Loss: 1.0408
Epoch 34/50, Train Loss: 0.7992, Val Loss: 1.0417
Epoch 35/50, Train Loss: 0.7993, Val Loss: 1.0417
Epoch 36/50, Train Loss: 0.7993, Val Loss: 1.0405
Epoch 37/50, Train Loss: 0.7991, Val Loss: 1.0409
Epoch 38/50, Train Loss: 0.7989, Val Loss: 1.0421
Epoch 39/50, Train Loss: 0.7994, Val Loss: 1.0426
Epoch 40/50, Train Loss: 0.7991, Val Loss: 1.0411
Epoch 41/50, Train Loss: 0.7992, Val Loss: 1.0415
Epoch 42/50, Train Loss: 0.7994, Val Loss: 1.0426
Epoch 43/50, Train Loss: 0.7995, Val Loss: 1.0426
Epoch 44/50, Train Loss: 0.7990, Val Loss: 1.0410
Epoch 45/50, Train Loss: 0.7993, Val Loss: 1.0412
Epoch 46/50, Train Loss: 0.7993, Val Loss: 1.0419
Epoch 47/50, Train Loss: 0.7992, Val Loss: 1.0414
Epoch 48/50, Train Loss: 0.7995, Val Loss: 1.0414
Epoch 49/50, Train Loss: 0.7989, Val Loss: 1.0412
Epoch 50/50, Train Loss: 0.7994, Val Loss: 1.0420
Metrics for Fold 10 (Train):
{'0.0': {'precision': 0.8952826895036248, 'recall': 0.8755925122519482, 'f1-score': 0.8853281343609428, 'support': 49788.0}, '1.0': {'precision': 0.8782697561071477, 'recall': 0.8975857636378244, 'f1-score': 0.8878227096184601, 'support': 49788.0}, 'accuracy': 0.8865891379448864, 'macro avg': {'precision': 0.8867762228053863, 'recall': 0.8865891379448863, 'f1-score': 0.8865754219897015, 'support': 99576.0}, 'weighted avg': {'precision': 0.8867762228053864, 'recall': 0.8865891379448864, 'f1-score': 0.8865754219897015, 'support': 99576.0}}
Metrics for Fold 10 (Test):
{'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}}
Fold 11: Test Patient chb19
Fold 11: Distribution of classes
Training set: Class 0: 49212, Class 1: 49212
Test set: Class 0: 1864, Class 1: 1864

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8771, Val Loss: 0.9261
Epoch 2/50, Train Loss: 0.8602, Val Loss: 0.8581
Epoch 3/50, Train Loss: 0.8529, Val Loss: 0.8546
Epoch 4/50, Train Loss: 0.8476, Val Loss: 0.8936
Epoch 5/50, Train Loss: 0.8484, Val Loss: 0.8888
Epoch 6/50, Train Loss: 0.8456, Val Loss: 0.9091
Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.
Epoch 7/50, Train Loss: 0.8447, Val Loss: 0.8831
Epoch 8/50, Train Loss: 0.8297, Val Loss: 0.8446
Epoch 9/50, Train Loss: 0.8226, Val Loss: 0.8632
Epoch 10/50, Train Loss: 0.8211, Val Loss: 0.8428
Epoch 11/50, Train Loss: 0.8205, Val Loss: 0.8402
Epoch 12/50, Train Loss: 0.8191, Val Loss: 0.8453
Epoch 13/50, Train Loss: 0.8171, Val Loss: 0.8316
Epoch 14/50, Train Loss: 0.8162, Val Loss: 0.8372
Epoch 15/50, Train Loss: 0.8161, Val Loss: 0.8382
Epoch 16/50, Train Loss: 0.8142, Val Loss: 0.8408
Epoch 17/50, Train Loss: 0.8128, Val Loss: 0.8297
Epoch 18/50, Train Loss: 0.8087, Val Loss: 0.8323
Epoch 19/50, Train Loss: 0.8068, Val Loss: 0.8363
Epoch 20/50, Train Loss: 0.8061, Val Loss: 0.8315
Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.
Epoch 21/50, Train Loss: 0.8052, Val Loss: 0.8349
Epoch 22/50, Train Loss: 0.7993, Val Loss: 0.8290
Epoch 23/50, Train Loss: 0.7977, Val Loss: 0.8269
Epoch 24/50, Train Loss: 0.7973, Val Loss: 0.8316
Epoch 25/50, Train Loss: 0.7963, Val Loss: 0.8299
Epoch 26/50, Train Loss: 0.7965, Val Loss: 0.8340
Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.
Epoch 27/50, Train Loss: 0.7956, Val Loss: 0.8309
Epoch 28/50, Train Loss: 0.7951, Val Loss: 0.8329
Epoch 29/50, Train Loss: 0.7949, Val Loss: 0.8330
Epoch 30/50, Train Loss: 0.7946, Val Loss: 0.8336
Epoch 00031: reducing learning rate of group 0 to 1.0000e-06.
Epoch 31/50, Train Loss: 0.7952, Val Loss: 0.8328
Epoch 32/50, Train Loss: 0.7950, Val Loss: 0.8330
Epoch 33/50, Train Loss: 0.7949, Val Loss: 0.8337
Epoch 34/50, Train Loss: 0.7950, Val Loss: 0.8321
Epoch 00035: reducing learning rate of group 0 to 1.0000e-07.
Epoch 35/50, Train Loss: 0.7947, Val Loss: 0.8345
Epoch 36/50, Train Loss: 0.7948, Val Loss: 0.8325
Epoch 37/50, Train Loss: 0.7947, Val Loss: 0.8325
Epoch 38/50, Train Loss: 0.7948, Val Loss: 0.8329
Epoch 00039: reducing learning rate of group 0 to 1.0000e-08.
Epoch 39/50, Train Loss: 0.7947, Val Loss: 0.8336
Epoch 40/50, Train Loss: 0.7953, Val Loss: 0.8334
Epoch 41/50, Train Loss: 0.7950, Val Loss: 0.8318
Epoch 42/50, Train Loss: 0.7942, Val Loss: 0.8322
Epoch 43/50, Train Loss: 0.7947, Val Loss: 0.8335
Epoch 44/50, Train Loss: 0.7950, Val Loss: 0.8330
Epoch 45/50, Train Loss: 0.7945, Val Loss: 0.8331
Epoch 46/50, Train Loss: 0.7949, Val Loss: 0.8324
Epoch 47/50, Train Loss: 0.7943, Val Loss: 0.8318
Epoch 48/50, Train Loss: 0.7949, Val Loss: 0.8315
Epoch 49/50, Train Loss: 0.7948, Val Loss: 0.8322
Epoch 50/50, Train Loss: 0.7946, Val Loss: 0.8316
Epoch 1/50, Train Loss: 0.7949, Val Loss: 0.8321
Epoch 2/50, Train Loss: 0.7949, Val Loss: 0.8341
Epoch 3/50, Train Loss: 0.7949, Val Loss: 0.8333
Epoch 4/50, Train Loss: 0.7949, Val Loss: 0.8329
Epoch 5/50, Train Loss: 0.7951, Val Loss: 0.8341
Epoch 6/50, Train Loss: 0.7944, Val Loss: 0.8324
Epoch 7/50, Train Loss: 0.7948, Val Loss: 0.8326
Epoch 8/50, Train Loss: 0.7949, Val Loss: 0.8313
Epoch 9/50, Train Loss: 0.7945, Val Loss: 0.8327
Epoch 10/50, Train Loss: 0.7947, Val Loss: 0.8318
Epoch 11/50, Train Loss: 0.7948, Val Loss: 0.8317
Epoch 12/50, Train Loss: 0.7952, Val Loss: 0.8326
Epoch 13/50, Train Loss: 0.7947, Val Loss: 0.8332
Epoch 14/50, Train Loss: 0.7948, Val Loss: 0.8327
Epoch 15/50, Train Loss: 0.7947, Val Loss: 0.8331
Epoch 16/50, Train Loss: 0.7948, Val Loss: 0.8328
Epoch 17/50, Train Loss: 0.7945, Val Loss: 0.8332
Epoch 18/50, Train Loss: 0.7948, Val Loss: 0.8328
Epoch 19/50, Train Loss: 0.7950, Val Loss: 0.8323
Epoch 20/50, Train Loss: 0.7949, Val Loss: 0.8321
Epoch 21/50, Train Loss: 0.7942, Val Loss: 0.8338
Epoch 22/50, Train Loss: 0.7951, Val Loss: 0.8326
Epoch 23/50, Train Loss: 0.7948, Val Loss: 0.8318
Epoch 24/50, Train Loss: 0.7949, Val Loss: 0.8322
Epoch 25/50, Train Loss: 0.7948, Val Loss: 0.8324
Epoch 26/50, Train Loss: 0.7949, Val Loss: 0.8334
Epoch 27/50, Train Loss: 0.7947, Val Loss: 0.8334
Epoch 28/50, Train Loss: 0.7949, Val Loss: 0.8338
Epoch 29/50, Train Loss: 0.7949, Val Loss: 0.8331
Epoch 30/50, Train Loss: 0.7944, Val Loss: 0.8325
Epoch 31/50, Train Loss: 0.7948, Val Loss: 0.8335
Epoch 32/50, Train Loss: 0.7951, Val Loss: 0.8337
Epoch 33/50, Train Loss: 0.7944, Val Loss: 0.8322
Epoch 34/50, Train Loss: 0.7943, Val Loss: 0.8323
Epoch 35/50, Train Loss: 0.7950, Val Loss: 0.8329
Epoch 36/50, Train Loss: 0.7944, Val Loss: 0.8333
Epoch 37/50, Train Loss: 0.7948, Val Loss: 0.8333
Epoch 38/50, Train Loss: 0.7947, Val Loss: 0.8323
Epoch 39/50, Train Loss: 0.7944, Val Loss: 0.8334
Epoch 40/50, Train Loss: 0.7953, Val Loss: 0.8341
Epoch 41/50, Train Loss: 0.7944, Val Loss: 0.8337
Epoch 42/50, Train Loss: 0.7952, Val Loss: 0.8327
Epoch 43/50, Train Loss: 0.7951, Val Loss: 0.8333
Epoch 44/50, Train Loss: 0.7946, Val Loss: 0.8324
Epoch 45/50, Train Loss: 0.7955, Val Loss: 0.8331
Epoch 46/50, Train Loss: 0.7948, Val Loss: 0.8330
Epoch 47/50, Train Loss: 0.7951, Val Loss: 0.8328
Epoch 48/50, Train Loss: 0.7952, Val Loss: 0.8331
Epoch 49/50, Train Loss: 0.7950, Val Loss: 0.8316
Epoch 50/50, Train Loss: 0.7946, Val Loss: 0.8328
Metrics for Fold 11 (Train):
{'0.0': {'precision': 0.9062291249164997, 'recall': 0.8821425668536129, 'f1-score': 0.8940236418303884, 'support': 49212.0}, '1.0': {'precision': 0.885193982581156, 'recall': 0.9087214500528327, 'f1-score': 0.8968034332009787, 'support': 49212.0}, 'accuracy': 0.8954320084532228, 'macro avg': {'precision': 0.8957115537488278, 'recall': 0.8954320084532228, 'f1-score': 0.8954135375156835, 'support': 98424.0}, 'weighted avg': {'precision': 0.8957115537488277, 'recall': 0.8954320084532228, 'f1-score': 0.8954135375156835, 'support': 98424.0}}
Metrics for Fold 11 (Test):
{'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}}
Fold 12: Test Patient chb24
Fold 12: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8865, Val Loss: 0.9588
Epoch 2/50, Train Loss: 0.8610, Val Loss: 0.9594
Epoch 3/50, Train Loss: 0.8549, Val Loss: 0.9081
Epoch 4/50, Train Loss: 0.8482, Val Loss: 0.9385
Epoch 5/50, Train Loss: 0.8394, Val Loss: 0.9212
Epoch 6/50, Train Loss: 0.8367, Val Loss: 0.9067
Epoch 7/50, Train Loss: 0.8356, Val Loss: 0.8804
Epoch 8/50, Train Loss: 0.8359, Val Loss: 1.0006
Epoch 9/50, Train Loss: 0.8343, Val Loss: 0.8982
Epoch 10/50, Train Loss: 0.8350, Val Loss: 0.9446
Epoch 00011: reducing learning rate of group 0 to 1.0000e-03.
Epoch 11/50, Train Loss: 0.8335, Val Loss: 0.9016
Epoch 12/50, Train Loss: 0.8165, Val Loss: 0.9265
Epoch 13/50, Train Loss: 0.8094, Val Loss: 0.9161
Epoch 14/50, Train Loss: 0.8071, Val Loss: 0.9328
Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.
Epoch 15/50, Train Loss: 0.8067, Val Loss: 0.9181
Epoch 16/50, Train Loss: 0.8026, Val Loss: 0.9085
Epoch 17/50, Train Loss: 0.8019, Val Loss: 0.9071
Epoch 18/50, Train Loss: 0.8008, Val Loss: 0.9155
Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.
Epoch 19/50, Train Loss: 0.8004, Val Loss: 0.9182
Epoch 20/50, Train Loss: 0.7999, Val Loss: 0.9169
Epoch 21/50, Train Loss: 0.7995, Val Loss: 0.9154
Epoch 22/50, Train Loss: 0.7997, Val Loss: 0.9133
Epoch 00023: reducing learning rate of group 0 to 1.0000e-06.
Epoch 23/50, Train Loss: 0.7994, Val Loss: 0.9149
Epoch 24/50, Train Loss: 0.7992, Val Loss: 0.9167
Epoch 25/50, Train Loss: 0.7997, Val Loss: 0.9146
Epoch 26/50, Train Loss: 0.7998, Val Loss: 0.9151
Epoch 00027: reducing learning rate of group 0 to 1.0000e-07.
Epoch 27/50, Train Loss: 0.7999, Val Loss: 0.9128
Epoch 28/50, Train Loss: 0.7994, Val Loss: 0.9145
Epoch 29/50, Train Loss: 0.7992, Val Loss: 0.9138
Epoch 30/50, Train Loss: 0.7996, Val Loss: 0.9142
Epoch 00031: reducing learning rate of group 0 to 1.0000e-08.
Epoch 31/50, Train Loss: 0.8001, Val Loss: 0.9139
Epoch 32/50, Train Loss: 0.7995, Val Loss: 0.9152
Epoch 33/50, Train Loss: 0.7990, Val Loss: 0.9139
Epoch 34/50, Train Loss: 0.7993, Val Loss: 0.9146
Epoch 35/50, Train Loss: 0.7996, Val Loss: 0.9148
Epoch 36/50, Train Loss: 0.7993, Val Loss: 0.9151
Epoch 37/50, Train Loss: 0.7992, Val Loss: 0.9149
Epoch 38/50, Train Loss: 0.7991, Val Loss: 0.9151
Epoch 39/50, Train Loss: 0.7994, Val Loss: 0.9152
Epoch 40/50, Train Loss: 0.7994, Val Loss: 0.9149
Epoch 41/50, Train Loss: 0.7998, Val Loss: 0.9136
Epoch 42/50, Train Loss: 0.7994, Val Loss: 0.9142
Epoch 43/50, Train Loss: 0.7995, Val Loss: 0.9150
Epoch 44/50, Train Loss: 0.7999, Val Loss: 0.9145
Epoch 45/50, Train Loss: 0.7998, Val Loss: 0.9138
Epoch 46/50, Train Loss: 0.7994, Val Loss: 0.9139
Epoch 47/50, Train Loss: 0.7993, Val Loss: 0.9136
Epoch 48/50, Train Loss: 0.7999, Val Loss: 0.9149
Epoch 49/50, Train Loss: 0.7995, Val Loss: 0.9149
Epoch 50/50, Train Loss: 0.7998, Val Loss: 0.9142
Epoch 1/50, Train Loss: 0.7994, Val Loss: 0.9140
Epoch 2/50, Train Loss: 0.7997, Val Loss: 0.9130
Epoch 3/50, Train Loss: 0.7992, Val Loss: 0.9144
Epoch 4/50, Train Loss: 0.7993, Val Loss: 0.9142
Epoch 5/50, Train Loss: 0.7998, Val Loss: 0.9145
Epoch 6/50, Train Loss: 0.7997, Val Loss: 0.9147
Epoch 7/50, Train Loss: 0.7998, Val Loss: 0.9155
Epoch 8/50, Train Loss: 0.8000, Val Loss: 0.9147
Epoch 9/50, Train Loss: 0.7998, Val Loss: 0.9147
Epoch 10/50, Train Loss: 0.7996, Val Loss: 0.9147
Epoch 11/50, Train Loss: 0.7993, Val Loss: 0.9147
Epoch 12/50, Train Loss: 0.7996, Val Loss: 0.9147
Epoch 13/50, Train Loss: 0.7995, Val Loss: 0.9149
Epoch 14/50, Train Loss: 0.7995, Val Loss: 0.9142
Epoch 15/50, Train Loss: 0.7995, Val Loss: 0.9154
Epoch 16/50, Train Loss: 0.7994, Val Loss: 0.9161
Epoch 17/50, Train Loss: 0.7996, Val Loss: 0.9149
Epoch 18/50, Train Loss: 0.7999, Val Loss: 0.9153
Epoch 19/50, Train Loss: 0.7996, Val Loss: 0.9151
Epoch 20/50, Train Loss: 0.7997, Val Loss: 0.9146
Epoch 21/50, Train Loss: 0.7996, Val Loss: 0.9138
Epoch 22/50, Train Loss: 0.7995, Val Loss: 0.9142
Epoch 23/50, Train Loss: 0.7995, Val Loss: 0.9157
Epoch 24/50, Train Loss: 0.7995, Val Loss: 0.9147
Epoch 25/50, Train Loss: 0.7994, Val Loss: 0.9141
Epoch 26/50, Train Loss: 0.7998, Val Loss: 0.9133
Epoch 27/50, Train Loss: 0.7997, Val Loss: 0.9137
Epoch 28/50, Train Loss: 0.7991, Val Loss: 0.9140
Epoch 29/50, Train Loss: 0.7992, Val Loss: 0.9142
Epoch 30/50, Train Loss: 0.7998, Val Loss: 0.9149
Epoch 31/50, Train Loss: 0.7994, Val Loss: 0.9144
Epoch 32/50, Train Loss: 0.7995, Val Loss: 0.9156
Epoch 33/50, Train Loss: 0.7995, Val Loss: 0.9143
Epoch 34/50, Train Loss: 0.7996, Val Loss: 0.9134
Epoch 35/50, Train Loss: 0.7999, Val Loss: 0.9143
Epoch 36/50, Train Loss: 0.7995, Val Loss: 0.9139
Epoch 37/50, Train Loss: 0.7994, Val Loss: 0.9146
Epoch 38/50, Train Loss: 0.8000, Val Loss: 0.9150
Epoch 39/50, Train Loss: 0.7995, Val Loss: 0.9158
Epoch 40/50, Train Loss: 0.7994, Val Loss: 0.9151
Epoch 41/50, Train Loss: 0.7997, Val Loss: 0.9143
Epoch 42/50, Train Loss: 0.7997, Val Loss: 0.9149
Epoch 43/50, Train Loss: 0.7995, Val Loss: 0.9135
Epoch 44/50, Train Loss: 0.7992, Val Loss: 0.9141
Epoch 45/50, Train Loss: 0.7994, Val Loss: 0.9146
Epoch 46/50, Train Loss: 0.7996, Val Loss: 0.9144
Epoch 47/50, Train Loss: 0.7999, Val Loss: 0.9138
Epoch 48/50, Train Loss: 0.7997, Val Loss: 0.9151
Epoch 49/50, Train Loss: 0.7993, Val Loss: 0.9130
Epoch 50/50, Train Loss: 0.7996, Val Loss: 0.9145
Metrics for Fold 12 (Train):
{'0.0': {'precision': 0.8994895452895069, 'recall': 0.866991930171278, 'f1-score': 0.8829418115873665, 'support': 48576.0}, '1.0': {'precision': 0.871629810653474, 'recall': 0.903120882740448, 'f1-score': 0.887095958830012, 'support': 48576.0}, 'accuracy': 0.885056406455863, 'macro avg': {'precision': 0.8855596779714905, 'recall': 0.885056406455863, 'f1-score': 0.8850188852086893, 'support': 97152.0}, 'weighted avg': {'precision': 0.8855596779714904, 'recall': 0.885056406455863, 'f1-score': 0.8850188852086892, 'support': 97152.0}}
Metrics for Fold 12 (Test):
{'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}}
Fold 13: Test Patient chb10
Fold 13: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8911, Val Loss: 0.8948
Epoch 2/50, Train Loss: 0.8671, Val Loss: 0.8673
Epoch 3/50, Train Loss: 0.8575, Val Loss: 0.8685
Epoch 4/50, Train Loss: 0.8527, Val Loss: 0.8359
Epoch 5/50, Train Loss: 0.8482, Val Loss: 0.8718
Epoch 6/50, Train Loss: 0.8473, Val Loss: 0.8793
Epoch 7/50, Train Loss: 0.8461, Val Loss: 0.8455
Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.
Epoch 8/50, Train Loss: 0.8456, Val Loss: 0.8588
Epoch 9/50, Train Loss: 0.8298, Val Loss: 0.8338
Epoch 10/50, Train Loss: 0.8228, Val Loss: 0.8173
Epoch 11/50, Train Loss: 0.8217, Val Loss: 0.8204
Epoch 12/50, Train Loss: 0.8207, Val Loss: 0.8263
Epoch 13/50, Train Loss: 0.8198, Val Loss: 0.8213
Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.
Epoch 14/50, Train Loss: 0.8189, Val Loss: 0.8204
Epoch 15/50, Train Loss: 0.8154, Val Loss: 0.8233
Epoch 16/50, Train Loss: 0.8139, Val Loss: 0.8231
Epoch 17/50, Train Loss: 0.8137, Val Loss: 0.8227
Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.
Epoch 18/50, Train Loss: 0.8136, Val Loss: 0.8258
Epoch 19/50, Train Loss: 0.8126, Val Loss: 0.8226
Epoch 20/50, Train Loss: 0.8123, Val Loss: 0.8223
Epoch 21/50, Train Loss: 0.8125, Val Loss: 0.8236
Epoch 00022: reducing learning rate of group 0 to 1.0000e-06.
Epoch 22/50, Train Loss: 0.8124, Val Loss: 0.8227
Epoch 23/50, Train Loss: 0.8124, Val Loss: 0.8224
Epoch 24/50, Train Loss: 0.8121, Val Loss: 0.8227
Epoch 25/50, Train Loss: 0.8127, Val Loss: 0.8228
Epoch 00026: reducing learning rate of group 0 to 1.0000e-07.
Epoch 26/50, Train Loss: 0.8121, Val Loss: 0.8227
Epoch 27/50, Train Loss: 0.8120, Val Loss: 0.8233
Epoch 28/50, Train Loss: 0.8123, Val Loss: 0.8223
Epoch 29/50, Train Loss: 0.8122, Val Loss: 0.8225
Epoch 00030: reducing learning rate of group 0 to 1.0000e-08.
Epoch 30/50, Train Loss: 0.8123, Val Loss: 0.8225
Epoch 31/50, Train Loss: 0.8122, Val Loss: 0.8233
Epoch 32/50, Train Loss: 0.8124, Val Loss: 0.8225
Epoch 33/50, Train Loss: 0.8123, Val Loss: 0.8223
Epoch 34/50, Train Loss: 0.8124, Val Loss: 0.8230
Epoch 35/50, Train Loss: 0.8121, Val Loss: 0.8231
Epoch 36/50, Train Loss: 0.8120, Val Loss: 0.8225
Epoch 37/50, Train Loss: 0.8123, Val Loss: 0.8228
Epoch 38/50, Train Loss: 0.8121, Val Loss: 0.8224
Epoch 39/50, Train Loss: 0.8124, Val Loss: 0.8226
Epoch 40/50, Train Loss: 0.8120, Val Loss: 0.8222
Epoch 41/50, Train Loss: 0.8124, Val Loss: 0.8219
Epoch 42/50, Train Loss: 0.8123, Val Loss: 0.8225
Epoch 43/50, Train Loss: 0.8119, Val Loss: 0.8228
Epoch 44/50, Train Loss: 0.8122, Val Loss: 0.8227
Epoch 45/50, Train Loss: 0.8125, Val Loss: 0.8225
Epoch 46/50, Train Loss: 0.8122, Val Loss: 0.8228
Epoch 47/50, Train Loss: 0.8125, Val Loss: 0.8232
Epoch 48/50, Train Loss: 0.8120, Val Loss: 0.8225
Epoch 49/50, Train Loss: 0.8119, Val Loss: 0.8224
Epoch 50/50, Train Loss: 0.8122, Val Loss: 0.8230
Epoch 1/50, Train Loss: 0.8123, Val Loss: 0.8232
Epoch 2/50, Train Loss: 0.8121, Val Loss: 0.8230
Epoch 3/50, Train Loss: 0.8123, Val Loss: 0.8232
Epoch 4/50, Train Loss: 0.8124, Val Loss: 0.8232
Epoch 5/50, Train Loss: 0.8124, Val Loss: 0.8224
Epoch 6/50, Train Loss: 0.8125, Val Loss: 0.8222
Epoch 7/50, Train Loss: 0.8119, Val Loss: 0.8225
Epoch 8/50, Train Loss: 0.8122, Val Loss: 0.8231
Epoch 9/50, Train Loss: 0.8121, Val Loss: 0.8235
Epoch 10/50, Train Loss: 0.8120, Val Loss: 0.8224
Epoch 11/50, Train Loss: 0.8120, Val Loss: 0.8222
Epoch 12/50, Train Loss: 0.8123, Val Loss: 0.8224
Epoch 13/50, Train Loss: 0.8121, Val Loss: 0.8227
Epoch 14/50, Train Loss: 0.8122, Val Loss: 0.8228
Epoch 15/50, Train Loss: 0.8124, Val Loss: 0.8234
Epoch 16/50, Train Loss: 0.8125, Val Loss: 0.8229
Epoch 17/50, Train Loss: 0.8123, Val Loss: 0.8227
Epoch 18/50, Train Loss: 0.8121, Val Loss: 0.8224
Epoch 19/50, Train Loss: 0.8121, Val Loss: 0.8230
Epoch 20/50, Train Loss: 0.8121, Val Loss: 0.8230
Epoch 21/50, Train Loss: 0.8124, Val Loss: 0.8221
Epoch 22/50, Train Loss: 0.8121, Val Loss: 0.8233
Epoch 23/50, Train Loss: 0.8121, Val Loss: 0.8223
Epoch 24/50, Train Loss: 0.8122, Val Loss: 0.8228
Epoch 25/50, Train Loss: 0.8123, Val Loss: 0.8225
Epoch 26/50, Train Loss: 0.8121, Val Loss: 0.8226
Epoch 27/50, Train Loss: 0.8123, Val Loss: 0.8225
Epoch 28/50, Train Loss: 0.8122, Val Loss: 0.8233
Epoch 29/50, Train Loss: 0.8121, Val Loss: 0.8221
Epoch 30/50, Train Loss: 0.8120, Val Loss: 0.8220
Epoch 31/50, Train Loss: 0.8119, Val Loss: 0.8222
Epoch 32/50, Train Loss: 0.8123, Val Loss: 0.8220
Epoch 33/50, Train Loss: 0.8122, Val Loss: 0.8223
Epoch 34/50, Train Loss: 0.8117, Val Loss: 0.8226
Epoch 35/50, Train Loss: 0.8120, Val Loss: 0.8230
Epoch 36/50, Train Loss: 0.8121, Val Loss: 0.8228
Epoch 37/50, Train Loss: 0.8122, Val Loss: 0.8224
Epoch 38/50, Train Loss: 0.8123, Val Loss: 0.8226
Epoch 39/50, Train Loss: 0.8125, Val Loss: 0.8228
Epoch 40/50, Train Loss: 0.8122, Val Loss: 0.8223
Epoch 41/50, Train Loss: 0.8122, Val Loss: 0.8225
Epoch 42/50, Train Loss: 0.8120, Val Loss: 0.8225
Epoch 43/50, Train Loss: 0.8124, Val Loss: 0.8227
Epoch 44/50, Train Loss: 0.8123, Val Loss: 0.8224
Epoch 45/50, Train Loss: 0.8121, Val Loss: 0.8225
Epoch 46/50, Train Loss: 0.8120, Val Loss: 0.8222
Epoch 47/50, Train Loss: 0.8125, Val Loss: 0.8222
Epoch 48/50, Train Loss: 0.8128, Val Loss: 0.8223
Epoch 49/50, Train Loss: 0.8129, Val Loss: 0.8224
Epoch 50/50, Train Loss: 0.8125, Val Loss: 0.8229
Metrics for Fold 13 (Train):
{'0.0': {'precision': 0.8986342810794221, 'recall': 0.8438941040843215, 'f1-score': 0.870404382491268, 'support': 48576.0}, '1.0': {'precision': 0.8528572814592025, 'recall': 0.9048089591567853, 'f1-score': 0.8780653474643145, 'support': 48576.0}, 'accuracy': 0.8743515316205533, 'macro avg': {'precision': 0.8757457812693124, 'recall': 0.8743515316205535, 'f1-score': 0.8742348649777912, 'support': 97152.0}, 'weighted avg': {'precision': 0.8757457812693122, 'recall': 0.8743515316205533, 'f1-score': 0.8742348649777912, 'support': 97152.0}}
Metrics for Fold 13 (Test):
{'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}}
Fold 14: Test Patient chb06
Fold 14: Distribution of classes
Training set: Class 0: 49932, Class 1: 49932
Test set: Class 0: 1144, Class 1: 1144

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8845, Val Loss: 0.9726
Epoch 2/50, Train Loss: 0.8590, Val Loss: 0.9551
Epoch 3/50, Train Loss: 0.8500, Val Loss: 0.9401
Epoch 4/50, Train Loss: 0.8462, Val Loss: 0.9587
Epoch 5/50, Train Loss: 0.8449, Val Loss: 0.9412
Epoch 6/50, Train Loss: 0.8405, Val Loss: 0.9481
Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.
Epoch 7/50, Train Loss: 0.8426, Val Loss: 0.9726
Epoch 8/50, Train Loss: 0.8263, Val Loss: 0.9684
Epoch 9/50, Train Loss: 0.8206, Val Loss: 0.9691
Epoch 10/50, Train Loss: 0.8193, Val Loss: 0.9566
Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.
Epoch 11/50, Train Loss: 0.8175, Val Loss: 0.9647
Epoch 12/50, Train Loss: 0.8133, Val Loss: 0.9605
Epoch 13/50, Train Loss: 0.8114, Val Loss: 0.9655
Epoch 14/50, Train Loss: 0.8109, Val Loss: 0.9611
Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.
Epoch 15/50, Train Loss: 0.8104, Val Loss: 0.9655
Epoch 16/50, Train Loss: 0.8094, Val Loss: 0.9615
Epoch 17/50, Train Loss: 0.8092, Val Loss: 0.9638
Epoch 18/50, Train Loss: 0.8095, Val Loss: 0.9617
Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.
Epoch 19/50, Train Loss: 0.8094, Val Loss: 0.9613
Epoch 20/50, Train Loss: 0.8093, Val Loss: 0.9604
Epoch 21/50, Train Loss: 0.8091, Val Loss: 0.9615
Epoch 22/50, Train Loss: 0.8093, Val Loss: 0.9615
Epoch 00023: reducing learning rate of group 0 to 1.0000e-07.
Epoch 23/50, Train Loss: 0.8091, Val Loss: 0.9612
Epoch 24/50, Train Loss: 0.8093, Val Loss: 0.9623
Epoch 25/50, Train Loss: 0.8095, Val Loss: 0.9628
Epoch 26/50, Train Loss: 0.8091, Val Loss: 0.9622
Epoch 00027: reducing learning rate of group 0 to 1.0000e-08.
Epoch 27/50, Train Loss: 0.8092, Val Loss: 0.9612
Epoch 28/50, Train Loss: 0.8093, Val Loss: 0.9619
Epoch 29/50, Train Loss: 0.8091, Val Loss: 0.9624
Epoch 30/50, Train Loss: 0.8094, Val Loss: 0.9628
Epoch 31/50, Train Loss: 0.8090, Val Loss: 0.9619
Epoch 32/50, Train Loss: 0.8093, Val Loss: 0.9622
Epoch 33/50, Train Loss: 0.8091, Val Loss: 0.9609
Epoch 34/50, Train Loss: 0.8094, Val Loss: 0.9619
Epoch 35/50, Train Loss: 0.8089, Val Loss: 0.9626
Epoch 36/50, Train Loss: 0.8094, Val Loss: 0.9637
Epoch 37/50, Train Loss: 0.8091, Val Loss: 0.9622
Epoch 38/50, Train Loss: 0.8090, Val Loss: 0.9622
Epoch 39/50, Train Loss: 0.8091, Val Loss: 0.9614
Epoch 40/50, Train Loss: 0.8092, Val Loss: 0.9617
Epoch 41/50, Train Loss: 0.8093, Val Loss: 0.9624
Epoch 42/50, Train Loss: 0.8091, Val Loss: 0.9620
Epoch 43/50, Train Loss: 0.8095, Val Loss: 0.9618
Epoch 44/50, Train Loss: 0.8091, Val Loss: 0.9619
Epoch 45/50, Train Loss: 0.8090, Val Loss: 0.9620
Epoch 46/50, Train Loss: 0.8091, Val Loss: 0.9631
Epoch 47/50, Train Loss: 0.8094, Val Loss: 0.9634
Epoch 48/50, Train Loss: 0.8092, Val Loss: 0.9616
Epoch 49/50, Train Loss: 0.8093, Val Loss: 0.9622
Epoch 50/50, Train Loss: 0.8096, Val Loss: 0.9631
Epoch 1/50, Train Loss: 0.8088, Val Loss: 0.9611
Epoch 2/50, Train Loss: 0.8094, Val Loss: 0.9622
Epoch 3/50, Train Loss: 0.8091, Val Loss: 0.9609
Epoch 4/50, Train Loss: 0.8092, Val Loss: 0.9617
Epoch 5/50, Train Loss: 0.8093, Val Loss: 0.9608
Epoch 6/50, Train Loss: 0.8093, Val Loss: 0.9621
Epoch 7/50, Train Loss: 0.8093, Val Loss: 0.9608
Epoch 8/50, Train Loss: 0.8092, Val Loss: 0.9609
Epoch 9/50, Train Loss: 0.8098, Val Loss: 0.9632
Epoch 10/50, Train Loss: 0.8092, Val Loss: 0.9614
Epoch 11/50, Train Loss: 0.8090, Val Loss: 0.9630
Epoch 12/50, Train Loss: 0.8092, Val Loss: 0.9617
Epoch 13/50, Train Loss: 0.8093, Val Loss: 0.9609
Epoch 14/50, Train Loss: 0.8093, Val Loss: 0.9621
Epoch 15/50, Train Loss: 0.8096, Val Loss: 0.9634
Epoch 16/50, Train Loss: 0.8090, Val Loss: 0.9609
Epoch 17/50, Train Loss: 0.8095, Val Loss: 0.9629
Epoch 18/50, Train Loss: 0.8091, Val Loss: 0.9642
Epoch 19/50, Train Loss: 0.8093, Val Loss: 0.9632
Epoch 20/50, Train Loss: 0.8093, Val Loss: 0.9620
Epoch 21/50, Train Loss: 0.8095, Val Loss: 0.9619
Epoch 22/50, Train Loss: 0.8095, Val Loss: 0.9603
Epoch 23/50, Train Loss: 0.8095, Val Loss: 0.9607
Epoch 24/50, Train Loss: 0.8095, Val Loss: 0.9632
Epoch 25/50, Train Loss: 0.8093, Val Loss: 0.9625
Epoch 26/50, Train Loss: 0.8091, Val Loss: 0.9626
Epoch 27/50, Train Loss: 0.8089, Val Loss: 0.9617
Epoch 28/50, Train Loss: 0.8094, Val Loss: 0.9622
Epoch 29/50, Train Loss: 0.8095, Val Loss: 0.9626
Epoch 30/50, Train Loss: 0.8096, Val Loss: 0.9615
Epoch 31/50, Train Loss: 0.8088, Val Loss: 0.9622
Epoch 32/50, Train Loss: 0.8093, Val Loss: 0.9612
Epoch 33/50, Train Loss: 0.8092, Val Loss: 0.9622
Epoch 34/50, Train Loss: 0.8093, Val Loss: 0.9615
Epoch 35/50, Train Loss: 0.8092, Val Loss: 0.9617
Epoch 36/50, Train Loss: 0.8091, Val Loss: 0.9626
Epoch 37/50, Train Loss: 0.8091, Val Loss: 0.9606
Epoch 38/50, Train Loss: 0.8094, Val Loss: 0.9622
Epoch 39/50, Train Loss: 0.8095, Val Loss: 0.9609
Epoch 40/50, Train Loss: 0.8092, Val Loss: 0.9612
Epoch 41/50, Train Loss: 0.8092, Val Loss: 0.9622
Epoch 42/50, Train Loss: 0.8090, Val Loss: 0.9618
Epoch 43/50, Train Loss: 0.8097, Val Loss: 0.9627
Epoch 44/50, Train Loss: 0.8094, Val Loss: 0.9613
Epoch 45/50, Train Loss: 0.8093, Val Loss: 0.9616
Epoch 46/50, Train Loss: 0.8095, Val Loss: 0.9623
Epoch 47/50, Train Loss: 0.8091, Val Loss: 0.9604
Epoch 48/50, Train Loss: 0.8092, Val Loss: 0.9610
Epoch 49/50, Train Loss: 0.8091, Val Loss: 0.9633
Epoch 50/50, Train Loss: 0.8089, Val Loss: 0.9634
Metrics for Fold 14 (Train):
{'0.0': {'precision': 0.8925049515271553, 'recall': 0.8573459905471441, 'f1-score': 0.8745722545123957, 'support': 49932.0}, '1.0': {'precision': 0.8627526541937224, 'recall': 0.8967395658095009, 'f1-score': 0.8794178590016791, 'support': 49932.0}, 'accuracy': 0.8770427781783225, 'macro avg': {'precision': 0.8776288028604389, 'recall': 0.8770427781783225, 'f1-score': 0.8769950567570375, 'support': 99864.0}, 'weighted avg': {'precision': 0.8776288028604389, 'recall': 0.8770427781783225, 'f1-score': 0.8769950567570374, 'support': 99864.0}}
Metrics for Fold 14 (Test):
{'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}}
Fold 15: Test Patient chb01
Fold 15: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8900, Val Loss: 0.9096
Epoch 2/50, Train Loss: 0.8681, Val Loss: 0.8692
Epoch 3/50, Train Loss: 0.8591, Val Loss: 0.8437
Epoch 4/50, Train Loss: 0.8518, Val Loss: 0.9113
Epoch 5/50, Train Loss: 0.8489, Val Loss: 0.9089
Epoch 6/50, Train Loss: 0.8452, Val Loss: 0.8913
Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.
Epoch 7/50, Train Loss: 0.8448, Val Loss: 0.9078
Epoch 8/50, Train Loss: 0.8284, Val Loss: 0.8382
Epoch 9/50, Train Loss: 0.8178, Val Loss: 0.8443
Epoch 10/50, Train Loss: 0.8155, Val Loss: 0.8577
Epoch 11/50, Train Loss: 0.8129, Val Loss: 0.8598
Epoch 12/50, Train Loss: 0.8111, Val Loss: 0.8357
Epoch 13/50, Train Loss: 0.8101, Val Loss: 0.8511
Epoch 14/50, Train Loss: 0.8076, Val Loss: 0.8545
Epoch 15/50, Train Loss: 0.8068, Val Loss: 0.8439
Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.
Epoch 16/50, Train Loss: 0.8056, Val Loss: 0.8596
Epoch 17/50, Train Loss: 0.8004, Val Loss: 0.8470
Epoch 18/50, Train Loss: 0.7991, Val Loss: 0.8473
Epoch 19/50, Train Loss: 0.7982, Val Loss: 0.8493
Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.
Epoch 20/50, Train Loss: 0.7977, Val Loss: 0.8478
Epoch 21/50, Train Loss: 0.7970, Val Loss: 0.8482
Epoch 22/50, Train Loss: 0.7971, Val Loss: 0.8486
Epoch 23/50, Train Loss: 0.7974, Val Loss: 0.8491
Epoch 00024: reducing learning rate of group 0 to 1.0000e-06.
Epoch 24/50, Train Loss: 0.7972, Val Loss: 0.8491
Epoch 25/50, Train Loss: 0.7971, Val Loss: 0.8486
Epoch 26/50, Train Loss: 0.7970, Val Loss: 0.8488
Epoch 27/50, Train Loss: 0.7969, Val Loss: 0.8492
Epoch 00028: reducing learning rate of group 0 to 1.0000e-07.
Epoch 28/50, Train Loss: 0.7968, Val Loss: 0.8490
Epoch 29/50, Train Loss: 0.7969, Val Loss: 0.8489
Epoch 30/50, Train Loss: 0.7966, Val Loss: 0.8490
Epoch 31/50, Train Loss: 0.7969, Val Loss: 0.8488
Epoch 00032: reducing learning rate of group 0 to 1.0000e-08.
Epoch 32/50, Train Loss: 0.7971, Val Loss: 0.8489
Epoch 33/50, Train Loss: 0.7970, Val Loss: 0.8488
Epoch 34/50, Train Loss: 0.7970, Val Loss: 0.8487
Epoch 35/50, Train Loss: 0.7968, Val Loss: 0.8486
Epoch 36/50, Train Loss: 0.7972, Val Loss: 0.8490
Epoch 37/50, Train Loss: 0.7968, Val Loss: 0.8488
Epoch 38/50, Train Loss: 0.7968, Val Loss: 0.8494
Epoch 39/50, Train Loss: 0.7971, Val Loss: 0.8491
Epoch 40/50, Train Loss: 0.7969, Val Loss: 0.8487
Epoch 41/50, Train Loss: 0.7970, Val Loss: 0.8490
Epoch 42/50, Train Loss: 0.7967, Val Loss: 0.8492
Epoch 43/50, Train Loss: 0.7973, Val Loss: 0.8490
Epoch 44/50, Train Loss: 0.7968, Val Loss: 0.8489
Epoch 45/50, Train Loss: 0.7969, Val Loss: 0.8483
Epoch 46/50, Train Loss: 0.7969, Val Loss: 0.8487
Epoch 47/50, Train Loss: 0.7967, Val Loss: 0.8490
Epoch 48/50, Train Loss: 0.7969, Val Loss: 0.8487
Epoch 49/50, Train Loss: 0.7972, Val Loss: 0.8488
Epoch 50/50, Train Loss: 0.7967, Val Loss: 0.8492
Epoch 1/50, Train Loss: 0.7968, Val Loss: 0.8490
Epoch 2/50, Train Loss: 0.7969, Val Loss: 0.8490
Epoch 3/50, Train Loss: 0.7969, Val Loss: 0.8489
Epoch 4/50, Train Loss: 0.7968, Val Loss: 0.8487
Epoch 5/50, Train Loss: 0.7966, Val Loss: 0.8489
Epoch 6/50, Train Loss: 0.7971, Val Loss: 0.8490
Epoch 7/50, Train Loss: 0.7967, Val Loss: 0.8492
Epoch 8/50, Train Loss: 0.7971, Val Loss: 0.8488
Epoch 9/50, Train Loss: 0.7971, Val Loss: 0.8487
Epoch 10/50, Train Loss: 0.7964, Val Loss: 0.8491
Epoch 11/50, Train Loss: 0.7970, Val Loss: 0.8491
Epoch 12/50, Train Loss: 0.7970, Val Loss: 0.8488
Epoch 13/50, Train Loss: 0.7967, Val Loss: 0.8491
Epoch 14/50, Train Loss: 0.7970, Val Loss: 0.8487
Epoch 15/50, Train Loss: 0.7968, Val Loss: 0.8498
Epoch 16/50, Train Loss: 0.7970, Val Loss: 0.8491
Epoch 17/50, Train Loss: 0.7969, Val Loss: 0.8490
Epoch 18/50, Train Loss: 0.7970, Val Loss: 0.8489
Epoch 19/50, Train Loss: 0.7968, Val Loss: 0.8488
Epoch 20/50, Train Loss: 0.7966, Val Loss: 0.8490
Epoch 21/50, Train Loss: 0.7967, Val Loss: 0.8487
Epoch 22/50, Train Loss: 0.7967, Val Loss: 0.8490
Epoch 23/50, Train Loss: 0.7965, Val Loss: 0.8489
Epoch 24/50, Train Loss: 0.7970, Val Loss: 0.8488
Epoch 25/50, Train Loss: 0.7970, Val Loss: 0.8488
Epoch 26/50, Train Loss: 0.7967, Val Loss: 0.8489
Epoch 27/50, Train Loss: 0.7970, Val Loss: 0.8492
Epoch 28/50, Train Loss: 0.7970, Val Loss: 0.8489
Epoch 29/50, Train Loss: 0.7968, Val Loss: 0.8486
Epoch 30/50, Train Loss: 0.7969, Val Loss: 0.8485
Epoch 31/50, Train Loss: 0.7967, Val Loss: 0.8487
Epoch 32/50, Train Loss: 0.7965, Val Loss: 0.8491
Epoch 33/50, Train Loss: 0.7966, Val Loss: 0.8487
Epoch 34/50, Train Loss: 0.7962, Val Loss: 0.8491
Epoch 35/50, Train Loss: 0.7971, Val Loss: 0.8491
Epoch 36/50, Train Loss: 0.7971, Val Loss: 0.8489
Epoch 37/50, Train Loss: 0.7970, Val Loss: 0.8487
Epoch 38/50, Train Loss: 0.7968, Val Loss: 0.8487
Epoch 39/50, Train Loss: 0.7970, Val Loss: 0.8489
Epoch 40/50, Train Loss: 0.7969, Val Loss: 0.8487
Epoch 41/50, Train Loss: 0.7966, Val Loss: 0.8489
Epoch 42/50, Train Loss: 0.7970, Val Loss: 0.8485
Epoch 43/50, Train Loss: 0.7970, Val Loss: 0.8492
Epoch 44/50, Train Loss: 0.7972, Val Loss: 0.8487
Epoch 45/50, Train Loss: 0.7974, Val Loss: 0.8492
Epoch 46/50, Train Loss: 0.7972, Val Loss: 0.8488
Epoch 47/50, Train Loss: 0.7969, Val Loss: 0.8486
Epoch 48/50, Train Loss: 0.7965, Val Loss: 0.8489
Epoch 49/50, Train Loss: 0.7970, Val Loss: 0.8492
Epoch 50/50, Train Loss: 0.7969, Val Loss: 0.8488
Metrics for Fold 15 (Train):
{'0.0': {'precision': 0.9010835224019846, 'recall': 0.8748353096179183, 'f1-score': 0.8877654407386902, 'support': 48576.0}, '1.0': {'precision': 0.8783781080594507, 'recall': 0.9039649209486166, 'f1-score': 0.890987855976138, 'support': 48576.0}, 'accuracy': 0.8894001152832675, 'macro avg': {'precision': 0.8897308152307177, 'recall': 0.8894001152832675, 'f1-score': 0.8893766483574141, 'support': 97152.0}, 'weighted avg': {'precision': 0.8897308152307176, 'recall': 0.8894001152832675, 'f1-score': 0.8893766483574141, 'support': 97152.0}}
Metrics for Fold 15 (Test):
{'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}}
Fold 16: Test Patient chb11
Fold 16: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8867, Val Loss: 0.8654
Epoch 2/50, Train Loss: 0.8656, Val Loss: 0.8560
Epoch 3/50, Train Loss: 0.8577, Val Loss: 0.8593
Epoch 4/50, Train Loss: 0.8494, Val Loss: 0.8428
Epoch 5/50, Train Loss: 0.8428, Val Loss: 0.8290
Epoch 6/50, Train Loss: 0.8425, Val Loss: 0.8183
Epoch 7/50, Train Loss: 0.8411, Val Loss: 0.8591
Epoch 8/50, Train Loss: 0.8406, Val Loss: 0.8480
Epoch 9/50, Train Loss: 0.8409, Val Loss: 0.8238
Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.
Epoch 10/50, Train Loss: 0.8395, Val Loss: 0.8422
Epoch 11/50, Train Loss: 0.8251, Val Loss: 0.8217
Epoch 12/50, Train Loss: 0.8192, Val Loss: 0.8137
Epoch 13/50, Train Loss: 0.8179, Val Loss: 0.8099
Epoch 14/50, Train Loss: 0.8170, Val Loss: 0.8066
Epoch 15/50, Train Loss: 0.8160, Val Loss: 0.8000
Epoch 16/50, Train Loss: 0.8139, Val Loss: 0.7945
Epoch 17/50, Train Loss: 0.8137, Val Loss: 0.7994
Epoch 18/50, Train Loss: 0.8133, Val Loss: 0.8052
Epoch 19/50, Train Loss: 0.8127, Val Loss: 0.7943
Epoch 20/50, Train Loss: 0.8129, Val Loss: 0.8219
Epoch 21/50, Train Loss: 0.8122, Val Loss: 0.8161
Epoch 22/50, Train Loss: 0.8112, Val Loss: 0.8132
Epoch 00023: reducing learning rate of group 0 to 1.0000e-04.
Epoch 23/50, Train Loss: 0.8108, Val Loss: 0.7963
Epoch 24/50, Train Loss: 0.8069, Val Loss: 0.8042
Epoch 25/50, Train Loss: 0.8053, Val Loss: 0.7997
Epoch 26/50, Train Loss: 0.8051, Val Loss: 0.8000
Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.
Epoch 27/50, Train Loss: 0.8041, Val Loss: 0.7992
Epoch 28/50, Train Loss: 0.8038, Val Loss: 0.7997
Epoch 29/50, Train Loss: 0.8033, Val Loss: 0.7998
Epoch 30/50, Train Loss: 0.8031, Val Loss: 0.7985
Epoch 00031: reducing learning rate of group 0 to 1.0000e-06.
Epoch 31/50, Train Loss: 0.8030, Val Loss: 0.7988
Epoch 32/50, Train Loss: 0.8031, Val Loss: 0.7985
Epoch 33/50, Train Loss: 0.8036, Val Loss: 0.7987
Epoch 34/50, Train Loss: 0.8031, Val Loss: 0.7992
Epoch 00035: reducing learning rate of group 0 to 1.0000e-07.
Epoch 35/50, Train Loss: 0.8031, Val Loss: 0.7989
Epoch 36/50, Train Loss: 0.8033, Val Loss: 0.7991
Epoch 37/50, Train Loss: 0.8031, Val Loss: 0.7988
Epoch 38/50, Train Loss: 0.8030, Val Loss: 0.8001
Epoch 00039: reducing learning rate of group 0 to 1.0000e-08.
Epoch 39/50, Train Loss: 0.8030, Val Loss: 0.7989
Epoch 40/50, Train Loss: 0.8037, Val Loss: 0.7985
Epoch 41/50, Train Loss: 0.8029, Val Loss: 0.7986
Epoch 42/50, Train Loss: 0.8031, Val Loss: 0.7990
Epoch 43/50, Train Loss: 0.8031, Val Loss: 0.7997
Epoch 44/50, Train Loss: 0.8034, Val Loss: 0.7987
Epoch 45/50, Train Loss: 0.8032, Val Loss: 0.7994
Epoch 46/50, Train Loss: 0.8033, Val Loss: 0.7981
Epoch 47/50, Train Loss: 0.8031, Val Loss: 0.7985
Epoch 48/50, Train Loss: 0.8029, Val Loss: 0.7988
Epoch 49/50, Train Loss: 0.8035, Val Loss: 0.7985
Epoch 50/50, Train Loss: 0.8031, Val Loss: 0.7990
Epoch 1/50, Train Loss: 0.8033, Val Loss: 0.7985
Epoch 2/50, Train Loss: 0.8032, Val Loss: 0.7988
Epoch 3/50, Train Loss: 0.8030, Val Loss: 0.7991
Epoch 4/50, Train Loss: 0.8033, Val Loss: 0.7989
Epoch 5/50, Train Loss: 0.8034, Val Loss: 0.7986
Epoch 6/50, Train Loss: 0.8035, Val Loss: 0.7986
Epoch 7/50, Train Loss: 0.8033, Val Loss: 0.7990
Epoch 8/50, Train Loss: 0.8030, Val Loss: 0.7985
Epoch 9/50, Train Loss: 0.8035, Val Loss: 0.7982
Epoch 10/50, Train Loss: 0.8027, Val Loss: 0.7990
Epoch 11/50, Train Loss: 0.8030, Val Loss: 0.7987
Epoch 12/50, Train Loss: 0.8030, Val Loss: 0.7987
Epoch 13/50, Train Loss: 0.8033, Val Loss: 0.7991
Epoch 14/50, Train Loss: 0.8033, Val Loss: 0.7985
Epoch 15/50, Train Loss: 0.8031, Val Loss: 0.7981
Epoch 16/50, Train Loss: 0.8030, Val Loss: 0.7985
Epoch 17/50, Train Loss: 0.8033, Val Loss: 0.7982
Epoch 18/50, Train Loss: 0.8034, Val Loss: 0.7985
Epoch 19/50, Train Loss: 0.8031, Val Loss: 0.7980
Epoch 20/50, Train Loss: 0.8030, Val Loss: 0.7986
Epoch 21/50, Train Loss: 0.8031, Val Loss: 0.7983
Epoch 22/50, Train Loss: 0.8030, Val Loss: 0.7986
Epoch 23/50, Train Loss: 0.8031, Val Loss: 0.7984
Epoch 24/50, Train Loss: 0.8034, Val Loss: 0.7989
Epoch 25/50, Train Loss: 0.8029, Val Loss: 0.7984
Epoch 26/50, Train Loss: 0.8032, Val Loss: 0.7986
Epoch 27/50, Train Loss: 0.8031, Val Loss: 0.7973
Epoch 28/50, Train Loss: 0.8029, Val Loss: 0.7988
Epoch 29/50, Train Loss: 0.8029, Val Loss: 0.7987
Epoch 30/50, Train Loss: 0.8032, Val Loss: 0.7989
Epoch 31/50, Train Loss: 0.8033, Val Loss: 0.7990
Epoch 32/50, Train Loss: 0.8036, Val Loss: 0.7984
Epoch 33/50, Train Loss: 0.8034, Val Loss: 0.7990
Epoch 34/50, Train Loss: 0.8033, Val Loss: 0.7983
Epoch 35/50, Train Loss: 0.8033, Val Loss: 0.7987
Epoch 36/50, Train Loss: 0.8035, Val Loss: 0.7986
Epoch 37/50, Train Loss: 0.8030, Val Loss: 0.8000
Epoch 38/50, Train Loss: 0.8029, Val Loss: 0.7984
Epoch 39/50, Train Loss: 0.8034, Val Loss: 0.7990
Epoch 40/50, Train Loss: 0.8029, Val Loss: 0.7983
Epoch 41/50, Train Loss: 0.8034, Val Loss: 0.7993
Epoch 42/50, Train Loss: 0.8034, Val Loss: 0.7989
Epoch 43/50, Train Loss: 0.8033, Val Loss: 0.7986
Epoch 44/50, Train Loss: 0.8030, Val Loss: 0.7981
Epoch 45/50, Train Loss: 0.8033, Val Loss: 0.7991
Epoch 46/50, Train Loss: 0.8032, Val Loss: 0.7993
Epoch 47/50, Train Loss: 0.8029, Val Loss: 0.7979
Epoch 48/50, Train Loss: 0.8032, Val Loss: 0.7981
Epoch 49/50, Train Loss: 0.8027, Val Loss: 0.7981
Epoch 50/50, Train Loss: 0.8032, Val Loss: 0.7991
Metrics for Fold 16 (Train):
{'0.0': {'precision': 0.8924891424678532, 'recall': 0.8630187747035574, 'f1-score': 0.8775065935446059, 'support': 48576.0}, '1.0': {'precision': 0.8673973694699083, 'recall': 0.8960391963109354, 'f1-score': 0.8814856818826198, 'support': 48576.0}, 'accuracy': 0.8795289855072463, 'macro avg': {'precision': 0.8799432559688807, 'recall': 0.8795289855072463, 'f1-score': 0.8794961377136128, 'support': 97152.0}, 'weighted avg': {'precision': 0.8799432559688808, 'recall': 0.8795289855072463, 'f1-score': 0.8794961377136129, 'support': 97152.0}}
Metrics for Fold 16 (Test):
{'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}}
Fold 17: Test Patient chb04
Fold 17: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8838, Val Loss: 0.8632
Epoch 2/50, Train Loss: 0.8613, Val Loss: 0.8655
Epoch 3/50, Train Loss: 0.8547, Val Loss: 0.8332
Epoch 4/50, Train Loss: 0.8507, Val Loss: 0.8183
Epoch 5/50, Train Loss: 0.8464, Val Loss: 0.8064
Epoch 6/50, Train Loss: 0.8474, Val Loss: 0.8304
Epoch 7/50, Train Loss: 0.8456, Val Loss: 0.7966
Epoch 8/50, Train Loss: 0.8454, Val Loss: 0.8398
Epoch 9/50, Train Loss: 0.8460, Val Loss: 0.8797
Epoch 10/50, Train Loss: 0.8424, Val Loss: 0.8265
Epoch 00011: reducing learning rate of group 0 to 1.0000e-03.
Epoch 11/50, Train Loss: 0.8412, Val Loss: 0.8161
Epoch 12/50, Train Loss: 0.8251, Val Loss: 0.7696
Epoch 13/50, Train Loss: 0.8192, Val Loss: 0.7708
Epoch 14/50, Train Loss: 0.8179, Val Loss: 0.7745
Epoch 15/50, Train Loss: 0.8169, Val Loss: 0.7760
Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.
Epoch 16/50, Train Loss: 0.8156, Val Loss: 0.7742
Epoch 17/50, Train Loss: 0.8127, Val Loss: 0.7693
Epoch 18/50, Train Loss: 0.8110, Val Loss: 0.7677
Epoch 19/50, Train Loss: 0.8107, Val Loss: 0.7700
Epoch 20/50, Train Loss: 0.8095, Val Loss: 0.7692
Epoch 21/50, Train Loss: 0.8091, Val Loss: 0.7684
Epoch 22/50, Train Loss: 0.8089, Val Loss: 0.7673
Epoch 23/50, Train Loss: 0.8082, Val Loss: 0.7672
Epoch 24/50, Train Loss: 0.8078, Val Loss: 0.7673
Epoch 25/50, Train Loss: 0.8076, Val Loss: 0.7659
Epoch 26/50, Train Loss: 0.8067, Val Loss: 0.7647
Epoch 27/50, Train Loss: 0.8069, Val Loss: 0.7673
Epoch 28/50, Train Loss: 0.8069, Val Loss: 0.7671
Epoch 29/50, Train Loss: 0.8064, Val Loss: 0.7667
Epoch 30/50, Train Loss: 0.8059, Val Loss: 0.7641
Epoch 31/50, Train Loss: 0.8058, Val Loss: 0.7667
Epoch 32/50, Train Loss: 0.8055, Val Loss: 0.7631
Epoch 33/50, Train Loss: 0.8055, Val Loss: 0.7648
Epoch 34/50, Train Loss: 0.8050, Val Loss: 0.7666
Epoch 35/50, Train Loss: 0.8051, Val Loss: 0.7659
Epoch 00036: reducing learning rate of group 0 to 1.0000e-05.
Epoch 36/50, Train Loss: 0.8047, Val Loss: 0.7640
Epoch 37/50, Train Loss: 0.8040, Val Loss: 0.7647
Epoch 38/50, Train Loss: 0.8038, Val Loss: 0.7651
Epoch 39/50, Train Loss: 0.8036, Val Loss: 0.7647
Epoch 00040: reducing learning rate of group 0 to 1.0000e-06.
Epoch 40/50, Train Loss: 0.8035, Val Loss: 0.7644
Epoch 41/50, Train Loss: 0.8036, Val Loss: 0.7648
Epoch 42/50, Train Loss: 0.8039, Val Loss: 0.7649
Epoch 43/50, Train Loss: 0.8033, Val Loss: 0.7647
Epoch 00044: reducing learning rate of group 0 to 1.0000e-07.
Epoch 44/50, Train Loss: 0.8037, Val Loss: 0.7648
Epoch 45/50, Train Loss: 0.8036, Val Loss: 0.7647
Epoch 46/50, Train Loss: 0.8034, Val Loss: 0.7649
Epoch 47/50, Train Loss: 0.8035, Val Loss: 0.7648
Epoch 00048: reducing learning rate of group 0 to 1.0000e-08.
Epoch 48/50, Train Loss: 0.8034, Val Loss: 0.7648
Epoch 49/50, Train Loss: 0.8035, Val Loss: 0.7650
Epoch 50/50, Train Loss: 0.8037, Val Loss: 0.7648
Epoch 1/50, Train Loss: 0.8036, Val Loss: 0.7645
Epoch 2/50, Train Loss: 0.8033, Val Loss: 0.7645
Epoch 3/50, Train Loss: 0.8037, Val Loss: 0.7645
Epoch 4/50, Train Loss: 0.8033, Val Loss: 0.7646
Epoch 5/50, Train Loss: 0.8036, Val Loss: 0.7650
Epoch 6/50, Train Loss: 0.8033, Val Loss: 0.7646
Epoch 7/50, Train Loss: 0.8037, Val Loss: 0.7642
Epoch 8/50, Train Loss: 0.8036, Val Loss: 0.7644
Epoch 9/50, Train Loss: 0.8032, Val Loss: 0.7646
Epoch 10/50, Train Loss: 0.8036, Val Loss: 0.7648
Epoch 11/50, Train Loss: 0.8033, Val Loss: 0.7646
Epoch 12/50, Train Loss: 0.8036, Val Loss: 0.7643
Epoch 13/50, Train Loss: 0.8033, Val Loss: 0.7645
Epoch 14/50, Train Loss: 0.8036, Val Loss: 0.7648
Epoch 15/50, Train Loss: 0.8038, Val Loss: 0.7643
Epoch 16/50, Train Loss: 0.8035, Val Loss: 0.7648
Epoch 17/50, Train Loss: 0.8036, Val Loss: 0.7645
Epoch 18/50, Train Loss: 0.8034, Val Loss: 0.7646
Epoch 19/50, Train Loss: 0.8036, Val Loss: 0.7644
Epoch 20/50, Train Loss: 0.8034, Val Loss: 0.7647
Epoch 21/50, Train Loss: 0.8032, Val Loss: 0.7649
Epoch 22/50, Train Loss: 0.8036, Val Loss: 0.7644
Epoch 23/50, Train Loss: 0.8042, Val Loss: 0.7645
Epoch 24/50, Train Loss: 0.8033, Val Loss: 0.7644
Epoch 25/50, Train Loss: 0.8033, Val Loss: 0.7648
Epoch 26/50, Train Loss: 0.8035, Val Loss: 0.7651
Epoch 27/50, Train Loss: 0.8033, Val Loss: 0.7648
Epoch 28/50, Train Loss: 0.8034, Val Loss: 0.7646
Epoch 29/50, Train Loss: 0.8033, Val Loss: 0.7650
Epoch 30/50, Train Loss: 0.8030, Val Loss: 0.7649
Epoch 31/50, Train Loss: 0.8034, Val Loss: 0.7643
Epoch 32/50, Train Loss: 0.8039, Val Loss: 0.7646
Epoch 33/50, Train Loss: 0.8035, Val Loss: 0.7645
Epoch 34/50, Train Loss: 0.8037, Val Loss: 0.7646
Epoch 35/50, Train Loss: 0.8032, Val Loss: 0.7646
Epoch 36/50, Train Loss: 0.8037, Val Loss: 0.7649
Epoch 37/50, Train Loss: 0.8034, Val Loss: 0.7646
Epoch 38/50, Train Loss: 0.8034, Val Loss: 0.7647
Epoch 39/50, Train Loss: 0.8033, Val Loss: 0.7643
Epoch 40/50, Train Loss: 0.8036, Val Loss: 0.7645
Epoch 41/50, Train Loss: 0.8038, Val Loss: 0.7644
Epoch 42/50, Train Loss: 0.8037, Val Loss: 0.7648
Epoch 43/50, Train Loss: 0.8034, Val Loss: 0.7647
Epoch 44/50, Train Loss: 0.8034, Val Loss: 0.7648
Epoch 45/50, Train Loss: 0.8037, Val Loss: 0.7648
Epoch 46/50, Train Loss: 0.8036, Val Loss: 0.7645
Epoch 47/50, Train Loss: 0.8039, Val Loss: 0.7653
Epoch 48/50, Train Loss: 0.8035, Val Loss: 0.7647
Epoch 49/50, Train Loss: 0.8034, Val Loss: 0.7647
Epoch 50/50, Train Loss: 0.8036, Val Loss: 0.7649
Metrics for Fold 17 (Train):
{'0.0': {'precision': 0.8928791517031811, 'recall': 0.8649950592885376, 'f1-score': 0.8787159512730696, 'support': 48576.0}, '1.0': {'precision': 0.8690835046812928, 'recall': 0.8962244729907773, 'f1-score': 0.8824453475762397, 'support': 48576.0}, 'accuracy': 0.8806097661396575, 'macro avg': {'precision': 0.880981328192237, 'recall': 0.8806097661396575, 'f1-score': 0.8805806494246546, 'support': 97152.0}, 'weighted avg': {'precision': 0.880981328192237, 'recall': 0.8806097661396575, 'f1-score': 0.8805806494246546, 'support': 97152.0}}
Metrics for Fold 17 (Test):
{'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}}
Fold 18: Test Patient chb20
Fold 18: Distribution of classes
Training set: Class 0: 48788, Class 1: 48788
Test set: Class 0: 2288, Class 1: 2288

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8771, Val Loss: 0.9497
Epoch 2/50, Train Loss: 0.8615, Val Loss: 0.9233
Epoch 3/50, Train Loss: 0.8544, Val Loss: 0.9095
Epoch 4/50, Train Loss: 0.8508, Val Loss: 0.9073
Epoch 5/50, Train Loss: 0.8472, Val Loss: 0.9152
Epoch 6/50, Train Loss: 0.8447, Val Loss: 0.9227
Epoch 7/50, Train Loss: 0.8427, Val Loss: 0.9207
Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.
Epoch 8/50, Train Loss: 0.8427, Val Loss: 0.9175
Epoch 9/50, Train Loss: 0.8268, Val Loss: 0.9109
Epoch 10/50, Train Loss: 0.8206, Val Loss: 0.8956
Epoch 11/50, Train Loss: 0.8193, Val Loss: 0.8990
Epoch 12/50, Train Loss: 0.8187, Val Loss: 0.8833
Epoch 13/50, Train Loss: 0.8177, Val Loss: 0.8978
Epoch 14/50, Train Loss: 0.8166, Val Loss: 0.8815
Epoch 15/50, Train Loss: 0.8165, Val Loss: 0.8783
Epoch 16/50, Train Loss: 0.8157, Val Loss: 0.8887
Epoch 17/50, Train Loss: 0.8148, Val Loss: 0.8843
Epoch 18/50, Train Loss: 0.8143, Val Loss: 0.8857
Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.
Epoch 19/50, Train Loss: 0.8138, Val Loss: 0.8784
Epoch 20/50, Train Loss: 0.8096, Val Loss: 0.8753
Epoch 21/50, Train Loss: 0.8085, Val Loss: 0.8820
Epoch 22/50, Train Loss: 0.8080, Val Loss: 0.8801
Epoch 23/50, Train Loss: 0.8077, Val Loss: 0.8767
Epoch 24/50, Train Loss: 0.8070, Val Loss: 0.8718
Epoch 25/50, Train Loss: 0.8069, Val Loss: 0.8744
Epoch 26/50, Train Loss: 0.8064, Val Loss: 0.8734
Epoch 27/50, Train Loss: 0.8061, Val Loss: 0.8738
Epoch 28/50, Train Loss: 0.8059, Val Loss: 0.8698
Epoch 29/50, Train Loss: 0.8060, Val Loss: 0.8698
Epoch 30/50, Train Loss: 0.8059, Val Loss: 0.8765
Epoch 31/50, Train Loss: 0.8056, Val Loss: 0.8723
Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.
Epoch 32/50, Train Loss: 0.8052, Val Loss: 0.8715
Epoch 33/50, Train Loss: 0.8050, Val Loss: 0.8734
Epoch 34/50, Train Loss: 0.8047, Val Loss: 0.8747
Epoch 35/50, Train Loss: 0.8044, Val Loss: 0.8737
Epoch 00036: reducing learning rate of group 0 to 1.0000e-06.
Epoch 36/50, Train Loss: 0.8047, Val Loss: 0.8724
Epoch 37/50, Train Loss: 0.8048, Val Loss: 0.8745
Epoch 38/50, Train Loss: 0.8047, Val Loss: 0.8753
Epoch 39/50, Train Loss: 0.8047, Val Loss: 0.8754
Epoch 00040: reducing learning rate of group 0 to 1.0000e-07.
Epoch 40/50, Train Loss: 0.8046, Val Loss: 0.8743
Epoch 41/50, Train Loss: 0.8044, Val Loss: 0.8750
Epoch 42/50, Train Loss: 0.8048, Val Loss: 0.8740
Epoch 43/50, Train Loss: 0.8044, Val Loss: 0.8740
Epoch 00044: reducing learning rate of group 0 to 1.0000e-08.
Epoch 44/50, Train Loss: 0.8045, Val Loss: 0.8736
Epoch 45/50, Train Loss: 0.8045, Val Loss: 0.8739
Epoch 46/50, Train Loss: 0.8046, Val Loss: 0.8738
Epoch 47/50, Train Loss: 0.8044, Val Loss: 0.8738
Epoch 48/50, Train Loss: 0.8045, Val Loss: 0.8743
Epoch 49/50, Train Loss: 0.8042, Val Loss: 0.8747
Epoch 50/50, Train Loss: 0.8041, Val Loss: 0.8733
Epoch 1/50, Train Loss: 0.8044, Val Loss: 0.8737
Epoch 2/50, Train Loss: 0.8049, Val Loss: 0.8728
Epoch 3/50, Train Loss: 0.8043, Val Loss: 0.8751
Epoch 4/50, Train Loss: 0.8044, Val Loss: 0.8737
Epoch 5/50, Train Loss: 0.8046, Val Loss: 0.8736
Epoch 6/50, Train Loss: 0.8045, Val Loss: 0.8741
Epoch 7/50, Train Loss: 0.8040, Val Loss: 0.8741
Epoch 8/50, Train Loss: 0.8041, Val Loss: 0.8753
Epoch 9/50, Train Loss: 0.8046, Val Loss: 0.8738
Epoch 10/50, Train Loss: 0.8046, Val Loss: 0.8740
Epoch 11/50, Train Loss: 0.8047, Val Loss: 0.8747
Epoch 12/50, Train Loss: 0.8049, Val Loss: 0.8747
Epoch 13/50, Train Loss: 0.8048, Val Loss: 0.8751
Epoch 14/50, Train Loss: 0.8047, Val Loss: 0.8740
Epoch 15/50, Train Loss: 0.8044, Val Loss: 0.8753
Epoch 16/50, Train Loss: 0.8041, Val Loss: 0.8753
Epoch 17/50, Train Loss: 0.8044, Val Loss: 0.8747
Epoch 18/50, Train Loss: 0.8045, Val Loss: 0.8740
Epoch 19/50, Train Loss: 0.8048, Val Loss: 0.8754
Epoch 20/50, Train Loss: 0.8047, Val Loss: 0.8739
Epoch 21/50, Train Loss: 0.8047, Val Loss: 0.8736
Epoch 22/50, Train Loss: 0.8046, Val Loss: 0.8739
Epoch 23/50, Train Loss: 0.8052, Val Loss: 0.8737
Epoch 24/50, Train Loss: 0.8048, Val Loss: 0.8753
Epoch 25/50, Train Loss: 0.8047, Val Loss: 0.8741
Epoch 26/50, Train Loss: 0.8041, Val Loss: 0.8746
Epoch 27/50, Train Loss: 0.8045, Val Loss: 0.8739
Epoch 28/50, Train Loss: 0.8045, Val Loss: 0.8740
Epoch 29/50, Train Loss: 0.8042, Val Loss: 0.8743
Epoch 30/50, Train Loss: 0.8045, Val Loss: 0.8738
Epoch 31/50, Train Loss: 0.8044, Val Loss: 0.8726
Epoch 32/50, Train Loss: 0.8041, Val Loss: 0.8756
Epoch 33/50, Train Loss: 0.8044, Val Loss: 0.8741
Epoch 34/50, Train Loss: 0.8046, Val Loss: 0.8730
Epoch 35/50, Train Loss: 0.8045, Val Loss: 0.8741
Epoch 36/50, Train Loss: 0.8045, Val Loss: 0.8739
Epoch 37/50, Train Loss: 0.8046, Val Loss: 0.8732
Epoch 38/50, Train Loss: 0.8047, Val Loss: 0.8737
Epoch 39/50, Train Loss: 0.8044, Val Loss: 0.8735
Epoch 40/50, Train Loss: 0.8044, Val Loss: 0.8741
Epoch 41/50, Train Loss: 0.8047, Val Loss: 0.8751
Epoch 42/50, Train Loss: 0.8042, Val Loss: 0.8729
Epoch 43/50, Train Loss: 0.8047, Val Loss: 0.8737
Epoch 44/50, Train Loss: 0.8041, Val Loss: 0.8741
Epoch 45/50, Train Loss: 0.8046, Val Loss: 0.8736
Epoch 46/50, Train Loss: 0.8043, Val Loss: 0.8744
Epoch 47/50, Train Loss: 0.8046, Val Loss: 0.8729
Epoch 48/50, Train Loss: 0.8045, Val Loss: 0.8750
Epoch 49/50, Train Loss: 0.8042, Val Loss: 0.8745
Epoch 50/50, Train Loss: 0.8047, Val Loss: 0.8738
Metrics for Fold 18 (Train):
{'0.0': {'precision': 0.8930088102934651, 'recall': 0.8663400836271214, 'f1-score': 0.8794723207690467, 'support': 48788.0}, '1.0': {'precision': 0.8702159418847647, 'recall': 0.8962039845863737, 'f1-score': 0.8830187917158927, 'support': 48788.0}, 'accuracy': 0.8812720341067476, 'macro avg': {'precision': 0.8816123760891149, 'recall': 0.8812720341067476, 'f1-score': 0.8812455562424697, 'support': 97576.0}, 'weighted avg': {'precision': 0.8816123760891149, 'recall': 0.8812720341067476, 'f1-score': 0.8812455562424698, 'support': 97576.0}}
Metrics for Fold 18 (Test):
{'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}}
Fold 19: Test Patient chb23
Fold 19: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8822, Val Loss: 0.8966
Epoch 2/50, Train Loss: 0.8632, Val Loss: 0.8373
Epoch 3/50, Train Loss: 0.8573, Val Loss: 0.8455
Epoch 4/50, Train Loss: 0.8522, Val Loss: 0.8563
Epoch 5/50, Train Loss: 0.8511, Val Loss: 0.8248
Epoch 6/50, Train Loss: 0.8479, Val Loss: 0.8679
Epoch 7/50, Train Loss: 0.8469, Val Loss: 0.8278
Epoch 8/50, Train Loss: 0.8465, Val Loss: 0.8322
Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.
Epoch 9/50, Train Loss: 0.8457, Val Loss: 0.8634
Epoch 10/50, Train Loss: 0.8301, Val Loss: 0.8182
Epoch 11/50, Train Loss: 0.8209, Val Loss: 0.8163
Epoch 12/50, Train Loss: 0.8178, Val Loss: 0.8267
Epoch 13/50, Train Loss: 0.8171, Val Loss: 0.8165
Epoch 14/50, Train Loss: 0.8155, Val Loss: 0.8111
Epoch 15/50, Train Loss: 0.8141, Val Loss: 0.8270
Epoch 16/50, Train Loss: 0.8137, Val Loss: 0.8107
Epoch 17/50, Train Loss: 0.8132, Val Loss: 0.8153
Epoch 18/50, Train Loss: 0.8118, Val Loss: 0.8188
Epoch 19/50, Train Loss: 0.8109, Val Loss: 0.8051
Epoch 20/50, Train Loss: 0.8108, Val Loss: 0.8049
Epoch 21/50, Train Loss: 0.8093, Val Loss: 0.8056
Epoch 22/50, Train Loss: 0.8093, Val Loss: 0.8330
Epoch 23/50, Train Loss: 0.8088, Val Loss: 0.8174
Epoch 24/50, Train Loss: 0.8084, Val Loss: 0.8045
Epoch 25/50, Train Loss: 0.8078, Val Loss: 0.8000
Epoch 26/50, Train Loss: 0.8066, Val Loss: 0.8033
Epoch 27/50, Train Loss: 0.8065, Val Loss: 0.8043
Epoch 28/50, Train Loss: 0.8066, Val Loss: 0.8232
Epoch 00029: reducing learning rate of group 0 to 1.0000e-04.
Epoch 29/50, Train Loss: 0.8063, Val Loss: 0.8309
Epoch 30/50, Train Loss: 0.8014, Val Loss: 0.8022
Epoch 31/50, Train Loss: 0.8003, Val Loss: 0.8069
Epoch 32/50, Train Loss: 0.7997, Val Loss: 0.8078
Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.
Epoch 33/50, Train Loss: 0.7993, Val Loss: 0.8097
Epoch 34/50, Train Loss: 0.7986, Val Loss: 0.8066
Epoch 35/50, Train Loss: 0.7984, Val Loss: 0.8068
Epoch 36/50, Train Loss: 0.7985, Val Loss: 0.8069
Epoch 00037: reducing learning rate of group 0 to 1.0000e-06.
Epoch 37/50, Train Loss: 0.7984, Val Loss: 0.8082
Epoch 38/50, Train Loss: 0.7985, Val Loss: 0.8081
Epoch 39/50, Train Loss: 0.7981, Val Loss: 0.8086
Epoch 40/50, Train Loss: 0.7985, Val Loss: 0.8079
Epoch 00041: reducing learning rate of group 0 to 1.0000e-07.
Epoch 41/50, Train Loss: 0.7984, Val Loss: 0.8090
Epoch 42/50, Train Loss: 0.7985, Val Loss: 0.8082
Epoch 43/50, Train Loss: 0.7983, Val Loss: 0.8080
Epoch 44/50, Train Loss: 0.7981, Val Loss: 0.8086
Epoch 00045: reducing learning rate of group 0 to 1.0000e-08.
Epoch 45/50, Train Loss: 0.7981, Val Loss: 0.8085
Epoch 46/50, Train Loss: 0.7983, Val Loss: 0.8095
Epoch 47/50, Train Loss: 0.7982, Val Loss: 0.8074
Epoch 48/50, Train Loss: 0.7985, Val Loss: 0.8085
Epoch 49/50, Train Loss: 0.7983, Val Loss: 0.8088
Epoch 50/50, Train Loss: 0.7984, Val Loss: 0.8078
Epoch 1/50, Train Loss: 0.7986, Val Loss: 0.8074
Epoch 2/50, Train Loss: 0.7981, Val Loss: 0.8079
Epoch 3/50, Train Loss: 0.7984, Val Loss: 0.8086
Epoch 4/50, Train Loss: 0.7987, Val Loss: 0.8079
Epoch 5/50, Train Loss: 0.7982, Val Loss: 0.8085
Epoch 6/50, Train Loss: 0.7982, Val Loss: 0.8077
Epoch 7/50, Train Loss: 0.7979, Val Loss: 0.8087
Epoch 8/50, Train Loss: 0.7979, Val Loss: 0.8082
Epoch 9/50, Train Loss: 0.7980, Val Loss: 0.8076
Epoch 10/50, Train Loss: 0.7982, Val Loss: 0.8088
Epoch 11/50, Train Loss: 0.7983, Val Loss: 0.8074
Epoch 12/50, Train Loss: 0.7981, Val Loss: 0.8087
Epoch 13/50, Train Loss: 0.7983, Val Loss: 0.8081
Epoch 14/50, Train Loss: 0.7985, Val Loss: 0.8081
Epoch 15/50, Train Loss: 0.7985, Val Loss: 0.8083
Epoch 16/50, Train Loss: 0.7984, Val Loss: 0.8077
Epoch 17/50, Train Loss: 0.7986, Val Loss: 0.8084
Epoch 18/50, Train Loss: 0.7984, Val Loss: 0.8084
Epoch 19/50, Train Loss: 0.7985, Val Loss: 0.8083
Epoch 20/50, Train Loss: 0.7983, Val Loss: 0.8077
Epoch 21/50, Train Loss: 0.7981, Val Loss: 0.8086
Epoch 22/50, Train Loss: 0.7983, Val Loss: 0.8079
Epoch 23/50, Train Loss: 0.7983, Val Loss: 0.8080
Epoch 24/50, Train Loss: 0.7983, Val Loss: 0.8079
Epoch 25/50, Train Loss: 0.7982, Val Loss: 0.8078
Epoch 26/50, Train Loss: 0.7984, Val Loss: 0.8081
Epoch 27/50, Train Loss: 0.7985, Val Loss: 0.8074
Epoch 28/50, Train Loss: 0.7985, Val Loss: 0.8079
Epoch 29/50, Train Loss: 0.7982, Val Loss: 0.8086
Epoch 30/50, Train Loss: 0.7983, Val Loss: 0.8079
Epoch 31/50, Train Loss: 0.7985, Val Loss: 0.8085
Epoch 32/50, Train Loss: 0.7983, Val Loss: 0.8083
Epoch 33/50, Train Loss: 0.7980, Val Loss: 0.8078
Epoch 34/50, Train Loss: 0.7982, Val Loss: 0.8072
Epoch 35/50, Train Loss: 0.7983, Val Loss: 0.8077
Epoch 36/50, Train Loss: 0.7984, Val Loss: 0.8083
Epoch 37/50, Train Loss: 0.7985, Val Loss: 0.8082
Epoch 38/50, Train Loss: 0.7981, Val Loss: 0.8084
Epoch 39/50, Train Loss: 0.7982, Val Loss: 0.8086
Epoch 40/50, Train Loss: 0.7984, Val Loss: 0.8078
Epoch 41/50, Train Loss: 0.7984, Val Loss: 0.8084
Epoch 42/50, Train Loss: 0.7983, Val Loss: 0.8090
Epoch 43/50, Train Loss: 0.7981, Val Loss: 0.8082
Epoch 44/50, Train Loss: 0.7981, Val Loss: 0.8082
Epoch 45/50, Train Loss: 0.7984, Val Loss: 0.8089
Epoch 46/50, Train Loss: 0.7984, Val Loss: 0.8086
Epoch 47/50, Train Loss: 0.7987, Val Loss: 0.8086
Epoch 48/50, Train Loss: 0.7985, Val Loss: 0.8084
Epoch 49/50, Train Loss: 0.7983, Val Loss: 0.8084
Epoch 50/50, Train Loss: 0.7979, Val Loss: 0.8085
Metrics for Fold 19 (Train):
{'0.0': {'precision': 0.8954223729949323, 'recall': 0.880270092226614, 'f1-score': 0.8877815841378595, 'support': 48576.0}, '1.0': {'precision': 0.8822624397748897, 'recall': 0.8971920289855072, 'f1-score': 0.8896646048951762, 'support': 48576.0}, 'accuracy': 0.8887310606060606, 'macro avg': {'precision': 0.888842406384911, 'recall': 0.8887310606060606, 'f1-score': 0.8887230945165179, 'support': 97152.0}, 'weighted avg': {'precision': 0.888842406384911, 'recall': 0.8887310606060606, 'f1-score': 0.8887230945165178, 'support': 97152.0}}
Metrics for Fold 19 (Test):
{'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}}
Fold 20: Test Patient chb09
Fold 20: Distribution of classes
Training set: Class 0: 48900, Class 1: 48900
Test set: Class 0: 2176, Class 1: 2176

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8845, Val Loss: 0.9335
Epoch 2/50, Train Loss: 0.8595, Val Loss: 0.8876
Epoch 3/50, Train Loss: 0.8503, Val Loss: 0.8962
Epoch 4/50, Train Loss: 0.8467, Val Loss: 0.8763
Epoch 5/50, Train Loss: 0.8463, Val Loss: 0.8980
Epoch 6/50, Train Loss: 0.8440, Val Loss: 0.8714
Epoch 7/50, Train Loss: 0.8425, Val Loss: 0.8952
Epoch 8/50, Train Loss: 0.8418, Val Loss: 0.8924
Epoch 9/50, Train Loss: 0.8423, Val Loss: 0.8872
Epoch 10/50, Train Loss: 0.8408, Val Loss: 0.8682
Epoch 11/50, Train Loss: 0.8421, Val Loss: 0.8619
Epoch 12/50, Train Loss: 0.8421, Val Loss: 0.9515
Epoch 13/50, Train Loss: 0.8400, Val Loss: 0.8746
Epoch 14/50, Train Loss: 0.8410, Val Loss: 0.9270
Epoch 00015: reducing learning rate of group 0 to 1.0000e-03.
Epoch 15/50, Train Loss: 0.8409, Val Loss: 0.8676
Epoch 16/50, Train Loss: 0.8262, Val Loss: 0.8735
Epoch 17/50, Train Loss: 0.8209, Val Loss: 0.8826
Epoch 18/50, Train Loss: 0.8194, Val Loss: 0.8756
Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.
Epoch 19/50, Train Loss: 0.8161, Val Loss: 0.8813
Epoch 20/50, Train Loss: 0.8114, Val Loss: 0.8726
Epoch 21/50, Train Loss: 0.8103, Val Loss: 0.8718
Epoch 22/50, Train Loss: 0.8091, Val Loss: 0.8756
Epoch 00023: reducing learning rate of group 0 to 1.0000e-05.
Epoch 23/50, Train Loss: 0.8085, Val Loss: 0.8719
Epoch 24/50, Train Loss: 0.8083, Val Loss: 0.8733
Epoch 25/50, Train Loss: 0.8081, Val Loss: 0.8741
Epoch 26/50, Train Loss: 0.8083, Val Loss: 0.8733
Epoch 00027: reducing learning rate of group 0 to 1.0000e-06.
Epoch 27/50, Train Loss: 0.8078, Val Loss: 0.8718
Epoch 28/50, Train Loss: 0.8078, Val Loss: 0.8719
Epoch 29/50, Train Loss: 0.8080, Val Loss: 0.8736
Epoch 30/50, Train Loss: 0.8078, Val Loss: 0.8741
Epoch 00031: reducing learning rate of group 0 to 1.0000e-07.
Epoch 31/50, Train Loss: 0.8079, Val Loss: 0.8733
Epoch 32/50, Train Loss: 0.8081, Val Loss: 0.8738
Epoch 33/50, Train Loss: 0.8078, Val Loss: 0.8727
Epoch 34/50, Train Loss: 0.8082, Val Loss: 0.8735
Epoch 00035: reducing learning rate of group 0 to 1.0000e-08.
Epoch 35/50, Train Loss: 0.8078, Val Loss: 0.8728
Epoch 36/50, Train Loss: 0.8077, Val Loss: 0.8727
Epoch 37/50, Train Loss: 0.8077, Val Loss: 0.8735
Epoch 38/50, Train Loss: 0.8077, Val Loss: 0.8725
Epoch 39/50, Train Loss: 0.8081, Val Loss: 0.8729
Epoch 40/50, Train Loss: 0.8083, Val Loss: 0.8726
Epoch 41/50, Train Loss: 0.8074, Val Loss: 0.8733
Epoch 42/50, Train Loss: 0.8078, Val Loss: 0.8724
Epoch 43/50, Train Loss: 0.8076, Val Loss: 0.8737
Epoch 44/50, Train Loss: 0.8078, Val Loss: 0.8726
Epoch 45/50, Train Loss: 0.8079, Val Loss: 0.8729
Epoch 46/50, Train Loss: 0.8079, Val Loss: 0.8721
Epoch 47/50, Train Loss: 0.8080, Val Loss: 0.8728
Epoch 48/50, Train Loss: 0.8080, Val Loss: 0.8726
Epoch 49/50, Train Loss: 0.8082, Val Loss: 0.8729
Epoch 50/50, Train Loss: 0.8078, Val Loss: 0.8725
Epoch 1/50, Train Loss: 0.8079, Val Loss: 0.8724
Epoch 2/50, Train Loss: 0.8076, Val Loss: 0.8724
Epoch 3/50, Train Loss: 0.8077, Val Loss: 0.8729
Epoch 4/50, Train Loss: 0.8075, Val Loss: 0.8736
Epoch 5/50, Train Loss: 0.8076, Val Loss: 0.8738
Epoch 6/50, Train Loss: 0.8082, Val Loss: 0.8734
Epoch 7/50, Train Loss: 0.8077, Val Loss: 0.8735
Epoch 8/50, Train Loss: 0.8079, Val Loss: 0.8724
Epoch 9/50, Train Loss: 0.8075, Val Loss: 0.8724
Epoch 10/50, Train Loss: 0.8080, Val Loss: 0.8725
Epoch 11/50, Train Loss: 0.8079, Val Loss: 0.8733
Epoch 12/50, Train Loss: 0.8076, Val Loss: 0.8724
Epoch 13/50, Train Loss: 0.8080, Val Loss: 0.8731
Epoch 14/50, Train Loss: 0.8078, Val Loss: 0.8733
Epoch 15/50, Train Loss: 0.8078, Val Loss: 0.8730
Epoch 16/50, Train Loss: 0.8076, Val Loss: 0.8726
Epoch 17/50, Train Loss: 0.8078, Val Loss: 0.8735
Epoch 18/50, Train Loss: 0.8080, Val Loss: 0.8728
Epoch 19/50, Train Loss: 0.8076, Val Loss: 0.8729
Epoch 20/50, Train Loss: 0.8078, Val Loss: 0.8737
Epoch 21/50, Train Loss: 0.8080, Val Loss: 0.8724
Epoch 22/50, Train Loss: 0.8077, Val Loss: 0.8731
Epoch 23/50, Train Loss: 0.8079, Val Loss: 0.8733
Epoch 24/50, Train Loss: 0.8081, Val Loss: 0.8733
Epoch 25/50, Train Loss: 0.8076, Val Loss: 0.8725
Epoch 26/50, Train Loss: 0.8074, Val Loss: 0.8718
Epoch 27/50, Train Loss: 0.8080, Val Loss: 0.8732
Epoch 28/50, Train Loss: 0.8079, Val Loss: 0.8734
Epoch 29/50, Train Loss: 0.8074, Val Loss: 0.8730
Epoch 30/50, Train Loss: 0.8084, Val Loss: 0.8734
Epoch 31/50, Train Loss: 0.8077, Val Loss: 0.8727
Epoch 32/50, Train Loss: 0.8077, Val Loss: 0.8724
Epoch 33/50, Train Loss: 0.8077, Val Loss: 0.8731
Epoch 34/50, Train Loss: 0.8079, Val Loss: 0.8730
Epoch 35/50, Train Loss: 0.8079, Val Loss: 0.8730
Epoch 36/50, Train Loss: 0.8076, Val Loss: 0.8730
Epoch 37/50, Train Loss: 0.8077, Val Loss: 0.8729
Epoch 38/50, Train Loss: 0.8082, Val Loss: 0.8731
Epoch 39/50, Train Loss: 0.8078, Val Loss: 0.8734
Epoch 40/50, Train Loss: 0.8078, Val Loss: 0.8729
Epoch 41/50, Train Loss: 0.8079, Val Loss: 0.8723
Epoch 42/50, Train Loss: 0.8075, Val Loss: 0.8727
Epoch 43/50, Train Loss: 0.8077, Val Loss: 0.8723
Epoch 44/50, Train Loss: 0.8076, Val Loss: 0.8730
Epoch 45/50, Train Loss: 0.8081, Val Loss: 0.8727
Epoch 46/50, Train Loss: 0.8076, Val Loss: 0.8737
Epoch 47/50, Train Loss: 0.8078, Val Loss: 0.8732
Epoch 48/50, Train Loss: 0.8074, Val Loss: 0.8744
Epoch 49/50, Train Loss: 0.8079, Val Loss: 0.8723
Epoch 50/50, Train Loss: 0.8083, Val Loss: 0.8725
Metrics for Fold 20 (Train):
{'0.0': {'precision': 0.8927695163320604, 'recall': 0.8557259713701432, 'f1-score': 0.8738553424314249, 'support': 48900.0}, '1.0': {'precision': 0.8614738164896228, 'recall': 0.8972188139059305, 'f1-score': 0.8789830610343689, 'support': 48900.0}, 'accuracy': 0.8764723926380368, 'macro avg': {'precision': 0.8771216664108417, 'recall': 0.8764723926380369, 'f1-score': 0.8764192017328969, 'support': 97800.0}, 'weighted avg': {'precision': 0.8771216664108415, 'recall': 0.8764723926380368, 'f1-score': 0.876419201732897, 'support': 97800.0}}
Metrics for Fold 20 (Test):
{'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}}
Fold 21: Test Patient chb12
Fold 21: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8746, Val Loss: 0.9392
Epoch 2/50, Train Loss: 0.8562, Val Loss: 0.9392
Epoch 3/50, Train Loss: 0.8512, Val Loss: 0.9387
Epoch 4/50, Train Loss: 0.8485, Val Loss: 0.9293
Epoch 5/50, Train Loss: 0.8435, Val Loss: 0.9366
Epoch 6/50, Train Loss: 0.8386, Val Loss: 0.9303
Epoch 7/50, Train Loss: 0.8372, Val Loss: 0.9239
Epoch 8/50, Train Loss: 0.8345, Val Loss: 0.9272
Epoch 9/50, Train Loss: 0.8336, Val Loss: 0.9077
Epoch 10/50, Train Loss: 0.8343, Val Loss: 0.9362
Epoch 11/50, Train Loss: 0.8313, Val Loss: 0.9245
Epoch 12/50, Train Loss: 0.8326, Val Loss: 0.9239
Epoch 00013: reducing learning rate of group 0 to 1.0000e-03.
Epoch 13/50, Train Loss: 0.8317, Val Loss: 0.9519
Epoch 14/50, Train Loss: 0.8163, Val Loss: 0.9159
Epoch 15/50, Train Loss: 0.8092, Val Loss: 0.9155
Epoch 16/50, Train Loss: 0.8074, Val Loss: 0.9095
Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.
Epoch 17/50, Train Loss: 0.8068, Val Loss: 0.9102
Epoch 18/50, Train Loss: 0.8021, Val Loss: 0.9082
Epoch 19/50, Train Loss: 0.8012, Val Loss: 0.9095
Epoch 20/50, Train Loss: 0.8000, Val Loss: 0.9140
Epoch 00021: reducing learning rate of group 0 to 1.0000e-05.
Epoch 21/50, Train Loss: 0.7995, Val Loss: 0.9152
Epoch 22/50, Train Loss: 0.7988, Val Loss: 0.9135
Epoch 23/50, Train Loss: 0.7983, Val Loss: 0.9131
Epoch 24/50, Train Loss: 0.7982, Val Loss: 0.9132
Epoch 00025: reducing learning rate of group 0 to 1.0000e-06.
Epoch 25/50, Train Loss: 0.7982, Val Loss: 0.9133
Epoch 26/50, Train Loss: 0.7977, Val Loss: 0.9135
Epoch 27/50, Train Loss: 0.7979, Val Loss: 0.9132
Epoch 28/50, Train Loss: 0.7979, Val Loss: 0.9132
Epoch 00029: reducing learning rate of group 0 to 1.0000e-07.
Epoch 29/50, Train Loss: 0.7981, Val Loss: 0.9132
Epoch 30/50, Train Loss: 0.7981, Val Loss: 0.9135
Epoch 31/50, Train Loss: 0.7982, Val Loss: 0.9131
Epoch 32/50, Train Loss: 0.7983, Val Loss: 0.9134
Epoch 00033: reducing learning rate of group 0 to 1.0000e-08.
Epoch 33/50, Train Loss: 0.7978, Val Loss: 0.9132
Epoch 34/50, Train Loss: 0.7981, Val Loss: 0.9133
Epoch 35/50, Train Loss: 0.7980, Val Loss: 0.9133
Epoch 36/50, Train Loss: 0.7980, Val Loss: 0.9134
Epoch 37/50, Train Loss: 0.7978, Val Loss: 0.9131
Epoch 38/50, Train Loss: 0.7982, Val Loss: 0.9135
Epoch 39/50, Train Loss: 0.7982, Val Loss: 0.9134
Epoch 40/50, Train Loss: 0.7983, Val Loss: 0.9132
Epoch 41/50, Train Loss: 0.7979, Val Loss: 0.9133
Epoch 42/50, Train Loss: 0.7979, Val Loss: 0.9135
Epoch 43/50, Train Loss: 0.7980, Val Loss: 0.9133
Epoch 44/50, Train Loss: 0.7977, Val Loss: 0.9134
Epoch 45/50, Train Loss: 0.7984, Val Loss: 0.9133
Epoch 46/50, Train Loss: 0.7980, Val Loss: 0.9131
Epoch 47/50, Train Loss: 0.7980, Val Loss: 0.9134
Epoch 48/50, Train Loss: 0.7980, Val Loss: 0.9135
Epoch 49/50, Train Loss: 0.7981, Val Loss: 0.9135
Epoch 50/50, Train Loss: 0.7980, Val Loss: 0.9133
Epoch 1/50, Train Loss: 0.7978, Val Loss: 0.9134
Epoch 2/50, Train Loss: 0.7981, Val Loss: 0.9132
Epoch 3/50, Train Loss: 0.7983, Val Loss: 0.9135
Epoch 4/50, Train Loss: 0.7983, Val Loss: 0.9133
Epoch 5/50, Train Loss: 0.7980, Val Loss: 0.9134
Epoch 6/50, Train Loss: 0.7981, Val Loss: 0.9134
Epoch 7/50, Train Loss: 0.7983, Val Loss: 0.9135
Epoch 8/50, Train Loss: 0.7981, Val Loss: 0.9135
Epoch 9/50, Train Loss: 0.7979, Val Loss: 0.9131
Epoch 10/50, Train Loss: 0.7976, Val Loss: 0.9133
Epoch 11/50, Train Loss: 0.7978, Val Loss: 0.9133
Epoch 12/50, Train Loss: 0.7979, Val Loss: 0.9133
Epoch 13/50, Train Loss: 0.7979, Val Loss: 0.9135
Epoch 14/50, Train Loss: 0.7981, Val Loss: 0.9133
Epoch 15/50, Train Loss: 0.7981, Val Loss: 0.9133
Epoch 16/50, Train Loss: 0.7986, Val Loss: 0.9131
Epoch 17/50, Train Loss: 0.7978, Val Loss: 0.9133
Epoch 18/50, Train Loss: 0.7983, Val Loss: 0.9134
Epoch 19/50, Train Loss: 0.7982, Val Loss: 0.9133
Epoch 20/50, Train Loss: 0.7980, Val Loss: 0.9135
Epoch 21/50, Train Loss: 0.7976, Val Loss: 0.9133
Epoch 22/50, Train Loss: 0.7981, Val Loss: 0.9134
Epoch 23/50, Train Loss: 0.7982, Val Loss: 0.9133
Epoch 24/50, Train Loss: 0.7982, Val Loss: 0.9133
Epoch 25/50, Train Loss: 0.7980, Val Loss: 0.9131
Epoch 26/50, Train Loss: 0.7978, Val Loss: 0.9133
Epoch 27/50, Train Loss: 0.7978, Val Loss: 0.9134
Epoch 28/50, Train Loss: 0.7979, Val Loss: 0.9133
Epoch 29/50, Train Loss: 0.7980, Val Loss: 0.9134
Epoch 30/50, Train Loss: 0.7982, Val Loss: 0.9133
Epoch 31/50, Train Loss: 0.7981, Val Loss: 0.9133
Epoch 32/50, Train Loss: 0.7982, Val Loss: 0.9133
Epoch 33/50, Train Loss: 0.7981, Val Loss: 0.9134
Epoch 34/50, Train Loss: 0.7981, Val Loss: 0.9134
Epoch 35/50, Train Loss: 0.7981, Val Loss: 0.9133
Epoch 36/50, Train Loss: 0.7983, Val Loss: 0.9135
Epoch 37/50, Train Loss: 0.7982, Val Loss: 0.9133
Epoch 38/50, Train Loss: 0.7979, Val Loss: 0.9133
Epoch 39/50, Train Loss: 0.7976, Val Loss: 0.9135
Epoch 40/50, Train Loss: 0.7981, Val Loss: 0.9133
Epoch 41/50, Train Loss: 0.7977, Val Loss: 0.9133
Epoch 42/50, Train Loss: 0.7976, Val Loss: 0.9132
Epoch 43/50, Train Loss: 0.7979, Val Loss: 0.9133
Epoch 44/50, Train Loss: 0.7980, Val Loss: 0.9132
Epoch 45/50, Train Loss: 0.7979, Val Loss: 0.9133
Epoch 46/50, Train Loss: 0.7975, Val Loss: 0.9133
Epoch 47/50, Train Loss: 0.7981, Val Loss: 0.9132
Epoch 48/50, Train Loss: 0.7980, Val Loss: 0.9133
Epoch 49/50, Train Loss: 0.7981, Val Loss: 0.9132
Epoch 50/50, Train Loss: 0.7979, Val Loss: 0.9133
Metrics for Fold 21 (Train):
{'0.0': {'precision': 0.897593521179951, 'recall': 0.8807229907773386, 'f1-score': 0.8890782323174596, 'support': 48576.0}, '1.0': {'precision': 0.8829234779445937, 'recall': 0.899518280632411, 'f1-score': 0.8911436292255136, 'support': 48576.0}, 'accuracy': 0.8901206357048749, 'macro avg': {'precision': 0.8902584995622723, 'recall': 0.8901206357048748, 'f1-score': 0.8901109307714866, 'support': 97152.0}, 'weighted avg': {'precision': 0.8902584995622723, 'recall': 0.8901206357048749, 'f1-score': 0.8901109307714867, 'support': 97152.0}}
Metrics for Fold 21 (Test):
{'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}}
Fold 22: Test Patient chb13
Fold 22: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8814, Val Loss: 0.9692
Epoch 2/50, Train Loss: 0.8565, Val Loss: 0.9697
Epoch 3/50, Train Loss: 0.8479, Val Loss: 0.9397
Epoch 4/50, Train Loss: 0.8417, Val Loss: 0.9748
Epoch 5/50, Train Loss: 0.8358, Val Loss: 0.9588
Epoch 6/50, Train Loss: 0.8352, Val Loss: 0.9327
Epoch 7/50, Train Loss: 0.8330, Val Loss: 0.9539
Epoch 8/50, Train Loss: 0.8323, Val Loss: 0.9526
Epoch 9/50, Train Loss: 0.8313, Val Loss: 0.9444
Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.
Epoch 10/50, Train Loss: 0.8308, Val Loss: 0.9552
Epoch 11/50, Train Loss: 0.8159, Val Loss: 0.9585
Epoch 12/50, Train Loss: 0.8093, Val Loss: 0.9621
Epoch 13/50, Train Loss: 0.8083, Val Loss: 0.9786
Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.
Epoch 14/50, Train Loss: 0.8067, Val Loss: 0.9682
Epoch 15/50, Train Loss: 0.8031, Val Loss: 0.9768
Epoch 16/50, Train Loss: 0.8018, Val Loss: 0.9781
Epoch 17/50, Train Loss: 0.8010, Val Loss: 0.9800
Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.
Epoch 18/50, Train Loss: 0.8007, Val Loss: 0.9782
Epoch 19/50, Train Loss: 0.7999, Val Loss: 0.9802
Epoch 20/50, Train Loss: 0.7998, Val Loss: 0.9798
Epoch 21/50, Train Loss: 0.8002, Val Loss: 0.9798
Epoch 00022: reducing learning rate of group 0 to 1.0000e-06.
Epoch 22/50, Train Loss: 0.7995, Val Loss: 0.9781
Epoch 23/50, Train Loss: 0.7998, Val Loss: 0.9792
Epoch 24/50, Train Loss: 0.7994, Val Loss: 0.9793
Epoch 25/50, Train Loss: 0.7999, Val Loss: 0.9797
Epoch 00026: reducing learning rate of group 0 to 1.0000e-07.
Epoch 26/50, Train Loss: 0.7996, Val Loss: 0.9793
Epoch 27/50, Train Loss: 0.8000, Val Loss: 0.9801
Epoch 28/50, Train Loss: 0.7994, Val Loss: 0.9795
Epoch 29/50, Train Loss: 0.7997, Val Loss: 0.9798
Epoch 00030: reducing learning rate of group 0 to 1.0000e-08.
Epoch 30/50, Train Loss: 0.7998, Val Loss: 0.9801
Epoch 31/50, Train Loss: 0.7995, Val Loss: 0.9795
Epoch 32/50, Train Loss: 0.7996, Val Loss: 0.9795
Epoch 33/50, Train Loss: 0.7996, Val Loss: 0.9794
Epoch 34/50, Train Loss: 0.7999, Val Loss: 0.9794
Epoch 35/50, Train Loss: 0.8000, Val Loss: 0.9795
Epoch 36/50, Train Loss: 0.7998, Val Loss: 0.9798
Epoch 37/50, Train Loss: 0.7999, Val Loss: 0.9797
Epoch 38/50, Train Loss: 0.7995, Val Loss: 0.9794
Epoch 39/50, Train Loss: 0.7998, Val Loss: 0.9799
Epoch 40/50, Train Loss: 0.7995, Val Loss: 0.9797
Epoch 41/50, Train Loss: 0.7999, Val Loss: 0.9795
Epoch 42/50, Train Loss: 0.7998, Val Loss: 0.9796
Epoch 43/50, Train Loss: 0.7997, Val Loss: 0.9800
Epoch 44/50, Train Loss: 0.7997, Val Loss: 0.9799
Epoch 45/50, Train Loss: 0.8003, Val Loss: 0.9797
Epoch 46/50, Train Loss: 0.7997, Val Loss: 0.9800
Epoch 47/50, Train Loss: 0.7995, Val Loss: 0.9799
Epoch 48/50, Train Loss: 0.7999, Val Loss: 0.9793
Epoch 49/50, Train Loss: 0.7997, Val Loss: 0.9796
Epoch 50/50, Train Loss: 0.8001, Val Loss: 0.9797
Epoch 1/50, Train Loss: 0.8000, Val Loss: 0.9794
Epoch 2/50, Train Loss: 0.7996, Val Loss: 0.9793
Epoch 3/50, Train Loss: 0.7997, Val Loss: 0.9793
Epoch 4/50, Train Loss: 0.7999, Val Loss: 0.9795
Epoch 5/50, Train Loss: 0.8004, Val Loss: 0.9794
Epoch 6/50, Train Loss: 0.7999, Val Loss: 0.9798
Epoch 7/50, Train Loss: 0.7994, Val Loss: 0.9797
Epoch 8/50, Train Loss: 0.8002, Val Loss: 0.9794
Epoch 9/50, Train Loss: 0.8000, Val Loss: 0.9798
Epoch 10/50, Train Loss: 0.7998, Val Loss: 0.9798
Epoch 11/50, Train Loss: 0.8000, Val Loss: 0.9795
Epoch 12/50, Train Loss: 0.7997, Val Loss: 0.9797
Epoch 13/50, Train Loss: 0.7994, Val Loss: 0.9799
Epoch 14/50, Train Loss: 0.8000, Val Loss: 0.9795
Epoch 15/50, Train Loss: 0.8005, Val Loss: 0.9794
Epoch 16/50, Train Loss: 0.8001, Val Loss: 0.9791
Epoch 17/50, Train Loss: 0.8001, Val Loss: 0.9797
Epoch 18/50, Train Loss: 0.7997, Val Loss: 0.9798
Epoch 19/50, Train Loss: 0.7993, Val Loss: 0.9797
Epoch 20/50, Train Loss: 0.7996, Val Loss: 0.9796
Epoch 21/50, Train Loss: 0.7997, Val Loss: 0.9796
Epoch 22/50, Train Loss: 0.7997, Val Loss: 0.9796
Epoch 23/50, Train Loss: 0.7999, Val Loss: 0.9798
Epoch 24/50, Train Loss: 0.7998, Val Loss: 0.9799
Epoch 25/50, Train Loss: 0.7999, Val Loss: 0.9794
Epoch 26/50, Train Loss: 0.8001, Val Loss: 0.9797
Epoch 27/50, Train Loss: 0.7994, Val Loss: 0.9799
Epoch 28/50, Train Loss: 0.7996, Val Loss: 0.9797
Epoch 29/50, Train Loss: 0.8000, Val Loss: 0.9797
Epoch 30/50, Train Loss: 0.7998, Val Loss: 0.9798
Epoch 31/50, Train Loss: 0.7998, Val Loss: 0.9796
Epoch 32/50, Train Loss: 0.8001, Val Loss: 0.9793
Epoch 33/50, Train Loss: 0.8000, Val Loss: 0.9798
Epoch 34/50, Train Loss: 0.7996, Val Loss: 0.9795
Epoch 35/50, Train Loss: 0.8000, Val Loss: 0.9795
Epoch 36/50, Train Loss: 0.7999, Val Loss: 0.9798
Epoch 37/50, Train Loss: 0.7999, Val Loss: 0.9798
Epoch 38/50, Train Loss: 0.7996, Val Loss: 0.9797
Epoch 39/50, Train Loss: 0.7993, Val Loss: 0.9794
Epoch 40/50, Train Loss: 0.7998, Val Loss: 0.9799
Epoch 41/50, Train Loss: 0.8001, Val Loss: 0.9795
Epoch 42/50, Train Loss: 0.8001, Val Loss: 0.9796
Epoch 43/50, Train Loss: 0.8002, Val Loss: 0.9798
Epoch 44/50, Train Loss: 0.7998, Val Loss: 0.9798
Epoch 45/50, Train Loss: 0.8001, Val Loss: 0.9794
Epoch 46/50, Train Loss: 0.8001, Val Loss: 0.9798
Epoch 47/50, Train Loss: 0.8002, Val Loss: 0.9799
Epoch 48/50, Train Loss: 0.8000, Val Loss: 0.9792
Epoch 49/50, Train Loss: 0.7998, Val Loss: 0.9797
Epoch 50/50, Train Loss: 0.7992, Val Loss: 0.9797
Metrics for Fold 22 (Train):
{'0.0': {'precision': 0.8934712237776937, 'recall': 0.8750411725955204, 'f1-score': 0.8841601664066562, 'support': 48576.0}, '1.0': {'precision': 0.8775666626326193, 'recall': 0.8956686429512516, 'f1-score': 0.8865252562300059, 'support': 48576.0}, 'accuracy': 0.885354907773386, 'macro avg': {'precision': 0.8855189432051565, 'recall': 0.885354907773386, 'f1-score': 0.885342711318331, 'support': 97152.0}, 'weighted avg': {'precision': 0.8855189432051565, 'recall': 0.885354907773386, 'f1-score': 0.8853427113183311, 'support': 97152.0}}
Metrics for Fold 22 (Test):
{'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}}
Fold 23: Test Patient chb07
Fold 23: Distribution of classes
Training set: Class 0: 48576, Class 1: 48576
Test set: Class 0: 2500, Class 1: 2500

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8827, Val Loss: 0.8890
Epoch 2/50, Train Loss: 0.8641, Val Loss: 0.8676
Epoch 3/50, Train Loss: 0.8583, Val Loss: 0.8887
Epoch 4/50, Train Loss: 0.8530, Val Loss: 0.8679
Epoch 5/50, Train Loss: 0.8510, Val Loss: 0.8552
Epoch 6/50, Train Loss: 0.8485, Val Loss: 0.8829
Epoch 7/50, Train Loss: 0.8480, Val Loss: 0.8773
Epoch 8/50, Train Loss: 0.8461, Val Loss: 0.8572
Epoch 9/50, Train Loss: 0.8434, Val Loss: 0.8458
Epoch 10/50, Train Loss: 0.8421, Val Loss: 0.8371
Epoch 11/50, Train Loss: 0.8402, Val Loss: 0.8426
Epoch 12/50, Train Loss: 0.8400, Val Loss: 0.8410
Epoch 13/50, Train Loss: 0.8392, Val Loss: 0.8156
Epoch 14/50, Train Loss: 0.8388, Val Loss: 0.8492
Epoch 15/50, Train Loss: 0.8372, Val Loss: 0.8364
Epoch 16/50, Train Loss: 0.8376, Val Loss: 0.8512
Epoch 00017: reducing learning rate of group 0 to 1.0000e-03.
Epoch 17/50, Train Loss: 0.8384, Val Loss: 0.8450
Epoch 18/50, Train Loss: 0.8225, Val Loss: 0.8267
Epoch 19/50, Train Loss: 0.8166, Val Loss: 0.8273
Epoch 20/50, Train Loss: 0.8148, Val Loss: 0.8297
Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.
Epoch 21/50, Train Loss: 0.8145, Val Loss: 0.8209
Epoch 22/50, Train Loss: 0.8108, Val Loss: 0.8226
Epoch 23/50, Train Loss: 0.8093, Val Loss: 0.8229
Epoch 24/50, Train Loss: 0.8085, Val Loss: 0.8224
Epoch 00025: reducing learning rate of group 0 to 1.0000e-05.
Epoch 25/50, Train Loss: 0.8074, Val Loss: 0.8199
Epoch 26/50, Train Loss: 0.8066, Val Loss: 0.8210
Epoch 27/50, Train Loss: 0.8067, Val Loss: 0.8213
Epoch 28/50, Train Loss: 0.8065, Val Loss: 0.8208
Epoch 00029: reducing learning rate of group 0 to 1.0000e-06.
Epoch 29/50, Train Loss: 0.8067, Val Loss: 0.8212
Epoch 30/50, Train Loss: 0.8066, Val Loss: 0.8213
Epoch 31/50, Train Loss: 0.8066, Val Loss: 0.8216
Epoch 32/50, Train Loss: 0.8066, Val Loss: 0.8209
Epoch 00033: reducing learning rate of group 0 to 1.0000e-07.
Epoch 33/50, Train Loss: 0.8067, Val Loss: 0.8212
Epoch 34/50, Train Loss: 0.8062, Val Loss: 0.8212
Epoch 35/50, Train Loss: 0.8061, Val Loss: 0.8214
Epoch 36/50, Train Loss: 0.8063, Val Loss: 0.8211
Epoch 00037: reducing learning rate of group 0 to 1.0000e-08.
Epoch 37/50, Train Loss: 0.8064, Val Loss: 0.8213
Epoch 38/50, Train Loss: 0.8065, Val Loss: 0.8214
Epoch 39/50, Train Loss: 0.8061, Val Loss: 0.8212
Epoch 40/50, Train Loss: 0.8066, Val Loss: 0.8214
Epoch 41/50, Train Loss: 0.8066, Val Loss: 0.8209
Epoch 42/50, Train Loss: 0.8063, Val Loss: 0.8211
Epoch 43/50, Train Loss: 0.8067, Val Loss: 0.8210
Epoch 44/50, Train Loss: 0.8061, Val Loss: 0.8210
Epoch 45/50, Train Loss: 0.8065, Val Loss: 0.8211
Epoch 46/50, Train Loss: 0.8066, Val Loss: 0.8209
Epoch 47/50, Train Loss: 0.8064, Val Loss: 0.8212
Epoch 48/50, Train Loss: 0.8066, Val Loss: 0.8210
Epoch 49/50, Train Loss: 0.8064, Val Loss: 0.8215
Epoch 50/50, Train Loss: 0.8063, Val Loss: 0.8213
Epoch 1/50, Train Loss: 0.8064, Val Loss: 0.8211
Epoch 2/50, Train Loss: 0.8059, Val Loss: 0.8212
Epoch 3/50, Train Loss: 0.8067, Val Loss: 0.8215
Epoch 4/50, Train Loss: 0.8066, Val Loss: 0.8209
Epoch 5/50, Train Loss: 0.8066, Val Loss: 0.8212
Epoch 6/50, Train Loss: 0.8070, Val Loss: 0.8213
Epoch 7/50, Train Loss: 0.8065, Val Loss: 0.8212
Epoch 8/50, Train Loss: 0.8069, Val Loss: 0.8212
Epoch 9/50, Train Loss: 0.8063, Val Loss: 0.8213
Epoch 10/50, Train Loss: 0.8065, Val Loss: 0.8212
Epoch 11/50, Train Loss: 0.8067, Val Loss: 0.8212
Epoch 12/50, Train Loss: 0.8064, Val Loss: 0.8212
Epoch 13/50, Train Loss: 0.8067, Val Loss: 0.8214
Epoch 14/50, Train Loss: 0.8063, Val Loss: 0.8210
Epoch 15/50, Train Loss: 0.8060, Val Loss: 0.8212
Epoch 16/50, Train Loss: 0.8063, Val Loss: 0.8212
Epoch 17/50, Train Loss: 0.8062, Val Loss: 0.8213
Epoch 18/50, Train Loss: 0.8064, Val Loss: 0.8207
Epoch 19/50, Train Loss: 0.8063, Val Loss: 0.8214
Epoch 20/50, Train Loss: 0.8066, Val Loss: 0.8212
Epoch 21/50, Train Loss: 0.8065, Val Loss: 0.8212
Epoch 22/50, Train Loss: 0.8066, Val Loss: 0.8217
Epoch 23/50, Train Loss: 0.8066, Val Loss: 0.8209
Epoch 24/50, Train Loss: 0.8067, Val Loss: 0.8212
Epoch 25/50, Train Loss: 0.8060, Val Loss: 0.8212
Epoch 26/50, Train Loss: 0.8065, Val Loss: 0.8213
Epoch 27/50, Train Loss: 0.8061, Val Loss: 0.8211
Epoch 28/50, Train Loss: 0.8067, Val Loss: 0.8211
Epoch 29/50, Train Loss: 0.8069, Val Loss: 0.8214
Epoch 30/50, Train Loss: 0.8062, Val Loss: 0.8212
Epoch 31/50, Train Loss: 0.8066, Val Loss: 0.8211
Epoch 32/50, Train Loss: 0.8065, Val Loss: 0.8213
Epoch 33/50, Train Loss: 0.8064, Val Loss: 0.8211
Epoch 34/50, Train Loss: 0.8061, Val Loss: 0.8216
Epoch 35/50, Train Loss: 0.8061, Val Loss: 0.8208
Epoch 36/50, Train Loss: 0.8065, Val Loss: 0.8213
Epoch 37/50, Train Loss: 0.8063, Val Loss: 0.8215
Epoch 38/50, Train Loss: 0.8063, Val Loss: 0.8212
Epoch 39/50, Train Loss: 0.8062, Val Loss: 0.8213
Epoch 40/50, Train Loss: 0.8063, Val Loss: 0.8212
Epoch 41/50, Train Loss: 0.8065, Val Loss: 0.8210
Epoch 42/50, Train Loss: 0.8066, Val Loss: 0.8214
Epoch 43/50, Train Loss: 0.8064, Val Loss: 0.8212
Epoch 44/50, Train Loss: 0.8067, Val Loss: 0.8215
Epoch 45/50, Train Loss: 0.8059, Val Loss: 0.8211
Epoch 46/50, Train Loss: 0.8063, Val Loss: 0.8213
Epoch 47/50, Train Loss: 0.8066, Val Loss: 0.8211
Epoch 48/50, Train Loss: 0.8064, Val Loss: 0.8209
Epoch 49/50, Train Loss: 0.8065, Val Loss: 0.8216
Epoch 50/50, Train Loss: 0.8064, Val Loss: 0.8213
Metrics for Fold 23 (Train):
{'0.0': {'precision': 0.8947152452324904, 'recall': 0.8528491436100132, 'f1-score': 0.8732807048978172, 'support': 48576.0}, '1.0': {'precision': 0.859426930716435, 'recall': 0.8996417984189723, 'f1-score': 0.8790746794065879, 'support': 48576.0}, 'accuracy': 0.8762454710144928, 'macro avg': {'precision': 0.8770710879744626, 'recall': 0.8762454710144927, 'f1-score': 0.8761776921522025, 'support': 97152.0}, 'weighted avg': {'precision': 0.8770710879744628, 'recall': 0.8762454710144928, 'f1-score': 0.8761776921522025, 'support': 97152.0}}
Metrics for Fold 23 (Test):
{'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}}
Fold 24: Test Patient chb18
Fold 24: Distribution of classes
Training set: Class 0: 48588, Class 1: 48588
Test set: Class 0: 2488, Class 1: 2488

Inicializando el modelo...
Epoch 1/50, Train Loss: 0.8776, Val Loss: 0.9271
Epoch 2/50, Train Loss: 0.8592, Val Loss: 0.8844
Epoch 3/50, Train Loss: 0.8520, Val Loss: 0.8908
Epoch 4/50, Train Loss: 0.8481, Val Loss: 0.8966
Epoch 5/50, Train Loss: 0.8450, Val Loss: 0.8894
Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.
Epoch 6/50, Train Loss: 0.8446, Val Loss: 0.8979
Epoch 7/50, Train Loss: 0.8292, Val Loss: 0.8732
Epoch 8/50, Train Loss: 0.8224, Val Loss: 0.8633
Epoch 9/50, Train Loss: 0.8214, Val Loss: 0.8710
Epoch 10/50, Train Loss: 0.8175, Val Loss: 0.8727
Epoch 11/50, Train Loss: 0.8145, Val Loss: 0.8656
Epoch 12/50, Train Loss: 0.8122, Val Loss: 0.8558
Epoch 13/50, Train Loss: 0.8110, Val Loss: 0.8617
Epoch 14/50, Train Loss: 0.8097, Val Loss: 0.8623
Epoch 15/50, Train Loss: 0.8088, Val Loss: 0.8513
Epoch 16/50, Train Loss: 0.8084, Val Loss: 0.8470
Epoch 17/50, Train Loss: 0.8078, Val Loss: 0.8490
Epoch 18/50, Train Loss: 0.8069, Val Loss: 0.8556
Epoch 19/50, Train Loss: 0.8058, Val Loss: 0.8611
Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.
Epoch 20/50, Train Loss: 0.8050, Val Loss: 0.8644
Epoch 21/50, Train Loss: 0.8010, Val Loss: 0.8451
Epoch 22/50, Train Loss: 0.7992, Val Loss: 0.8444
Epoch 23/50, Train Loss: 0.7986, Val Loss: 0.8435
Epoch 24/50, Train Loss: 0.7979, Val Loss: 0.8430
Epoch 25/50, Train Loss: 0.7977, Val Loss: 0.8443
Epoch 26/50, Train Loss: 0.7973, Val Loss: 0.8446
Epoch 27/50, Train Loss: 0.7971, Val Loss: 0.8398
Epoch 28/50, Train Loss: 0.7963, Val Loss: 0.8456
Epoch 29/50, Train Loss: 0.7964, Val Loss: 0.8398
Epoch 30/50, Train Loss: 0.7966, Val Loss: 0.8409
Epoch 00031: reducing learning rate of group 0 to 1.0000e-05.
Epoch 31/50, Train Loss: 0.7961, Val Loss: 0.8426
Epoch 32/50, Train Loss: 0.7951, Val Loss: 0.8421
Epoch 33/50, Train Loss: 0.7957, Val Loss: 0.8419
Epoch 34/50, Train Loss: 0.7950, Val Loss: 0.8418
Epoch 00035: reducing learning rate of group 0 to 1.0000e-06.
Epoch 35/50, Train Loss: 0.7952, Val Loss: 0.8412
Epoch 36/50, Train Loss: 0.7951, Val Loss: 0.8413
Epoch 37/50, Train Loss: 0.7949, Val Loss: 0.8416
Epoch 38/50, Train Loss: 0.7947, Val Loss: 0.8412
Epoch 00039: reducing learning rate of group 0 to 1.0000e-07.
Epoch 39/50, Train Loss: 0.7947, Val Loss: 0.8417
Epoch 40/50, Train Loss: 0.7950, Val Loss: 0.8418
Epoch 41/50, Train Loss: 0.7950, Val Loss: 0.8417
Epoch 42/50, Train Loss: 0.7948, Val Loss: 0.8417
Epoch 00043: reducing learning rate of group 0 to 1.0000e-08.
Epoch 43/50, Train Loss: 0.7950, Val Loss: 0.8413
Epoch 44/50, Train Loss: 0.7945, Val Loss: 0.8418
Epoch 45/50, Train Loss: 0.7949, Val Loss: 0.8414
Epoch 46/50, Train Loss: 0.7949, Val Loss: 0.8415
Epoch 47/50, Train Loss: 0.7951, Val Loss: 0.8411
Epoch 48/50, Train Loss: 0.7947, Val Loss: 0.8416
Epoch 49/50, Train Loss: 0.7954, Val Loss: 0.8411
Epoch 50/50, Train Loss: 0.7950, Val Loss: 0.8413
Epoch 1/50, Train Loss: 0.7949, Val Loss: 0.8418
Epoch 2/50, Train Loss: 0.7954, Val Loss: 0.8418
Epoch 3/50, Train Loss: 0.7951, Val Loss: 0.8418
Epoch 4/50, Train Loss: 0.7952, Val Loss: 0.8414
Epoch 5/50, Train Loss: 0.7949, Val Loss: 0.8417
Epoch 6/50, Train Loss: 0.7952, Val Loss: 0.8416
Epoch 7/50, Train Loss: 0.7951, Val Loss: 0.8416
Epoch 8/50, Train Loss: 0.7949, Val Loss: 0.8417
Epoch 9/50, Train Loss: 0.7948, Val Loss: 0.8418
Epoch 10/50, Train Loss: 0.7951, Val Loss: 0.8414
Epoch 11/50, Train Loss: 0.7951, Val Loss: 0.8414
Epoch 12/50, Train Loss: 0.7949, Val Loss: 0.8415
Epoch 13/50, Train Loss: 0.7946, Val Loss: 0.8412
Epoch 14/50, Train Loss: 0.7951, Val Loss: 0.8412
Epoch 15/50, Train Loss: 0.7950, Val Loss: 0.8414
Epoch 16/50, Train Loss: 0.7950, Val Loss: 0.8413
Epoch 17/50, Train Loss: 0.7950, Val Loss: 0.8413
Epoch 18/50, Train Loss: 0.7951, Val Loss: 0.8418
Epoch 19/50, Train Loss: 0.7947, Val Loss: 0.8416
Epoch 20/50, Train Loss: 0.7950, Val Loss: 0.8413
Epoch 21/50, Train Loss: 0.7950, Val Loss: 0.8416
Epoch 22/50, Train Loss: 0.7952, Val Loss: 0.8414
Epoch 23/50, Train Loss: 0.7949, Val Loss: 0.8410
Epoch 24/50, Train Loss: 0.7949, Val Loss: 0.8413
Epoch 25/50, Train Loss: 0.7952, Val Loss: 0.8412
Epoch 26/50, Train Loss: 0.7949, Val Loss: 0.8414
Epoch 27/50, Train Loss: 0.7948, Val Loss: 0.8417
Epoch 28/50, Train Loss: 0.7950, Val Loss: 0.8412
Epoch 29/50, Train Loss: 0.7953, Val Loss: 0.8414
Epoch 30/50, Train Loss: 0.7949, Val Loss: 0.8412
Epoch 31/50, Train Loss: 0.7948, Val Loss: 0.8411
Epoch 32/50, Train Loss: 0.7951, Val Loss: 0.8414
Epoch 33/50, Train Loss: 0.7949, Val Loss: 0.8414
Epoch 34/50, Train Loss: 0.7950, Val Loss: 0.8421
Epoch 35/50, Train Loss: 0.7954, Val Loss: 0.8418
Epoch 36/50, Train Loss: 0.7946, Val Loss: 0.8409
Epoch 37/50, Train Loss: 0.7950, Val Loss: 0.8421
Epoch 38/50, Train Loss: 0.7950, Val Loss: 0.8412
Epoch 39/50, Train Loss: 0.7951, Val Loss: 0.8419
Epoch 40/50, Train Loss: 0.7947, Val Loss: 0.8417
Epoch 41/50, Train Loss: 0.7949, Val Loss: 0.8416
Epoch 42/50, Train Loss: 0.7951, Val Loss: 0.8415
Epoch 43/50, Train Loss: 0.7949, Val Loss: 0.8411
Epoch 44/50, Train Loss: 0.7950, Val Loss: 0.8418
Epoch 45/50, Train Loss: 0.7947, Val Loss: 0.8416
Epoch 46/50, Train Loss: 0.7952, Val Loss: 0.8415
Epoch 47/50, Train Loss: 0.7953, Val Loss: 0.8417
Epoch 48/50, Train Loss: 0.7952, Val Loss: 0.8412
Epoch 49/50, Train Loss: 0.7953, Val Loss: 0.8416
Epoch 50/50, Train Loss: 0.7947, Val Loss: 0.8413
Metrics for Fold 24 (Train):
{'0.0': {'precision': 0.9070299145299145, 'recall': 0.8736519305178233, 'f1-score': 0.8900280957772466, 'support': 48588.0}, '1.0': {'precision': 0.8781364141654756, 'recall': 0.9104511401992261, 'f1-score': 0.8940018592619537, 'support': 48588.0}, 'accuracy': 0.8920515353585248, 'macro avg': {'precision': 0.8925831643476951, 'recall': 0.8920515353585248, 'f1-score': 0.8920149775196002, 'support': 97176.0}, 'weighted avg': {'precision': 0.8925831643476951, 'recall': 0.8920515353585248, 'f1-score': 0.8920149775196001, 'support': 97176.0}}
Metrics for Fold 24 (Test):
{'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}}
[{'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}}, {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}}, {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}}, {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}}, {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}}, {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}}, {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}}, {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}}, {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}}, {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}}, {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}}, {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}}, {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}}, {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}}, {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}}, {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}}, {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}}, {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}}, {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}}, {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}}, {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}}, {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}}, {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}}, {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}}]
Fold metrics: {'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}} precision
0.0 {'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}} recall
0.0 {'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}} f1-score
1.0 {'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}} precision
1.0 {'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}} recall
1.0 {'0.0': {'precision': 0.786509900990099, 'recall': 0.9400887573964497, 'f1-score': 0.8564690026954178, 'support': 1352.0}, '1.0': {'precision': 0.9255514705882353, 'recall': 0.7448224852071006, 'f1-score': 0.8254098360655737, 'support': 1352.0}, 'accuracy': 0.8424556213017751, 'macro avg': {'precision': 0.8560306857891671, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}, 'weighted avg': {'precision': 0.8560306857891672, 'recall': 0.8424556213017751, 'f1-score': 0.8409394193804958, 'support': 2704.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.8663838812301167, 'recall': 0.3268, 'f1-score': 0.47458611675864065, 'support': 2500.0}, '1.0': {'precision': 0.585161449346808, 'recall': 0.9496, 'f1-score': 0.7241116364190942, 'support': 2500.0}, 'accuracy': 0.6382, 'macro avg': {'precision': 0.7257726652884624, 'recall': 0.6382, 'f1-score': 0.5993488765888674, 'support': 5000.0}, 'weighted avg': {'precision': 0.7257726652884623, 'recall': 0.6382, 'f1-score': 0.5993488765888675, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.5593495934959349, 'recall': 0.9632, 'f1-score': 0.7077149155033063, 'support': 2500.0}, '1.0': {'precision': 0.8676258992805755, 'recall': 0.2412, 'f1-score': 0.37746478873239436, 'support': 2500.0}, 'accuracy': 0.6022, 'macro avg': {'precision': 0.7134877463882552, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}, 'weighted avg': {'precision': 0.7134877463882553, 'recall': 0.6022, 'f1-score': 0.5425898521178504, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}} precision
0.0 {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}} recall
0.0 {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}} f1-score
1.0 {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}} precision
1.0 {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}} recall
1.0 {'0.0': {'precision': 0.9190166305133767, 'recall': 0.7904228855721394, 'f1-score': 0.8498829822801739, 'support': 1608.0}, '1.0': {'precision': 0.8161483906164757, 'recall': 0.9303482587064676, 'f1-score': 0.8695146759662888, 'support': 1608.0}, 'accuracy': 0.8603855721393034, 'macro avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232314, 'support': 3216.0}, 'weighted avg': {'precision': 0.8675825105649262, 'recall': 0.8603855721393034, 'f1-score': 0.8596988291232315, 'support': 3216.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.5776233035418735, 'recall': 0.698, 'f1-score': 0.632131860170259, 'support': 2500.0}, '1.0': {'precision': 0.6184941889843355, 'recall': 0.4896, 'f1-score': 0.5465505693235098, 'support': 2500.0}, 'accuracy': 0.5938, 'macro avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}, 'weighted avg': {'precision': 0.5980587462631045, 'recall': 0.5938, 'f1-score': 0.5893412147468844, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}} precision
0.0 {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}} recall
0.0 {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}} f1-score
1.0 {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}} precision
1.0 {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}} recall
1.0 {'0.0': {'precision': 0.7745311818578282, 'recall': 0.7655172413793103, 'f1-score': 0.7699978322133103, 'support': 2320.0}, '1.0': {'precision': 0.7682147422241159, 'recall': 0.7771551724137931, 'f1-score': 0.7726590957788729, 'support': 2320.0}, 'accuracy': 0.7713362068965517, 'macro avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960916, 'support': 4640.0}, 'weighted avg': {'precision': 0.771372962040972, 'recall': 0.7713362068965517, 'f1-score': 0.7713284639960917, 'support': 4640.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}} precision
0.0 {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}} recall
0.0 {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}} f1-score
1.0 {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}} precision
1.0 {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}} recall
1.0 {'0.0': {'precision': 0.5991561181434599, 'recall': 0.8729508196721312, 'f1-score': 0.7105921601334446, 'support': 488.0}, '1.0': {'precision': 0.7660377358490567, 'recall': 0.41598360655737704, 'f1-score': 0.5391766268260293, 'support': 488.0}, 'accuracy': 0.6444672131147541, 'macro avg': {'precision': 0.6825969269962583, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}, 'weighted avg': {'precision': 0.6825969269962584, 'recall': 0.6444672131147541, 'f1-score': 0.624884393479737, 'support': 976.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.7432641684732115, 'recall': 0.96, 'f1-score': 0.837842555419794, 'support': 2500.0}, '1.0': {'precision': 0.9435347261434218, 'recall': 0.6684, 'f1-score': 0.7824865371107469, 'support': 2500.0}, 'accuracy': 0.8142, 'macro avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}, 'weighted avg': {'precision': 0.8433994473083166, 'recall': 0.8142, 'f1-score': 0.8101645462652705, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}} precision
0.0 {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}} recall
0.0 {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}} f1-score
1.0 {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}} precision
1.0 {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}} recall
1.0 {'0.0': {'precision': 0.6426259824318077, 'recall': 0.8910256410256411, 'f1-score': 0.7467096427612142, 'support': 1560.0}, '1.0': {'precision': 0.8223615464994776, 'recall': 0.5044871794871795, 'f1-score': 0.6253476360746921, 'support': 1560.0}, 'accuracy': 0.6977564102564102, 'macro avg': {'precision': 0.7324937644656426, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179531, 'support': 3120.0}, 'weighted avg': {'precision': 0.7324937644656427, 'recall': 0.6977564102564102, 'f1-score': 0.6860286394179532, 'support': 3120.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}} precision
0.0 {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}} recall
0.0 {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}} f1-score
1.0 {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}} precision
1.0 {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}} recall
1.0 {'0.0': {'precision': 0.35118219749652296, 'recall': 0.3920807453416149, 'f1-score': 0.37050623624358037, 'support': 1288.0}, '1.0': {'precision': 0.31195079086115995, 'recall': 0.27562111801242234, 'f1-score': 0.2926628194558945, 'support': 1288.0}, 'accuracy': 0.3338509316770186, 'macro avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.33158452784973746, 'support': 2576.0}, 'weighted avg': {'precision': 0.33156649417884143, 'recall': 0.3338509316770186, 'f1-score': 0.3315845278497374, 'support': 2576.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}} precision
0.0 {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}} recall
0.0 {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}} f1-score
1.0 {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}} precision
1.0 {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}} recall
1.0 {'0.0': {'precision': 0.8388761467889908, 'recall': 0.7848712446351931, 'f1-score': 0.8109756097560975, 'support': 1864.0}, '1.0': {'precision': 0.797883064516129, 'recall': 0.8492489270386266, 'f1-score': 0.8227650727650728, 'support': 1864.0}, 'accuracy': 0.8170600858369099, 'macro avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605851, 'support': 3728.0}, 'weighted avg': {'precision': 0.8183796056525598, 'recall': 0.8170600858369099, 'f1-score': 0.8168703412605852, 'support': 3728.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.605, 'recall': 0.8712, 'f1-score': 0.7140983606557377, 'support': 2500.0}, '1.0': {'precision': 0.77, 'recall': 0.4312, 'f1-score': 0.5528205128205129, 'support': 2500.0}, 'accuracy': 0.6512, 'macro avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381253, 'support': 5000.0}, 'weighted avg': {'precision': 0.6875, 'recall': 0.6512, 'f1-score': 0.6334594367381252, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.9493223335297584, 'recall': 0.6444, 'f1-score': 0.7676912080057183, 'support': 2500.0}, '1.0': {'precision': 0.7308507417499243, 'recall': 0.9656, 'f1-score': 0.8319834568326728, 'support': 2500.0}, 'accuracy': 0.805, 'macro avg': {'precision': 0.8400865376398413, 'recall': 0.8049999999999999, 'f1-score': 0.7998373324191956, 'support': 5000.0}, 'weighted avg': {'precision': 0.8400865376398413, 'recall': 0.805, 'f1-score': 0.7998373324191956, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}} precision
0.0 {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}} recall
0.0 {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}} f1-score
1.0 {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}} precision
1.0 {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}} recall
1.0 {'0.0': {'precision': 0.5320020481310804, 'recall': 0.9082167832167832, 'f1-score': 0.6709719082983533, 'support': 1144.0}, '1.0': {'precision': 0.6865671641791045, 'recall': 0.20104895104895104, 'f1-score': 0.31102096010818114, 'support': 1144.0}, 'accuracy': 0.5546328671328671, 'macro avg': {'precision': 0.6092846061550925, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}, 'weighted avg': {'precision': 0.6092846061550924, 'recall': 0.5546328671328671, 'f1-score': 0.4909964342032672, 'support': 2288.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.9147982062780269, 'recall': 0.5712, 'f1-score': 0.7032750554050726, 'support': 2500.0}, '1.0': {'precision': 0.688281477173597, 'recall': 0.9468, 'f1-score': 0.7971038895436942, 'support': 2500.0}, 'accuracy': 0.759, 'macro avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}, 'weighted avg': {'precision': 0.8015398417258119, 'recall': 0.759, 'f1-score': 0.7501894724743834, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.9528851244044468, 'recall': 0.72, 'f1-score': 0.8202323991797676, 'support': 2500.0}, '1.0': {'precision': 0.7749919639987143, 'recall': 0.9644, 'f1-score': 0.8593833541258243, 'support': 2500.0}, 'accuracy': 0.8422, 'macro avg': {'precision': 0.8639385442015806, 'recall': 0.8422000000000001, 'f1-score': 0.839807876652796, 'support': 5000.0}, 'weighted avg': {'precision': 0.8639385442015806, 'recall': 0.8422, 'f1-score': 0.839807876652796, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.9000796178343949, 'recall': 0.9044, 'f1-score': 0.9022346368715084, 'support': 2500.0}, '1.0': {'precision': 0.9039389067524116, 'recall': 0.8996, 'f1-score': 0.9017642341619888, 'support': 2500.0}, 'accuracy': 0.902, 'macro avg': {'precision': 0.9020092622934033, 'recall': 0.9019999999999999, 'f1-score': 0.9019994355167487, 'support': 5000.0}, 'weighted avg': {'precision': 0.9020092622934033, 'recall': 0.902, 'f1-score': 0.9019994355167485, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}} precision
0.0 {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}} recall
0.0 {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}} f1-score
1.0 {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}} precision
1.0 {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}} recall
1.0 {'0.0': {'precision': 0.6905859117840685, 'recall': 0.916958041958042, 'f1-score': 0.7878332707472775, 'support': 2288.0}, '1.0': {'precision': 0.8764629388816645, 'recall': 0.5891608391608392, 'f1-score': 0.7046523784631469, 'support': 2288.0}, 'accuracy': 0.7530594405594405, 'macro avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594406, 'f1-score': 0.7462428246052122, 'support': 4576.0}, 'weighted avg': {'precision': 0.7835244253328665, 'recall': 0.7530594405594405, 'f1-score': 0.7462428246052123, 'support': 4576.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.8004101161995899, 'recall': 0.9368, 'f1-score': 0.8632510136380391, 'support': 2500.0}, '1.0': {'precision': 0.9238187078109933, 'recall': 0.7664, 'f1-score': 0.8377787494534324, 'support': 2500.0}, 'accuracy': 0.8516, 'macro avg': {'precision': 0.8621144120052916, 'recall': 0.8515999999999999, 'f1-score': 0.8505148815457357, 'support': 5000.0}, 'weighted avg': {'precision': 0.8621144120052915, 'recall': 0.8516, 'f1-score': 0.8505148815457358, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}} precision
0.0 {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}} recall
0.0 {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}} f1-score
1.0 {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}} precision
1.0 {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}} recall
1.0 {'0.0': {'precision': 0.938347718865598, 'recall': 0.34972426470588236, 'f1-score': 0.5095413458319384, 'support': 2176.0}, '1.0': {'precision': 0.6003953685399605, 'recall': 0.9770220588235294, 'f1-score': 0.7437467203078537, 'support': 2176.0}, 'accuracy': 0.6633731617647058, 'macro avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.6266440330698961, 'support': 4352.0}, 'weighted avg': {'precision': 0.7693715437027793, 'recall': 0.6633731617647058, 'f1-score': 0.626644033069896, 'support': 4352.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.6426823299069995, 'recall': 0.5252, 'f1-score': 0.5780321373541711, 'support': 2500.0}, '1.0': {'precision': 0.5985796415285762, 'recall': 0.708, 'f1-score': 0.6487080813633865, 'support': 2500.0}, 'accuracy': 0.6166, 'macro avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587789, 'support': 5000.0}, 'weighted avg': {'precision': 0.6206309857177879, 'recall': 0.6166, 'f1-score': 0.6133701093587788, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.5201259275916348, 'recall': 0.9252, 'f1-score': 0.6658989491866992, 'support': 2500.0}, '1.0': {'precision': 0.6618444846292948, 'recall': 0.1464, 'f1-score': 0.2397641663937111, 'support': 2500.0}, 'accuracy': 0.5358, 'macro avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}, 'weighted avg': {'precision': 0.5909852061104648, 'recall': 0.5358, 'f1-score': 0.4528315577902052, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}} precision
0.0 {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}} recall
0.0 {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}} f1-score
1.0 {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}} precision
1.0 {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}} recall
1.0 {'0.0': {'precision': 0.9141865079365079, 'recall': 0.7372, 'f1-score': 0.8162090345438441, 'support': 2500.0}, '1.0': {'precision': 0.7798257372654156, 'recall': 0.9308, 'f1-score': 0.848650619985412, 'support': 2500.0}, 'accuracy': 0.834, 'macro avg': {'precision': 0.8470061226009618, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}, 'weighted avg': {'precision': 0.8470061226009616, 'recall': 0.834, 'f1-score': 0.832429827264628, 'support': 5000.0}} f1-score
Fold metrics: {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}}
Type: <class 'dict'>
0.0 {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}} precision
0.0 {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}} recall
0.0 {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}} f1-score
1.0 {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}} precision
1.0 {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}} recall
1.0 {'0.0': {'precision': 0.746524064171123, 'recall': 0.8416398713826366, 'f1-score': 0.7912337048932554, 'support': 2488.0}, '1.0': {'precision': 0.8185168125287886, 'recall': 0.7142282958199357, 'f1-score': 0.7628246404807898, 'support': 2488.0}, 'accuracy': 0.7779340836012861, 'macro avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012862, 'f1-score': 0.7770291726870227, 'support': 4976.0}, 'weighted avg': {'precision': 0.7825204383499558, 'recall': 0.7779340836012861, 'f1-score': 0.7770291726870227, 'support': 4976.0}} f1-score
